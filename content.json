{"meta":{"title":"My Research Wiki","subtitle":null,"description":null,"author":"Haowen Xu","url":"https://wiki.haowen-xu.com","root":"/"},"pages":[{"title":"Page Not Found","date":"2020-06-01T12:15:58.996Z","updated":"2020-06-01T12:15:58.996Z","comments":false,"path":"/404.html","permalink":"https://wiki.haowen-xu.com//404.html","excerpt":"","text":"That's all what we know.Home | Archives | Categories"},{"title":"About","date":"2020-06-01T12:15:59.028Z","updated":"2020-06-01T12:15:59.028Z","comments":true,"path":"about/index.html","permalink":"https://wiki.haowen-xu.com/about/index.html","excerpt":"","text":""},{"title":"Categories","date":"2020-06-01T12:15:59.028Z","updated":"2020-06-01T12:15:59.028Z","comments":true,"path":"categories/index.html","permalink":"https://wiki.haowen-xu.com/categories/index.html","excerpt":"","text":""},{"title":"Tags","date":"2020-06-01T12:15:59.028Z","updated":"2020-06-01T12:15:59.028Z","comments":true,"path":"tags/index.html","permalink":"https://wiki.haowen-xu.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Stochastic Gradient descent","slug":"Deep_Learning/Optimization/Stochastic_Gradient_Descent","date":"2020-05-29T17:53:26.000Z","updated":"2020-06-01T12:15:59.012Z","comments":true,"path":"Deep_Learning/Optimization/Stochastic_Gradient_Descent/","link":"","permalink":"https://wiki.haowen-xu.com/Deep_Learning/Optimization/Stochastic_Gradient_Descent/","excerpt":"","text":"First-Order MethodsFor simplicity, we first introduce the following common notations:\\(J(\\theta)\\): The objective function, which should be minimized w.r.t. \\(\\theta\\).\\(g(\\theta)\\): The gradient of \\(J(\\theta)\\), i.e., \\(g(\\theta) = \\nabla J(\\theta)\\).\\(g_t\\): The abbrevation for \\(g(\\theta_t)\\).One key difference between this article and that of (“An Overview of Gradient Descent Optimization Algorithms” 2016) is that, \\(\\eta\\) is applied on the whole delta when updating the parameters \\(\\theta_t\\), including the momentum term. This is because, practically, one may expect \\(\\eta\\) to be served as a direct constraint of how much the parameters \\(\\theta_t\\) are updated within each training step.(Naive) Stochastic Gradient Descent\\[ \\theta_{t+1} = \\theta_t - \\eta \\, g_t \\]Momentum SGD\\[ \\begin{align} v_t &amp;= \\gamma \\, v_{t-1} + g_t \\\\ \\theta_{t+1} &amp;= \\theta_t - \\eta \\,v_t \\end{align} \\]It converges generally faster than the naive stochastic gradient descent, due to the accumulated \\(v_t\\) can help eliminate the unrelated directions of the gradient, and amplify the most related directions of the gradient. The ratio of amplification is roughly \\(\\frac{1}{1 - \\gamma}\\).Nesterov Momentum SGD\\[ \\begin{align} v_t &amp;= \\gamma \\,v_{t-1} + g(\\theta_t - \\eta\\, \\gamma \\,v_{t-1}) \\\\ \\theta_{t+1} &amp;= \\theta_t - \\eta\\, v_t \\end{align} \\]Alternative FormDozat (2016) proposed to combine the two momentum steps into one, resulting in: \\[ \\begin{align} v_t &amp;= \\gamma \\, v_{t-1} + g_t \\\\ \\theta_{t+1} &amp;= \\theta_t - \\eta \\, (\\gamma \\,v_t + g_t) \\end{align} \\] which may be more preferable in practice.ProofLet \\(\\hat{\\theta}_t = \\theta_t - \\eta\\,\\gamma\\,v_{t-1}\\) and \\(\\hat{g}_t = g(\\hat{\\theta}_t) = g(\\theta_t - \\eta\\, \\gamma \\,v_{t-1})\\), and substitute into the original form, we can obtain: \\[ \\begin{align} v_t &amp;= \\gamma\\,v_{t-1} + \\hat{g}_t \\\\ \\hat{\\theta}_{t+1} &amp;= \\theta_{t+1} - \\eta\\,\\gamma\\,v_t \\\\ &amp;= \\theta_t - \\eta\\,v_t - \\eta\\,\\gamma\\,v_t \\\\ &amp;= \\theta_t - \\eta(\\gamma\\,v_{t-1} + \\hat{g}_t) - \\eta\\,\\gamma\\,v_t \\\\ &amp;= (\\theta_t - \\eta\\,\\gamma\\,v_{t-1}) - \\eta\\,(\\hat{g}_t + \\gamma\\,v_t) \\\\ &amp;= \\hat{\\theta}_t - \\eta\\,(\\gamma\\,v_t + \\hat{g}_t) \\end{align} \\] Discarding all \\(\\hat{ }\\) marks, we then get to the conclusion.Convergence AnalysisThis is a modified version of the above momentum SGD, which converges generally faster. (“比Momentum更快：揭开Nesterov Accelerated Gradient的真面目,” n.d.) suggests that this difference may be caused by the (approximately) second-order property of nesterov momentum SGD, since if we let: \\[ \\begin{align} \\hat{\\theta}_t &amp;= \\theta_t - \\eta\\,\\gamma\\,v_{t-1} \\\\ \\hat{v}_t &amp;= \\gamma^2 \\,v_{t-1} + (\\gamma + 1) \\, g(\\hat{\\theta}_t) \\end{align} \\] we can obtain the following iterative equations: \\[ \\begin{align} \\hat{v}_t &amp;= \\gamma\\,\\hat{v}_{t-1} + g(\\hat{\\theta}_t) + \\gamma\\left[ g(\\hat{\\theta}_t) - g(\\hat{\\theta}_{t-1}) \\right] \\\\ \\hat{\\theta}_{t+1} &amp;= \\hat{\\theta}_t - \\eta \\, \\hat{v}_t \\end{align} \\] which suggests the nesterov momentum SGD uses the second order gradient (approximated by \\(g(\\hat{\\theta}_{t-1}) - g(\\hat{\\theta}_{t-2})\\)) to revise the trajectory produced by the first order gradient \\(g(\\hat{\\theta}_{t-1})\\).ProofFrom the original equation of nesterov momentum SGD, we have: \\[ \\begin{align} \\theta_{t+1} - \\eta\\,\\gamma\\,v_t &amp;= \\theta_t - \\eta\\,(\\gamma+1)\\,v_t \\\\ &amp;= \\theta_t - \\eta\\,(\\gamma+1)\\,\\big( \\gamma\\,v_{t-1} + g(\\theta_t - \\eta\\,\\gamma\\,v_{t-1}) \\big) \\\\ &amp;= \\theta_t - \\eta\\, \\gamma\\,v_{t-1} - \\eta\\,\\gamma^2\\,v_{t-1} - \\eta\\,(\\gamma+1)\\,g(\\theta_t - \\eta\\,\\gamma\\,v_{t-1}) \\end{align} \\] Substitute \\(\\hat{\\theta}_t\\) and \\(\\hat{v}_t\\) into the above equation, we have: \\[ \\hat{\\theta}_{t+1} = \\hat{\\theta}_t - \\eta \\, \\hat{v}_t \\] We then next deal with the term \\(\\hat{v}_t\\). Substitute \\(v_t\\), we have: \\[ \\begin{align} \\hat{v}_t &amp;= (\\gamma+1) \\, g(\\hat{\\theta}_t) + \\gamma^2\\,v_{t-1} \\\\ &amp;= (\\gamma+1) \\, g(\\hat{\\theta}_t) + \\gamma^2 \\left( \\gamma \\, v_{t-2} + g(\\hat{\\theta}_{t-1})\\right) \\\\ &amp;= (\\gamma+1) \\, g(\\hat{\\theta}_t) + \\gamma^2 \\, g(\\hat{\\theta}_{t-1}) + \\gamma^3\\left( \\gamma\\,v_{t-3} + g(\\hat{\\theta}_{t-2}) \\right) \\\\ &amp;= (\\gamma+1) \\, g(\\hat{\\theta}_t) + \\gamma^2 \\, g(\\hat{\\theta}_{t-1}) + \\gamma^3 \\, g(\\hat{\\theta}_{t-2}) + \\dots \\\\ \\hat{v}_{t-1} &amp;= \\qquad \\qquad \\quad \\;\\, (\\gamma+1) \\, g(\\hat{\\theta}_{t-1}) + \\gamma^2 \\, g(\\hat{\\theta}_{t-2}) + \\dots \\end{align} \\]Subtract \\(\\gamma \\, \\hat{v}_{t-1}\\) from \\(\\hat{v}_t\\), we have: \\[ \\hat{v}_t - \\gamma\\,\\hat{v}_{t-1} = (\\gamma+1) \\, g(\\hat{\\theta}_t) - \\gamma \\, g(\\hat{\\theta}_{t-1}) \\] We thus obtain: \\[ \\hat{v}_t = \\gamma \\, \\hat{v}_{t-1} + g(\\hat{\\theta}_t) + \\gamma\\left[ g(\\hat{\\theta}_t) - g(\\hat{\\theta}_{t-1}) \\right] \\]AdagradDuchi, Hazan, and Singer (2011) proposed a method, which adapts the learning rate according to the update rate of each parameter: for fast updating parameters, it uses smaller learning rate, and conversely, it uses larger learning rate. This mechanism is well suitable for sparse data (“An Overview of Gradient Descent Optimization Algorithms” 2016). \\[ \\begin{align} G_t &amp;= \\sum_{\\tau=1}^t g_{\\tau}^2 \\\\ \\theta_{t+1} &amp;= \\theta_t - \\frac{\\eta}{\\sqrt{G_t + \\epsilon}} \\odot g_t \\end{align} \\] where \\(\\odot\\) denotes the element-wise multiplication between two vectors. Here \\(g_t^2 = g_t \\odot g_t\\), is the element-wise square of \\(g_t\\). \\(G_t\\) is the sum of squares of all gradients of \\(\\theta\\) since the beginning of training. The small infinitesimal \\(\\epsilon\\) is adopted to avoid dividing by zero.AdadeltaTo avoid having an infinitesimal learning rate as that of Adagrad, Zeiler (2012) proposed to use a exponential moving average to estimate the expectation of \\(g_t^2\\) (denoted as \\(E[g^2]\\)), instead of summing up all squares of gradients.Also, it maintains the moving average for square of the term \\(\\Delta \\theta\\) (i.e., the update of parameters at \\(t\\), denoted as \\(E[\\Delta\\theta^2]\\)) to match the units of \\(E[g^2]_t\\). \\[ \\begin{align} E[g^2]_t &amp;= \\gamma \\,E[g^2]_{t-1} + (1-\\gamma) \\,g_t^2 \\\\ E[\\Delta\\theta^2]_t &amp;= \\gamma\\,E[\\Delta\\theta^2]_{t-1} + (1-\\gamma)\\,(\\Delta\\theta_t)^2 \\\\ \\mathop{RMS}[g]_t &amp;= \\sqrt{E[g^2]_t + \\epsilon} \\\\ RMS[\\Delta\\theta]_t &amp;= \\sqrt{E[\\Delta\\theta^2]_t + \\epsilon} \\\\ \\Delta \\theta_t &amp;= -\\frac{RMS[\\Delta\\theta]_{t-1}}{RMS[g]_{t-1}} \\odot g_t \\\\ \\theta_{t+1} &amp;= \\theta_t + \\eta \\,\\Delta \\theta_t \\end{align} \\] where the learning rate \\(\\eta\\) is chosen to be 1 in the original paper.RMSpropHinton proposed an unpublished SGD method based on Adagrad, also to avoid having an infinitesimal learning rate. (See Lecture 6e of his Coursera Class). \\[ \\begin{align} E[g^2]_t &amp;= \\gamma \\,E[g^2]_{t-1} + (1-\\gamma)\\,g_t^2 \\\\ \\theta_{t+1} &amp;= \\theta_t - \\frac{\\eta}{\\sqrt{E[g^2]_t + \\epsilon}} \\odot g_t \\end{align} \\] with the denominator exactly the same as Adadelta. The moving average decay factor \\(\\gamma\\) is suggested to be 0.9, while the initial learning rate \\(\\eta\\) is suggested to be 0.001.AdamIn addition to tracking the moving average of squares of gradients, Kingma and Ba (2017) proposed to also track the moving average of the gradients. \\[ \\begin{align} m_t &amp;= \\beta_1 m_{t-1} + (1-\\beta_1)\\,g_t \\\\ v_t &amp;= \\beta_2 v_{t-1} + (1-\\beta_2)\\,g_t^2 \\\\ \\end{align} \\] The authors also proposed to apply a zero-debias term to the moving average estimates: \\[ \\begin{align} \\hat{m}_t &amp;= \\frac{m_t}{1-\\beta_1^t} \\\\ \\hat{v}_t &amp;= \\frac{v_t}{1-\\beta_2^t} \\end{align} \\] Then update the parameters accordingly: \\[ \\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\odot \\hat{m}_t \\] By default, the hyper-parameters are suggested to be \\(\\beta_1 = 0.9\\), \\(\\beta_2 = 0.999\\), \\(\\epsilon = 10^{-8}\\).AdamaxAs a variant of Adam, Adamax uses \\(l_{\\infty}\\) norm instead of \\(l_2\\) norm for the denominator: \\[ \\begin{align} m_t &amp;= \\beta_1 m_{t-1} + (1-\\beta_1)\\,g_t \\\\ \\hat{m}_t &amp;= \\frac{m_t}{1-\\beta_1^t} \\\\ u_t &amp;= \\left( \\beta_2^{\\infty} \\, u_{t-1}^{\\infty} + (1-\\beta_2^{\\infty}) \\,\\left| g_t \\right|^{\\infty} \\right)^{1/\\infty} \\\\ &amp;= \\max\\left( \\beta_2\\,u_{t-1}, \\left| g_t \\right| \\right) \\\\ \\theta_{t+1} &amp;= \\theta_t - \\frac{\\eta}{u_t} \\, \\hat{m}_t \\end{align} \\] By default, the hyper-parameters are suggested to be \\(\\beta_1 = 0.9\\), \\(\\beta_2 = 0.999\\), \\(\\epsilon = 10^{-8}\\).NadamDozat (2016) incorporates Nesterov momentum into Adam.Comparing the momentum method: \\[ \\begin{align} v_t &amp;= \\gamma \\, v_{t-1} + g_t \\\\ \\theta_{t+1} &amp;= \\theta_t - \\eta \\,v_t \\\\ &amp;= \\theta_t - \\eta\\,(\\gamma\\,v_{t-1} + g_t) \\end{align} \\]with the “alternative form” of Nesterov Momentum method: \\[ \\begin{align} v_t &amp;= \\gamma \\, v_{t-1} + g_t \\\\ \\theta_{t+1} &amp;= \\theta_t - \\eta \\, (\\gamma \\,v_t + g_t) \\end{align} \\]It is clear that, we can obtain the Nesterov Momentum method by replacing \\(v_{t-1}\\) with \\(v_t\\) in the term \\(\\gamma\\,v_{t-1} + g_t\\) in the Momentum method.We then first seek to write Adam in such a form. Given that: \\[ \\begin{align} m_t &amp;= \\beta_1 m_{t-1} + (1-\\beta_1)\\,g_t \\\\ \\hat{m}_t &amp;= \\frac{m_t}{1-\\beta_1^t} \\end{align} \\] We have: \\[ \\begin{align} \\theta_{t+1} &amp;= \\theta_t - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\odot \\hat{m}_t \\\\ &amp;= \\theta_t - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\odot \\frac{m_t}{1-\\beta_1^t} \\\\ &amp;= \\theta_t - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\odot \\left( \\frac{\\beta_1}{1-\\beta_1^t} \\cdot m_{t-1} + \\frac{1-\\beta_1}{1-\\beta_1^t} \\cdot g_t \\right) \\end{align} \\]If we consider \\(m_{t-1}/(1-\\beta_1^t) \\approx m_{t-1}/(1-\\beta_1^{t-1})\\), which is true when \\(t \\to \\infty\\), we can obtain: \\[ \\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{\\hat{v}_t}+\\epsilon}\\odot \\left( \\beta_1\\,\\hat{m}_{t-1} + \\frac{1-\\beta_1}{1-\\beta_1^t} \\cdot g_t \\right) \\] By replacing \\(\\hat{m}_{t-1}\\) with \\(\\hat{m}_t\\) as analogue to the Nesterov Momentum method, we finally obtain: \\[ \\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{\\hat{v}_t}+\\epsilon}\\odot \\left( \\beta_1\\,\\hat{m}_t + \\frac{1-\\beta_1}{1-\\beta_1^t} \\cdot g_t \\right) \\]Note the \\(\\hat{v}_t\\) is not changed in Nadam. In summary, we get the following update equations for Nadam: \\[ \\begin{align} v_t &amp;= \\beta_2 v_{t-1} + (1-\\beta_2)\\,g_t^2 \\\\ \\hat{v}_t &amp;= \\frac{v_t}{1-\\beta_2^t} \\\\ m_t &amp;= \\beta_1 m_{t-1} + (1-\\beta_1)\\,g_t \\\\ \\hat{m}_t &amp;= \\frac{m_t}{1-\\beta_1^t} \\\\ \\theta_{t+1} &amp;= \\theta_t - \\frac{\\eta}{\\sqrt{\\hat{v}_t}+\\epsilon}\\odot \\left( \\beta_1\\,\\hat{m}_t + \\frac{1-\\beta_1}{1-\\beta_1^t} \\cdot g_t \\right) \\end{align} \\]AMSGradThe aggressive moving average strategy used by Adam may cause it hard to converge in some problems (Reddi, Kale, and Kumar 2019). AMSGrad is thus proposed as a modified version of Adam: \\[ \\begin{align} m_t &amp;= \\beta_1 m_{t-1} + (1-\\beta_1)\\,g_t \\\\ v_t &amp;= \\beta_2 v_{t-1} + (1-\\beta_2)\\,g_t^2 \\\\ \\hat{v}_t &amp;= \\max(\\hat{v}_{t-1}, v_t) \\\\ \\theta_{t+1} &amp;= \\theta_t - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\odot m_t \\end{align} \\] The authors discarded the bias correction term \\(1 / (1 - \\beta_1^t)\\) and \\(1 / (1 - \\beta_2^t)\\) for simplicity. But in practice, some implementations may still consider this correction term, resulting in: \\[ \\begin{align} m_t &amp;= \\beta_1 m_{t-1} + (1-\\beta_1)\\,g_t \\\\ v_t &amp;= \\beta_2 v_{t-1} + (1-\\beta_2)\\,g_t^2 \\\\ u_t &amp;= \\max(u_{t-1}, v_t) \\\\ \\hat{m}_t &amp;= \\frac{m_t}{1 - \\beta_1^t} \\\\ \\hat{u}_t &amp;= \\frac{u_t}{1 - \\beta_2^t} \\\\ \\theta_{t+1} &amp;= \\theta_t - \\frac{\\eta}{\\sqrt{\\hat{u}_t} + \\epsilon} \\odot \\hat{m}_t \\end{align} \\]References“An Overview of Gradient Descent Optimization Algorithms.” 2016. Sebastian Ruder. https://ruder.io/optimizing-gradient-descent/.Dozat, Timothy. 2016. “Incorporating Nesterov Momentum into Adam,” 4.Duchi, John, Elad Hazan, and Yoram Singer. 2011. “Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.” Journal of Machine Learning Research 12 (61): 2121–59.Kingma, Diederik P., and Jimmy Ba. 2017. “Adam: A Method for Stochastic Optimization.” arXiv:1412.6980 [Cs], January. http://arxiv.org/abs/1412.6980.Reddi, Sashank J., Satyen Kale, and Sanjiv Kumar. 2019. “On the Convergence of Adam and Beyond.” arXiv:1904.09237 [Cs, Math, Stat], April. http://arxiv.org/abs/1904.09237.Zeiler, Matthew D. 2012. “ADADELTA: An Adaptive Learning Rate Method.” arXiv:1212.5701 [Cs], December. http://arxiv.org/abs/1212.5701.“比Momentum更快：揭开Nesterov Accelerated Gradient的真面目.” n.d. 知乎专栏. https://zhuanlan.zhihu.com/p/22810533.","categories":[{"name":"Deep_Learning","slug":"Deep-Learning","permalink":"https://wiki.haowen-xu.com/categories/Deep-Learning/"},{"name":"Optimization","slug":"Deep-Learning/Optimization","permalink":"https://wiki.haowen-xu.com/categories/Deep-Learning/Optimization/"}],"tags":[]},{"title":"Visualizing High Dimensional Space","slug":"Deep_Learning/Evaluation/Visualizing_High_Dimensional_Space","date":"2020-05-22T02:46:41.000Z","updated":"2020-06-01T12:15:59.000Z","comments":true,"path":"Deep_Learning/Evaluation/Visualizing_High_Dimensional_Space/","link":"","permalink":"https://wiki.haowen-xu.com/Deep_Learning/Evaluation/Visualizing_High_Dimensional_Space/","excerpt":"","text":"Orthogonal ProjectionIzmailov et al. (2018) proposed to project weight tensors \\(w\\) into 2d plane (i.e., obtain the projector \\(w(x,y)\\)), by constructing the orthogonal basis \\((\\hat{u},\\hat{v})\\) from three given weight tensors \\((w_1, w_2, w_3)\\): \\[ \\begin{align} u &amp;= w_2 - w_1 \\\\ v &amp;= (w_3 - w_1) - \\frac{\\left&lt;w_3-w_1, w_2-w_1\\right&gt;}{\\left\\| w_2-w_1 \\right\\|^2} \\cdot (w_2 - w_1) \\\\ \\hat{u} &amp;= \\frac{u}{\\left\\| u \\right\\|} \\\\ \\hat{v} &amp;= \\frac{v}{\\left\\| v \\right\\|} \\\\ w(x,y) &amp;= w_1 + x\\cdot \\hat{u} + y \\cdot \\hat{v} \\end{align} \\]ReferencesIzmailov, Pavel, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wilson. 2018. “Averaging Weights Leads to Wider Optima and Better Generalization.” arXiv Preprint arXiv:1803.05407.","categories":[{"name":"Deep_Learning","slug":"Deep-Learning","permalink":"https://wiki.haowen-xu.com/categories/Deep-Learning/"},{"name":"Evaluation","slug":"Deep-Learning/Evaluation","permalink":"https://wiki.haowen-xu.com/categories/Deep-Learning/Evaluation/"}],"tags":[]},{"title":"Loss Surface and Generalization","slug":"Deep_Learning/Optimization/Loss_Surface_and_Generalization","date":"2020-05-21T00:13:22.000Z","updated":"2020-06-01T12:15:59.008Z","comments":true,"path":"Deep_Learning/Optimization/Loss_Surface_and_Generalization/","link":"","permalink":"https://wiki.haowen-xu.com/Deep_Learning/Optimization/Loss_Surface_and_Generalization/","excerpt":"","text":"Stochastic Weight AveragingIzmailov et al. (2018) suggestedFigure 1: Stochastic Weighted Averaging Illustration (view pdf)The visualization method of the above figure can be found at Orthogonal Projection.ReferencesIzmailov, Pavel, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wilson. 2018. “Averaging Weights Leads to Wider Optima and Better Generalization.” arXiv Preprint arXiv:1803.05407.","categories":[{"name":"Deep_Learning","slug":"Deep-Learning","permalink":"https://wiki.haowen-xu.com/categories/Deep-Learning/"},{"name":"Optimization","slug":"Deep-Learning/Optimization","permalink":"https://wiki.haowen-xu.com/categories/Deep-Learning/Optimization/"}],"tags":[]},{"title":"Overview","slug":"Deep_Learning/Generative_Adversarial_Nets/Overview","date":"2020-03-21T04:07:29.000Z","updated":"2020-06-01T12:15:59.000Z","comments":true,"path":"Deep_Learning/Generative_Adversarial_Nets/Overview/","link":"","permalink":"https://wiki.haowen-xu.com/Deep_Learning/Generative_Adversarial_Nets/Overview/","excerpt":"","text":"Original GANGoodfellow et al. (2014) proposed the following first GAN architecture:Prior: \\(p_g(\\mathbf{z}) = \\mathcal{N}(\\mathbf{0},\\mathbf{I})\\)Generator: \\(G(\\mathbf{z})\\)Discriminator: \\(D(\\mathbf{x}) \\in [0, 1]\\)The overall loss from their theory: \\[ \\mathcal{L} = \\min_G \\max_D \\left\\{ \\mathbb{E}_{p_d(\\mathbf{x})}\\left[ \\log D(\\mathbf{x}) \\right] + \\mathbb{E}_{p_g(\\mathbf{z})}\\left[ \\log(1 - D(G(\\mathbf{z}))) \\right] \\right\\} \\] The discriminator loss (to maximize): \\[ \\mathcal{L}_D = \\mathbb{E}_{p_d(\\mathbf{x})} \\left[ \\log D(\\mathbf{x}) \\right] + \\mathbb{E}_{p_g(\\mathbf{z})}\\left[ \\log(1 - D(G(\\mathbf{z}))) \\right] \\]The theoretical generator loss (to minimize): \\[ \\mathcal{L}_G = \\mathbb{E}_{p_g(\\mathbf{z})}\\left[ \\log(1 - D(G(\\mathbf{z}))) \\right] \\] The actual generator loss (to maximize) in experiments, to avoid saturation in the early stage of training: \\[ \\mathcal{L}_G = \\mathbb{E}_{p_g(\\mathbf{z})}\\left[ \\log(D(G(\\mathbf{z}))) \\right] \\]GAN Training AlgorithmThe most widely adopted GAN training algorithm, which is proposed in this work, alternates between training the discriminator and the generator. That is, in each training iteration:Repeat for n_critics iterationsSample \\(\\mathbf{x}^{(1)}, \\dots, \\mathbf{x}^{(b)}\\) from \\(p_d(\\mathbf{x})\\), \\(\\mathbf{z}^{(1)}, \\dots, \\mathbf{z}^{(b)}\\) from \\(p_g(\\mathbf{z})\\).Update the discriminator by: \\[ \\theta_D = \\theta_D + \\eta \\, \\nabla_{\\theta_D}\\left( \\frac{1}{b}\\sum_{i=1}^b \\left[ \\log D(\\mathbf{x}^{(i)}) \\right] + \\frac{1}{b}\\sum_{i=1}^b \\left[ \\log(1 - D(G(\\mathbf{z}^{(i)}))) \\right] \\right) \\]Sample \\(\\mathbf{z}^{(1)}, \\dots, \\mathbf{z}^{(b)}\\) from \\(p_g(\\mathbf{z})\\).Update the generator by: \\[ \\theta_G = \\theta_G - \\eta\\,\\nabla_{\\theta_G}\\left( \\frac{1}{b} \\sum_{i=1}^b \\log(1 - D(G(\\mathbf{z}^{(i)}))) \\right) \\] or alternatively, \\[ \\theta_G = \\theta_G + \\eta\\,\\nabla_{\\theta_G}\\left( \\frac{1}{b} \\sum_{i=1}^b \\log(D(G(\\mathbf{z}^{(i)}))) \\right) \\]ReferencesGoodfellow, Ian, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. “Generative Adversarial Nets.” In Advances in Neural Information Processing Systems, 2672–80.","categories":[{"name":"Deep_Learning","slug":"Deep-Learning","permalink":"https://wiki.haowen-xu.com/categories/Deep-Learning/"},{"name":"Generative_Adversarial_Nets","slug":"Deep-Learning/Generative-Adversarial-Nets","permalink":"https://wiki.haowen-xu.com/categories/Deep-Learning/Generative-Adversarial-Nets/"}],"tags":[]},{"title":"Evaluation Metrics","slug":"Deep_Learning/Evaluation/Evaluation_Metrics","date":"2020-03-20T18:07:18.000Z","updated":"2020-06-01T12:15:59.000Z","comments":true,"path":"Deep_Learning/Evaluation/Evaluation_Metrics/","link":"","permalink":"https://wiki.haowen-xu.com/Deep_Learning/Evaluation/Evaluation_Metrics/","excerpt":"","text":"LikelihoodNegative Log-LikelihoodThe negative log-likelihood (NLL) for \\(p_{\\theta}(\\mathbf{x})\\) is defined as:\\[ \\begin{align} \\text{NLL} &amp;= \\mathbb{E}_{p_d(\\mathbf{x})} \\left[ -\\log p_{\\theta}(\\mathbf{x}) \\right] \\end{align} \\]Find the Original NLL using Scaled DataIf the original data \\(\\mathbf{x}\\) is \\(k\\)-dimensional ,and is scaled by \\(\\frac{1}{\\sigma}\\) at each of its dimensions, such that the data fed into the model is \\(\\tilde{\\mathbf{x}} = \\frac{1}{\\sigma} \\mathbf{x}\\), then: \\[ \\begin{align} p_d(\\mathbf{x}) &amp;= \\tilde{p}_d(\\tilde{\\mathbf{x}}) \\left| \\det\\left( \\frac{\\mathrm{d}\\tilde{\\mathbf{x}}}{\\mathrm{d}\\mathbf{x}} \\right) \\right| = \\tilde{p}_d(\\tilde{\\mathbf{x}})\\left| \\det\\left( \\frac{\\mathrm{d}(\\mathbf{x} / \\sigma)}{\\mathrm{d}\\mathbf{x}} \\right) \\right| = \\frac{1}{\\sigma^k}\\,\\tilde{p}_d(\\tilde{\\mathbf{x}}) \\\\ p_{\\theta}(\\mathbf{x}) &amp;= \\frac{1}{\\sigma^k}\\, \\tilde{p}_{\\theta}(\\tilde{\\mathbf{x}}) \\end{align} \\] Thus the computed NLL for \\(\\mathbf{x}\\) and \\(\\tilde{\\mathbf{x}}\\) has the following relationship: \\[ \\begin{align} \\text{NLL} &amp;= \\mathbb{E}_{p_d(\\mathbf{x})} \\left[ -\\log p_{\\theta}(\\mathbf{x}) \\right] \\\\ &amp;= -\\int p_d(\\mathbf{x}) \\log p_{\\theta}(\\mathbf{x})\\,\\mathrm{d}\\mathbf{x} \\\\ &amp;= -\\int \\frac{1}{\\sigma^k}\\,\\tilde{p}_{d}(\\tilde{\\mathbf{x}}) \\log \\left( \\frac{1}{\\sigma^k}\\, \\tilde{p}_{\\theta}(\\tilde{\\mathbf{x}}) \\right)\\left| \\det\\left( \\frac{\\mathrm{d}\\mathbf{x}}{\\mathrm{d}\\tilde{\\mathbf{x}}} \\right) \\right|\\mathrm{d}\\tilde{\\mathbf{x}} \\\\ &amp;= -\\int \\tilde{p}_{d}(\\tilde{\\mathbf{x}}) \\left[ \\log \\tilde{p}_{\\theta}(\\tilde{\\mathbf{x}}) - k\\log \\sigma \\right]\\mathrm{d}\\tilde{\\mathbf{x}} \\\\ &amp;= \\mathbb{E}_{\\tilde{p}_d(\\mathbf{x})} \\left[ -\\log \\tilde{p}_{\\theta}(\\mathbf{x}) \\right] + k\\log \\sigma \\\\ &amp;= \\widetilde{NLL} + k\\log\\sigma \\end{align} \\]Continuous NLL as an Upper-Bound of Discrete NLLTo train a continuous model upon discrete data (e.g., images), one may add a uniform noise to the data, and obtain an upper-bound of the discrete data NLL with the augmented data.For pixel integer-valued \\(\\mathbf{x}\\) ranging from 0 to 255, adding a uniform noise \\(\\mathbf{u} \\sim \\mathcal{U}[0, 1)\\), such that \\(\\tilde{\\mathbf{x}} = \\mathbf{x} + \\mathbf{u}\\), we have (Theis, Oord, and Bethge 2015): \\[ \\begin{align} -\\int \\tilde{p}_d(\\tilde{\\mathbf{x}}) \\log \\tilde{p}_{\\theta}(\\tilde{\\mathbf{x}}) \\,\\mathrm{d}\\tilde{\\mathbf{x}} &amp;= -\\sum_{\\mathbf{x}} P_d(\\mathbf{x}) \\int \\log \\tilde{p}_{\\theta}(\\mathbf{x} + \\mathbf{u}) \\,\\mathrm{d}\\mathbf{u} \\\\ &amp;\\geq -\\sum_{\\mathbf{x}} P_d(\\mathbf{x}) \\log \\int \\tilde{p}_{\\theta}(\\mathbf{x} + \\mathbf{u}) \\,\\mathrm{d}\\mathbf{u} \\\\ \\\\ &amp;= -\\sum_{\\mathbf{x}} P_d(\\mathbf{x}) \\log P_{\\theta}(\\mathbf{x}) \\\\ \\end{align} \\] where we define the probability of the true discrete data to be: \\[ P_{\\theta}(\\mathbf{x}) = \\int \\tilde{p}_{\\theta}(\\mathbf{x} + \\mathbf{u}) \\,\\mathrm{d}\\mathbf{u} \\] That is to say, the NLL of the augmented continuous random variable \\(\\tilde{\\mathbf{x}}\\) can serve as an upper-bound as the true discrete data NLL.Image QualityROC and AUC(Davis and Goadrich 2006)There is a one-to-one correspondence between the points on ROC and AUC curves.A curve dominates the ROC (fpr-tpr curve) \\(\\Leftrightarrow\\) dominates the AUC (recall-precision curve).Interpolation between two points \\(A\\) and \\(B\\):On ROC: linear interpolation.On AUC: \\[ \\left( \\frac{TP_A + x}{\\text{Total Pos}}, \\frac{TP_A + x}{TP_A + x + FP_A + \\frac{FP_B - FP_A}{TP_B - TP_A} x} \\right) \\]Compute the area: include the interpolation and use composite trapezoidal method.Incorrect interpoluation for computing AUC-PR will cause over-estimate.Optimize Area under ROC and AUC curves: not exactly the same. (especially when not one algorithm dominates the curve?)ReferencesDavis, Jesse, and Mark Goadrich. 2006. “The Relationship Between Precision-Recall and ROC Curves.” In Proceedings of the 23rd International Conference on Machine Learning - ICML ’06, 233–40. Pittsburgh, Pennsylvania: ACM Press. https://doi.org/10.1145/1143844.1143874.Theis, Lucas, Aäron van den Oord, and Matthias Bethge. 2015. “A Note on the Evaluation of Generative Models.” arXiv Preprint arXiv:1511.01844.","categories":[{"name":"Deep_Learning","slug":"Deep-Learning","permalink":"https://wiki.haowen-xu.com/categories/Deep-Learning/"},{"name":"Evaluation","slug":"Deep-Learning/Evaluation","permalink":"https://wiki.haowen-xu.com/categories/Deep-Learning/Evaluation/"}],"tags":[]},{"title":"Energy GAN","slug":"Deep_Learning/Generative_Adversarial_Nets/Energy_GAN","date":"2020-03-19T20:46:25.000Z","updated":"2020-06-01T12:15:59.000Z","comments":true,"path":"Deep_Learning/Generative_Adversarial_Nets/Energy_GAN/","link":"","permalink":"https://wiki.haowen-xu.com/Deep_Learning/Generative_Adversarial_Nets/Energy_GAN/","excerpt":"","text":"OverviewGiven the discriminator \\(E_{\\theta}(\\mathbf{x})\\), a density function can be derived as: \\[ \\begin{align} p_{\\theta}(\\mathbf{x}) &amp;= \\frac{1}{Z_{\\theta}} e^{-E_{\\theta}(\\mathbf{x})} \\\\ Z_{\\theta} &amp;= \\int e^{-E_{\\theta}(\\mathbf{x})}\\,\\mathrm{d}\\mathbf{x} \\end{align} \\]Maximum Entropy Generators for Energy-Based ModelsKumar et al. (2019) proposed the following architecture:Prior: \\(p_z(\\mathbf{z})\\)Generator: \\(G_{\\omega}(\\mathbf{z})\\)Discriminator: \\(E_{\\theta}(\\mathbf{x}) \\in (-\\infty, \\infty)\\), the energy functionThe density function: \\(p_{\\theta}(\\mathbf{x}) = \\frac{1}{Z_{\\theta}} e^{-E_{\\theta}(\\mathbf{x})}\\)Discriminator for the mutual information estimator: \\(T_{\\phi}(\\mathbf{x},\\mathbf{z}) \\in (-\\infty,\\infty)\\)The discriminator loss (to minimize): \\[ \\begin{align} \\mathcal{L}_E &amp;= \\mathbb{E}_{p_d(\\mathbf{x})}\\left[ E_{\\theta}(\\mathbf{x}) \\right] - \\mathbb{E}_{p_G(\\mathbf{x})}\\left[ E_{\\theta}(\\mathbf{x}) \\right] + \\Omega \\\\ \\Omega &amp;= \\lambda\\,\\mathbb{E}_{p_d(\\mathbf{x})} \\left[ \\left\\| \\nabla_{\\mathbf{x}} E_{\\theta}(\\mathbf{x}) \\right\\|^2 \\right] \\end{align} \\] The generator loss (to minimize) \\[ \\begin{align} \\mathcal{L}_G &amp;= \\mathbb{E}_{p_z(\\mathbf{z})}\\left[ E_{\\theta}(G_{\\omega}(\\mathbf{z})) \\right] - H_{p_G}[X] \\\\ &amp;= \\mathbb{E}_{p_z(\\mathbf{z})}\\left[ E_{\\theta}(G_{\\omega}(\\mathbf{z})) \\right] - I_{p_G}(X;Z) \\end{align} \\] where \\(H_{p_G}[X] = I_{p_G}(X;Z) + H(G_{\\omega}(Z)|Z)\\), and \\(H(G_{\\omega}(Z)|Z) \\equiv 0\\) since \\(G_{\\omega}(\\mathbf{z})\\) is a deterministic mapper.The mutual information \\(I_{p_G}(X;Z)\\) is estimated via the Deep InfoMax estimator by Kumar et al. (2019), formulated as: \\[ \\begin{align} I_{p_G}(X;Z) &amp;= \\mathbb{E}_{p_z(\\mathbf{z})}\\left[ -\\text{sp}(-T_{\\phi}(G_{\\omega}(\\mathbf{z}),\\mathbf{z})) \\right] - \\mathbb{E}_{p_z(\\mathbf{z})\\times\\tilde{p}_z(\\tilde{\\mathbf{z}})}\\left[ \\text{sp}(T_{\\phi}(G_{\\omega}(\\mathbf{z}),\\tilde{\\mathbf{z}})) \\right] \\\\ &amp;= \\mathbb{E}_{p_z(\\mathbf{z})}\\left[ \\log \\sigma(T_{\\phi}(G_{\\omega}(\\mathbf{z}),\\mathbf{z})) \\right] - \\mathbb{E}_{p_z(\\mathbf{z})\\times\\tilde{p}_z(\\tilde{\\mathbf{z}})}\\left[ \\log\\left(1 - \\sigma(T_{\\phi}(G_{\\omega}(\\mathbf{z}),\\tilde{\\mathbf{z}}))\\right) \\right] \\end{align} \\]where \\(\\text{sp}(a) = \\log(1 + e^a)\\) is the SoftPlus function, and \\(\\sigma(a) = \\frac{1}{1 + e^{-a}}\\) is the sigmoid function.Training AlgorithmKumar et al. (2019) proposed the following training algorithm:Repeat for n_critics IterationsSample \\(\\mathbf{x}^{(1)}, \\dots, \\mathbf{x}^{(b)}\\) from \\(p_d(\\mathbf{x})\\), \\(\\mathbf{z}^{(1)}, \\dots, \\mathbf{z}^{(b)}\\) from \\(p_z(\\mathbf{z})\\).Obtain \\(\\tilde{\\mathbf{x}}^{(i)} = G_{\\omega}(\\mathbf{z}^{(i)})\\).Calculate: \\[ \\mathcal{L}_E = \\frac{1}{b} \\left[ \\sum_{i=1}^b E_{\\theta}(\\mathbf{x}^{(i)}) - \\sum_{i=1}^b E_{\\theta}(\\tilde{\\mathbf{x}}^{(i)}) + \\lambda\\sum_{i=1}^b \\left\\| \\nabla_{\\mathbf{x}} E_{\\theta}(\\mathbf{x}^{(i)}) \\right\\|^2 \\right] \\]Gradient descent: \\[ \\theta^{t+1} = \\theta^{t} - \\eta \\, \\nabla_{\\theta} \\mathcal{L}_E \\]Sample \\(\\mathbf{z}^{(1)}, \\dots, \\mathbf{z}^{(b)}\\) from \\(p_z(\\mathbf{z})\\).Per-dimensional shuffle of \\(\\mathbf{z}\\), yielding \\(\\tilde{\\mathbf{z}}^{(1)}, \\dots, \\tilde{\\mathbf{z}}^{(b)}\\).(Is this really better than re-sampling from the prior \\(p_z(\\mathbf{z})\\)?)Obtain \\(\\tilde{\\mathbf{x}}^{(i)} = G_{\\omega}(\\mathbf{z}^{(i)})\\).Calculate: \\[ \\begin{align} \\mathcal{L}_H &amp;= \\frac{1}{b} \\sum_{i=1}^b \\left[ \\log \\sigma(T_{\\phi}(\\tilde{\\mathbf{x}}^{(i)},\\mathbf{z}^{(i)})) - \\log\\left(1 - \\sigma(T_{\\phi}(\\tilde{\\mathbf{x}}^{(i)},\\tilde{\\mathbf{z}}^{(i)}))\\right) \\right] \\\\ \\mathcal{L}_G &amp;= \\frac{1}{b} \\sum_{i=1}^b \\left[ E_{\\theta}(\\tilde{\\mathbf{x}}^{(i)}) \\right] - \\mathcal{L}_H \\end{align} \\]Gradient descent: \\[ \\begin{align} \\omega^{t+1} &amp;= \\omega^{t} - \\eta \\, \\nabla_{\\omega} \\mathcal{L}_G \\\\ \\phi^{t+1} &amp;= \\phi^{t} - \\eta \\, \\nabla_{\\phi} \\mathcal{L}_H \\\\ \\end{align} \\]In summary, this algorithm:Minimize \\(\\theta\\) w.r.t. \\(\\mathcal{L}_E\\) =&gt;Minimizes \\(E_{\\theta}\\) in the discriminator loss \\(\\mathcal{L}_E\\).Minimize \\(\\phi\\) w.r.t. \\(L_H\\) =&gt;Minimizes \\(T_{\\phi}\\) in the mutual information regularizer \\(I_{p_G}(X;Z)\\).Minimize \\(\\omega\\) w.r.t. \\(L_G\\) =&gt;Maximizes \\(G_{\\omega}\\) in the mutual information regularizer \\(I_{p_G}(X;Z)\\);Minimizes \\(G_{\\omega}\\) in the generator loss \\(\\mathbb{E}_{p_z(\\mathbf{z})}\\left[ E_{\\theta}(G_{\\omega}(\\mathbf{z})) \\right]\\).Latent Space MCMCKumar et al. (2019) also proposed an MCMC method to refine the \\(\\mathbf{z}\\) samples obtained from the prior \\(p_z(\\mathbf{z})\\), according to the energy function on \\(\\mathbf{z}\\), derived as: \\[ E(\\mathbf{z}) = E_{\\theta}(G_{\\omega}(\\mathbf{z})) \\] Then Metropolis-adjusted Langevin algorithm is adopted to sample \\(\\mathbf{z}\\):Langevin dynamics: \\[ \\mathbf{z}^{(t+1)} = \\mathbf{z}^{(t)} - \\alpha \\frac{\\partial E_{\\theta}(G_{\\omega}(\\mathbf{z}^{(t)}))}{\\partial \\mathbf{z}^{(t)}} + \\epsilon \\sqrt{2 * \\alpha} \\] where \\(\\epsilon \\sim \\mathcal{N}(\\mathbf{0},\\mathbf{I})\\).Metropolis-Hastings Algorithm: \\[ \\begin{align} r &amp;= \\min\\left\\{ 1, \\frac{p(\\mathbf{z}^{(t+1)})}{p(\\mathbf{z}^{(t)})} \\cdot \\frac{q(\\mathbf{z}^{(t)}|\\mathbf{z}^{(t+1)})}{q(\\mathbf{z}^{(t+1)}|\\mathbf{z}^{(t)})} \\right\\} \\\\ p(\\mathbf{z}^{(t)}) &amp;\\propto \\exp\\left\\{ -E_{\\theta}(G_{\\omega}(\\mathbf{z}^{(t)})) \\right\\} \\\\ q(\\mathbf{z}^{(t+1)}|\\mathbf{z}^{(t)}) &amp;\\propto \\exp\\left( -\\frac{1}{4 \\alpha}\\left\\| \\mathbf{z}^{(t+1)} - \\mathbf{z}^{(t)} + \\alpha \\frac{\\partial E_{\\theta}(G_{\\omega}(\\mathbf{z}^{(t)}))}{\\partial \\mathbf{z}^{(t)}} \\right\\|^2_2 \\right) \\end{align} \\]ReferencesKumar, Rithesh, Anirudh Goyal, Aaron Courville, and Yoshua Bengio. 2019. “Maximum Entropy Generators for Energy-Based Models.” arXiv:1901.08508 [Cs, Stat], January. http://arxiv.org/abs/1901.08508.","categories":[{"name":"Deep_Learning","slug":"Deep-Learning","permalink":"https://wiki.haowen-xu.com/categories/Deep-Learning/"},{"name":"Generative_Adversarial_Nets","slug":"Deep-Learning/Generative-Adversarial-Nets","permalink":"https://wiki.haowen-xu.com/categories/Deep-Learning/Generative-Adversarial-Nets/"}],"tags":[]},{"title":"Gradient Tricks","slug":"Deep_Learning/Optimization/Gradient_Tricks","date":"2020-03-18T17:28:16.000Z","updated":"2020-06-01T12:15:59.008Z","comments":true,"path":"Deep_Learning/Optimization/Gradient_Tricks/","link":"","permalink":"https://wiki.haowen-xu.com/Deep_Learning/Optimization/Gradient_Tricks/","excerpt":"","text":"ClippingAdaptive ClippingTo avoid the optimizer putting too much attention on just one of the loss components (or adversarial losses), adaptive clipping can be adopted (Belghazi et al. 2018), to match the gradient scales of different losses.For example, Belghazi et al. (2018) adapts \\(g_m\\) to match the scale of \\(g_u\\), where \\(g_u\\) is the main loss gradient, and \\(g_m\\) is the gradient of the mutual information regularizer: \\[ \\tilde{g}_m = \\min\\left( \\|g_u\\|, \\|g_m\\| \\right) \\frac{g_m}{\\|g_m\\|} \\]ReferencesBelghazi, Mohamed Ishmael, Aristide Baratin, Sai Rajeswar, Sherjil Ozair, Yoshua Bengio, Aaron Courville, and R. Devon Hjelm. 2018. “Mine: Mutual Information Neural Estimation.” arXiv Preprint arXiv:1801.04062.","categories":[{"name":"Deep_Learning","slug":"Deep-Learning","permalink":"https://wiki.haowen-xu.com/categories/Deep-Learning/"},{"name":"Optimization","slug":"Deep-Learning/Optimization","permalink":"https://wiki.haowen-xu.com/categories/Deep-Learning/Optimization/"}],"tags":[]},{"title":"f-GAN","slug":"Deep_Learning/Generative_Adversarial_Nets/f-GAN","date":"2020-03-16T22:58:04.000Z","updated":"2020-06-01T12:15:59.000Z","comments":true,"path":"Deep_Learning/Generative_Adversarial_Nets/f-GAN/","link":"","permalink":"https://wiki.haowen-xu.com/Deep_Learning/Generative_Adversarial_Nets/f-GAN/","excerpt":"","text":"Proposed by Nowozin, Cseke, and Tomioka (2016), f-GAN generalizes the KL-divergence of the original GAN framework (Goodfellow et al. 2014) to the f-divergence.Theoretical Frameworkf-Divergence\\[ D_{f}(P \\| Q) = \\int q(\\mathbf{x}) \\,f\\left( \\frac{p(\\mathbf{x})}{q(\\mathbf{x})} \\right)\\,\\mathrm{d}\\mathbf{x} \\]where \\(f: \\mathbb{R}^+ \\to \\mathbb{R}\\) is a convex, lower-semicontinuous function satisfying \\(f(\\mathbf{1})=0\\).Fenchel conjugateThe Fenchel conjugate of \\(f(u)\\) is: \\[ f^*(t) = \\sup_{u \\in \\text{dom}_f}\\left\\{ ut - f(u) \\right\\} \\] Since \\(f\\) is a convex and lower-semicontinuous function, \\(f^{**} = f\\), therefore: \\[ f(u) = \\sup_{t \\in \\text{dom}_{f^*}} \\left\\{ tu - f^*(t) \\right\\} \\] And further we have: \\[ D_{f}(P \\| Q) = \\int q(\\mathbf{x}) \\, \\sup_{t \\in \\text{dom}_{f^*}} \\left\\{ t\\,\\frac{p(\\mathbf{x})}{q(\\mathbf{x})} - f^*(t) \\right\\} \\,\\mathrm{d}\\mathbf{x} \\]Variational Estimation of f-DivergenceUsing a function \\(T: \\mathcal{X} \\to \\mathbb{R}\\) of family \\(\\mathcal{T}\\), we can can obtain the following lower-bound (Nowozin, Cseke, and Tomioka 2016): \\[ \\begin{align} D_{f}(P \\| Q) &amp;= \\int q(\\mathbf{x}) \\, \\sup_{t \\in \\text{dom}_{f^*}} \\left\\{ t\\,\\frac{p(\\mathbf{x})}{q(\\mathbf{x})} - f^*(t) \\right\\} \\,\\mathrm{d}\\mathbf{x} \\\\ &amp;\\geq \\sup_{T \\in \\mathcal{T}} \\left\\{ \\int q(\\mathbf{x}) \\left( T(\\mathbf{x})\\,\\frac{p(\\mathbf{x})}{q(\\mathbf{x})} - f^*(T(\\mathbf{x})) \\right) \\,\\mathrm{d}\\mathbf{x} \\right\\} \\\\ &amp;= \\sup_{T \\in \\mathcal{T}} \\left\\{ \\int p(\\mathbf{x})\\,T(\\mathbf{x}) \\,\\mathrm{d}\\mathbf{x} - \\int q(\\mathbf{x}) \\,f^*(T(\\mathbf{x})) \\,\\mathrm{d}\\mathbf{x} \\right\\} \\\\ &amp;= \\sup_{T \\in \\mathcal{T}} \\left\\{ \\mathbb{E}_{p(\\mathbf{x})}\\left[ T(\\mathbf{x}) \\right] - \\mathbb{E}_{q(\\mathbf{x})} \\left[ f^*(T(\\mathbf{x})) \\right] \\right\\} \\end{align} \\] It is a lower-bound, because the family \\(\\mathcal{T}\\) might not cover all of the possible functions. In fact, the bound is tight for \\[ T^*(\\mathbf{x}) = f'\\left(\\frac{p(\\mathbf{x})}{q(\\mathbf{x})}\\right) \\] where \\(f'\\) is the first derivative of \\(f\\).Variational Divergence MinimizationUsing the variational lower-bound of the f-divergence \\(D_{f}(P \\| Q)\\), we can obtain the objective \\(\\mathcal{F}(\\theta,\\omega)\\), for the generator \\(q_{\\theta}(\\mathbf{x})\\), which should be minimized w.r.t. \\(\\theta\\), and maximized w.r.t. \\(\\omega\\):\\[ \\mathcal{F}(\\theta,\\omega) = \\mathbb{E}_{p_d(\\mathbf{x})}\\left[ T_{\\omega}(\\mathbf{x}) \\right] - \\mathbb{E}_{q_{\\theta}(\\mathbf{x})}\\left[ f^*(T_{\\omega}(\\mathbf{x})) \\right] \\]Note: \\(\\mathcal{F}(\\theta,\\omega)\\) is just a lower-bound of \\(D_{f}(P \\| Q)\\). Minimizing a lower-bound may not lead to optimal result (commented by me).To ensure \\(T_{\\omega}\\) match the domain of \\(f^*\\), it can be further reparameterized as: \\[ T_{\\omega}(\\mathbf{x}) = g_f(V_{\\omega}(\\mathbf{x})) \\] where \\(V_{\\omega}: \\mathcal{X} \\to \\mathbb{R}\\), and \\(g_f: \\mathbb{R} \\to \\text{dom}_{f^*}\\) being the activation function chosen according to \\(f\\). Then the above objective can be rewritten as: \\[ \\mathcal{F}(\\theta,\\omega) = \\mathbb{E}_{p_d(\\mathbf{x})}\\left[ g_f(V_{\\omega}(\\mathbf{x})) \\right] + \\mathbb{E}_{q_{\\theta}(\\mathbf{x})}\\left[ -f^*(g_f(V_{\\omega}(\\mathbf{x}))) \\right] \\]Single-Step Gradient MethodInstead of alternate between optimizing \\(\\omega\\) and \\(\\theta\\) in different steps, Nowozin, Cseke, and Tomioka (2016) proposed to optimize these two set of parameters in a single step by: \\[ \\begin{align} \\omega^{t+1} &amp;= \\omega^{t} + \\eta \\,\\nabla_{\\omega} \\mathcal{F}(\\theta^t,\\omega^t) \\\\ &amp;= \\omega^{t} + \\eta \\,\\nabla_{\\omega} \\left( \\mathbb{E}_{p_d(\\mathbf{x})}\\left[ g_f(V_{\\omega}(\\mathbf{x})) \\right] + \\mathbb{E}_{q_{\\theta}(\\mathbf{x})}\\left[ -f^*(g_f(V_{\\omega}(\\mathbf{x}))) \\right] \\right) \\\\ \\theta^{t+1} &amp;= \\theta^{t} - \\eta \\,\\nabla_{\\theta} \\mathcal{F}(\\theta^t,\\omega^t) \\\\ &amp;= \\theta^{t} - \\eta \\,\\nabla_{\\theta} \\left( \\mathbb{E}_{q_{\\theta}(\\mathbf{x})}\\left[ -f^*(g_f(V_{\\omega}(\\mathbf{x}))) \\right] \\right) \\end{align} \\]And similar to Goodfellow et al. (2014), Nowozin, Cseke, and Tomioka (2016) also proposed an alternative method to update the paramter \\(\\theta\\), so as to speed up training: \\[ \\theta^{t+1} = \\theta^{t} + \\eta \\,\\nabla_{\\theta} \\mathbb{E}_{q_{\\theta}(\\mathbf{x})}\\left[ g_f(V_{\\omega}(\\mathbf{x})) \\right] \\] Also note that, in many architectures, the gradient estimator \\(\\nabla_{\\theta} \\mathbb{E}_{q_{\\theta}(\\mathbf{x})}\\) can be computed with the re-paramterization trick, i.e., reparameterize \\(\\mathbf{x}\\) by a differentiable function \\(\\mathbf{x} = \\mathbf{x}_{\\theta}(\\mathbf{\\epsilon})\\), where \\(\\mathbf{\\epsilon}\\) is a random noise independent of \\(\\theta\\).Formulations for Various f-DivergenceMore details can be found in Nowozin, Cseke, and Tomioka (2016), including the formulations for Pearson \\(\\chi^2\\), and the Squared Hellinger.Kullback-Leibler\\[ \\begin{align} D_{f}(P \\| Q) &amp;= D_{\\mathrm{KL}}(P\\|Q) \\\\ &amp;= \\int p(\\mathbf{x}) \\log \\frac{p(\\mathbf{x})}{q(\\mathbf{x})}\\,\\mathrm{d}\\mathbf{x} \\\\ f(u) &amp;= u \\log u \\\\ f^*(t) &amp;= \\exp(t - 1) \\\\ g_f(v) &amp;= v \\end{align} \\]Reverse KL\\[ \\begin{align} D_{f}(P \\| Q) &amp;= D_{\\mathrm{KL}}(Q\\|P) \\\\ &amp;= \\int q(\\mathbf{x}) \\log \\frac{q(\\mathbf{x})}{p(\\mathbf{x})}\\,\\mathrm{d}\\mathbf{x} \\\\ f(u) &amp;= -\\log u \\\\ f^*(t) &amp;= -1 - \\log(-t) \\\\ g_f(v) &amp;= -\\exp(-v) \\end{align} \\]Jensen-Shannon\\[ \\begin{align} D_{f}(P \\| Q) &amp;= \\text{JSD}(P\\|Q) = \\frac{1}{2}\\Big( D_{\\mathrm{KL}}(P\\|M) +D_{\\mathrm{KL}}(Q\\|M) \\Big) \\\\ &amp;= \\frac{1}{2}\\int \\left( p(\\mathbf{x}) \\log \\frac{2p(\\mathbf{x})}{p(\\mathbf{x}) + q(\\mathbf{x})} + q(\\mathbf{x}) \\log \\frac{2 q(\\mathbf{x})}{p(\\mathbf{x}) + q(\\mathbf{x})} \\right)\\,\\mathrm{d}\\mathbf{x} \\\\ f(u) &amp;= u\\log u - (u+1)\\log \\frac{u+1}{2} \\\\ f^*(t) &amp;= -\\log(2-\\exp(t)) \\\\ g_f(v) &amp;= \\log 2 - \\log (1+\\exp(-v)) \\end{align} \\]where \\(M = \\frac{P+Q}{2}\\).Original GAN (Goodfellow et al. 2014)\\[ \\begin{align} D_{f}(P \\| Q) &amp;= \\int \\left( p(\\mathbf{x}) \\log \\frac{2p(\\mathbf{x})}{p(\\mathbf{x}) + q(\\mathbf{x})} + q(\\mathbf{x}) \\log \\frac{2 q(\\mathbf{x})}{p(\\mathbf{x}) + q(\\mathbf{x})} \\right)\\mathrm{d}\\mathbf{x} - \\log 4 \\\\ f(u) &amp;= u \\log u - (u+1) \\log (u+1) \\\\ f^*(t) &amp;= -\\log(1-\\exp(t)) \\\\ g_f(v) &amp;= -\\log(1+\\exp(-v)) \\end{align} \\]ReferencesGoodfellow, Ian, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. “Generative Adversarial Nets.” In Advances in Neural Information Processing Systems, 2672–80.Nowozin, Sebastian, Botond Cseke, and Ryota Tomioka. 2016. “F-GAN: Training Generative Neural Samplers Using Variational Divergence Minimization.” In Advances in Neural Information Processing Systems 29, edited by D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, 271–79. Curran Associates, Inc.","categories":[{"name":"Deep_Learning","slug":"Deep-Learning","permalink":"https://wiki.haowen-xu.com/categories/Deep-Learning/"},{"name":"Generative_Adversarial_Nets","slug":"Deep-Learning/Generative-Adversarial-Nets","permalink":"https://wiki.haowen-xu.com/categories/Deep-Learning/Generative-Adversarial-Nets/"}],"tags":[]},{"title":"Mutual Information","slug":"Deep_Learning/Information_Theoretical/Mutual_Information","date":"2020-03-16T20:02:24.000Z","updated":"2020-06-01T12:15:59.008Z","comments":true,"path":"Deep_Learning/Information_Theoretical/Mutual_Information/","link":"","permalink":"https://wiki.haowen-xu.com/Deep_Learning/Information_Theoretical/Mutual_Information/","excerpt":"","text":"Definition\\[ \\begin{align} I(X;Y) &amp;= D_{\\mathrm{KL}}\\left( p(x,y) \\,\\|\\, p(x)\\,p(y) \\right) \\\\ &amp;= \\int p(x,y)\\, \\log \\frac{p(x,y)}{p(x)\\,p(y)}\\,dx \\end{align} \\]Invariant Under ReparameterizationIf \\(f\\) and \\(g\\) are homeormophism, then \\[ I(X;Y) = I(f(X);f(Y)) \\] that is, mutual information is invariant under reparameterization (Brakel and Bengio 2017).EstimatorsInfoNCE(to be completed)MINEBelghazi et al. (2018) proposed a mutual information estimator based on the Donsker-Varadhan dual representation (Donsker and Varadhan 1983). \\[ \\begin{align} I(X;Z) &amp;\\geq \\sup_{T_{\\theta}} \\left\\{ \\mathbb{E}_{p(x,z)}\\left[T_{\\psi}(x,z)\\right] - \\log \\left( \\mathbb{E}_{p(x)\\,p(z)}\\left[e^{T_{\\psi}(x,z)}\\right] \\right) \\right\\} \\\\ &amp;\\approx \\sup_{T_{\\theta}} \\left\\{ \\frac{1}{b} \\sum_{i=1}^b\\left[T_{\\psi}(x^{(i)},z^{(i)})\\right] - \\log \\left( \\frac{1}{b} \\sum_{i=1}^b\\left[e^{T_{\\psi}(x^{(i)},\\tilde{z}^{(i)})}\\right] \\right) \\right\\} \\end{align} \\] where \\(x^{(i)}, z^{(i)} \\sim p(x,z)\\), and \\(\\tilde{z}^{(i)} \\sim p(z)\\). \\(b\\) is the batch size.The gradient of MINE estimator might have too large a scale, in which situation the Adaptive Clipping might be used. Also, Belghazi et al. (2018) proposed to correct the bias of the gradient estimator by an additional moving average term.Deep InfoMaxHjelm et al. (2018) proposed a mutual information estimator based on the JSD estimator, via f-GAN formulation: \\[ \\mathcal{I}^{(JSD)}(X;Z) = \\mathbb{E}_{p(x,z)}\\left[ -\\text{sp}(-T_{\\psi}(x,z)) \\right] - \\mathbb{E}_{p(x)\\,p(z)}\\left[ \\text{sp}(T_{\\psi}(x,z)) \\right] \\] where \\(\\text{sp}(a) = \\log(1 + e^a)\\).For encoder architecture \\(z = E_{\\theta}(x)\\) and \\(p(x) = p_d(x)\\), it can also be formulated as: \\[ \\begin{align} \\mathcal{I}^{(JSD)}(X;Z) &amp;= \\mathbb{E}_{\\mathbb{P}}\\left[ -\\text{sp}(-T_{\\psi}(x,E_{\\theta}(x))) \\right] - \\mathbb{E}_{\\mathbb{P}\\times\\tilde{\\mathbb{P}}}\\left[ \\text{sp}(T_{\\psi}(\\tilde{x},E_{\\theta}(x))) \\right] \\\\ &amp;\\approx \\frac{1}{b} \\sum_{i=1}^b \\left[ -\\text{sp}\\left(-T_{\\psi}(x^{(i)},E_{\\theta}(x^{(i)}))\\right) \\right] - \\frac{1}{b} \\sum_{i=1}^b \\left[ \\text{sp}\\left({T_{\\psi}(\\tilde{x}^{(i)},E_{\\theta}(x^{(i)}))}\\right) \\right] \\end{align} \\] where \\(\\mathbb{P} = \\tilde{\\mathbb{P}} = p(x)\\). \\(x^{(i)}\\) and \\(\\tilde{x}^{(i)}\\) are samples from \\(\\mathbb{P}\\) and \\(\\tilde{\\mathbb{P}}\\), respectively. \\(b\\) is the batch size.Note: Deep InfoMax actually has much more content then summarized here. The architecture, the global/local deep information maximum and the prior matching techniques can be referred to the original paper.Mutual Information as RegularizersAvoid Mode Collapse in GANMaximizing the entropy \\(H(X) = H(G(Z))\\) of a GAN generator could help avoid mode collapse. (Chen et al. 2016; Belghazi et al. 2018; Kumar et al. 2019)Since \\(X=G(Z)\\) is a deterministic mapping, we have \\(H(G(Z)|Z) \\equiv 0\\), and therefore: \\[ H(G(Z)) = I(G(Z);Z) - H(G(Z)|Z) = I(G(Z);Z) \\]Bound the Reconstruction ErrorGiven the encoder \\(q(\\mathbf{z}|\\mathbf{x})\\) and decoder \\(p(\\mathbf{x}|\\mathbf{z})\\), and the prior distribution \\(p(\\mathbf{z})\\), assuming \\(q(\\mathbf{x}) = p(\\mathbf{x}) = p_d(\\mathbf{x})\\), then the reconstruction error \\(\\mathcal{R}\\) defined as: \\[ \\mathcal{R} = \\mathbb{E}_{p_d(\\mathbf{x})} \\mathbb{E}_{q(\\mathbf{z}|\\mathbf{x})}\\left[ -\\log p(\\mathbf{x}|\\mathbf{z}) \\right] \\] can be decomposed as: \\[ \\begin{align} \\mathcal{R} &amp;= \\mathbb{E}_{q(\\mathbf{x},\\mathbf{z})}\\left[ -\\log p(\\mathbf{x}|\\mathbf{z}) \\right] \\\\ &amp;= \\mathbb{E}_{q(\\mathbf{x},\\mathbf{z})}\\left[ -\\log p(\\mathbf{x},\\mathbf{z}) + \\log p(\\mathbf{z}) \\right] \\\\ &amp;= \\mathbb{E}_{q(\\mathbf{x},\\mathbf{z})}\\left[ \\log \\frac{q(\\mathbf{x},\\mathbf{z})}{p(\\mathbf{x},\\mathbf{z})} - \\log q(\\mathbf{z},\\mathbf{x}) + \\log p(\\mathbf{z}) \\right] \\\\ &amp;= D_{\\mathrm{KL}}(q(\\mathbf{x},\\mathbf{z})\\,\\|\\,p(\\mathbf{x},\\mathbf{z})) + H_{q}(Z,X) + \\mathbb{E}_{q(\\mathbf{x},\\mathbf{z})}\\left[ \\log p(\\mathbf{z}) \\right] \\end{align} \\] where \\[ \\begin{align} \\mathbb{E}_{q(\\mathbf{x},\\mathbf{z})}\\left[ \\log p(\\mathbf{z}) \\right] &amp;= \\iint q(\\mathbf{x},\\mathbf{z}) \\log p(\\mathbf{z}) \\,\\mathrm{d}\\mathbf{z}\\,\\mathrm{d}\\mathbf{x} \\\\ &amp;= \\int \\left( \\int q(\\mathbf{x},\\mathbf{z}) \\,\\mathrm{d}\\mathbf{x} \\right)\\log p(\\mathbf{z}) \\,\\mathrm{d}\\mathbf{z} \\\\ &amp;= \\mathbb{E}_{q(\\mathbf{z})}\\left[ \\log p(\\mathbf{z}) \\right] \\\\ &amp;= \\mathbb{E}_{q(\\mathbf{z})}\\left[ -\\log \\frac{q(\\mathbf{z})}{p(\\mathbf{z})} + \\log q(\\mathbf{z}) \\right] \\\\ &amp;= -D_{\\mathrm{KL}}(q(\\mathbf{z})\\,\\|\\,p(\\mathbf{z})) - H_{q}(Z) \\end{align} \\] and since \\[ H_q(Z,X) - H_q(Z) = H_q(Z|X) = H_q(Z) - I_q(Z|X) \\] we have (Belghazi et al. 2018): \\[ \\begin{align} \\mathcal{R} &amp;= D_{\\mathrm{KL}}(q(\\mathbf{x},\\mathbf{z})\\,\\|\\,p(\\mathbf{x},\\mathbf{z})) + H_{q}(Z,X) - D_{\\mathrm{KL}}(q(\\mathbf{z})\\,\\|\\,p(\\mathbf{z})) - H_{q}(Z) \\\\ &amp;= D_{\\mathrm{KL}}(q(\\mathbf{x},\\mathbf{z})\\,\\|\\,p(\\mathbf{x},\\mathbf{z})) - D_{\\mathrm{KL}}(q(\\mathbf{z})\\,\\|\\,p(\\mathbf{z})) + H_q(Z) - I_q(Z|X) \\end{align} \\]As long as \\(q(\\mathbf{z})\\) is trained to match \\(p(\\mathbf{z})\\), maximizing \\(I_q(Z|X)\\) can be an efficient way to bound the reconstruction loss.Information BottleneckFor a latent variable model \\(X \\to Z \\to Y\\), mutual information can be served as a regularizer to limit the amount of information passed through \\(Z\\), leaving only the most relevant part reaching the output variable \\(Y\\).Minimizing the Information Bottleneck Lagrangian for encoder \\(q(z|x)\\) can serve the constraint: \\[ \\mathcal{L}(q(z|x)) = H(Y|Z) + \\beta I(X,Z) \\]ReferencesBelghazi, Mohamed Ishmael, Aristide Baratin, Sai Rajeswar, Sherjil Ozair, Yoshua Bengio, Aaron Courville, and R. Devon Hjelm. 2018. “Mine: Mutual Information Neural Estimation.” arXiv Preprint arXiv:1801.04062.Brakel, Philemon, and Yoshua Bengio. 2017. “Learning Independent Features with Adversarial Nets for Non-Linear Ica.” arXiv Preprint arXiv:1710.05050.Chen, Xi, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. 2016. “InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets.” In Advances in Neural Information Processing Systems 29, edited by D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, 2172–80. Curran Associates, Inc.Donsker, Monroe D., and SR Srinivasa Varadhan. 1983. “Asymptotic Evaluation of Certain Markov Process Expectations for Large Time. IV.” Communications on Pure and Applied Mathematics 36 (2): 183–212.Hjelm, R. Devon, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam Trischler, and Yoshua Bengio. 2018. “Learning Deep Representations by Mutual Information Estimation and Maximization.” arXiv Preprint arXiv:1808.06670.Kumar, Rithesh, Anirudh Goyal, Aaron Courville, and Yoshua Bengio. 2019. “Maximum Entropy Generators for Energy-Based Models.” arXiv:1901.08508 [Cs, Stat], January. http://arxiv.org/abs/1901.08508.","categories":[{"name":"Deep_Learning","slug":"Deep-Learning","permalink":"https://wiki.haowen-xu.com/categories/Deep-Learning/"},{"name":"Information_Theoretical","slug":"Deep-Learning/Information-Theoretical","permalink":"https://wiki.haowen-xu.com/categories/Deep-Learning/Information-Theoretical/"}],"tags":[]},{"title":"KL-Divergence","slug":"Deep_Learning/Information_Theoretical/KL-Divergence","date":"2020-03-16T18:56:07.000Z","updated":"2020-06-01T12:15:59.008Z","comments":true,"path":"Deep_Learning/Information_Theoretical/KL-Divergence/","link":"","permalink":"https://wiki.haowen-xu.com/Deep_Learning/Information_Theoretical/KL-Divergence/","excerpt":"","text":"Definition\\[ \\begin{align} D_{\\mathrm{KL}}\\left( p\\|q \\right) &amp;= \\int p(x) \\log\\frac{p(x)}{q(x)} \\,dx \\\\ &amp;= -\\int p(x) \\log\\frac{q(x)}{p(x)} \\,dx \\end{align} \\]Measures the divergence between two distributions: \\(p(x)\\) and \\(q(x)\\). It is always non-negative, since: \\[ D_{\\mathrm{KL}}\\left( p\\|q \\right) = -\\int p(x) \\log\\frac{q(x)}{p(x)} \\,dx \\geq -\\log \\int p(x) \\frac{q(x)}{p(x)} = 0 \\]KL-Divergence is Not SymmetricThere is a fact that \\[ D_{\\mathrm{KL}}\\left( p\\|q \\right) \\neq D_{\\mathrm{KL}}\\left( q\\|p \\right) \\] To train model distribution \\(q(x)\\) according to data distribution \\(p(x)\\), \\(D_{\\mathrm{KL}}\\left( p\\|q \\right)\\) will deduce a spread-out \\(q(x)\\) as follows:Figure 1: \\(D_{\\mathrm{KL}}\\left( p\\|q \\right)\\) will deduce a spread-out \\(q(x)\\)1Conversely, the reverse KL \\(D_{\\mathrm{KL}}\\left( q\\|p \\right)\\) will deduce a \\(q(x)\\) concentrated at some modes of \\(p(x)\\):Figure 2: \\(D_{\\mathrm{KL}}\\left( p\\|q \\right)\\) will deduce a \\(q(x)\\) concentrated at some modes of \\(p(x)\\)Estimate KL-Divergence with ClassifierTheoryDefine a classifier: \\[ p(x\\in p|x) = \\frac{1}{1+\\exp(-r(x))} \\] then for the mixed training data distribution: \\[ \\tilde{p}(x) = 0.5 (p(x) + q(x)) \\] the \\(r^{\\star}(x)\\) for the optimal classifier \\(p^\\star(x\\in p|x)\\) is: \\[ \\begin{align} p^\\star(x\\in p|x) &amp;= \\frac{p(x)}{p(x)+q(x)} \\\\ r^\\star(x) &amp;= \\log \\frac{p(x)}{q(x)} \\end{align} \\] Thus we can use the learned \\(r^{\\star}(x)\\) to estimate the KL-divergence: \\[ D_{\\mathrm{KL}}\\left( p\\|q \\right) = \\mathbb{E}_{p(x)}\\left[ r^{\\star}(x) \\right] \\]Training objectiveMaximizing \\(\\mathcal{L}\\) can obtain \\(r(x) = D_{\\mathrm{KL}}\\left( p\\|q \\right)\\), where \\(\\mathcal{L}\\) is formulated as: \\[ \\begin{align} \\mathcal{L} &amp;= \\mathbb{E}_{\\tilde{p}(x)}\\left[ I(x\\in p) \\log p(x\\in p|x) + (1 - I(x \\in p)) \\log (1 - p(x\\in p|x)) \\right] \\\\ &amp;= \\mathbb{E}_{p(x)}\\left[ \\log \\frac{1}{1+\\exp(-r(x))} \\right] + \\mathbb{E}_{q(x)}\\left[ \\log \\frac{\\exp(-r(x))}{1+\\exp(-r(x))} \\right] \\\\ &amp;= -\\mathbb{E}_{p(x)}\\left[ \\log \\left( 1+\\exp(-r(x)) \\right) \\right] - \\mathbb{E}_{q(x)}\\left[ \\log \\left( 1+\\exp(r(x)) \\right) \\right] \\end{align} \\]The Donsker-Varadhan representationDonsker and Varadhan (1983) proposed a dual representation of the KL-Divergence: \\[ \\mathrm{D}_{KL}(p\\|q) = \\sup_{T:\\Omega \\to \\mathbb{R}} \\left\\{ \\mathbb{E}_{p}[T(\\mathbf{x})] - \\log\\left( \\mathbb{E}_{q}[e^{T(x,y)}] \\right) \\right\\} \\]And also, the following inequilty holds for \\(T \\in \\mathcal{F}\\), a specific family of functions: \\[ \\mathrm{D}_{KL}(p\\|q) \\geq \\sup_{T \\in \\mathcal{F}} \\left\\{ \\mathbb{E}_{p}[T(\\mathbf{x})] - \\log\\left( \\mathbb{E}_{q}[e^{T(x,y)}] \\right) \\right\\} \\] where the inequilty is tight for \\(T^*\\) satisfying: \\[ p(x) \\,dx = \\frac{1}{Z} e^{T^*(x,y)}\\,q(x)\\,dx, \\quad \\text{where } Z = \\mathbb{E}_{q}\\left[e^{T^*(x,y)}\\right] \\]ReferencesDonsker, Monroe D., and SR Srinivasa Varadhan. 1983. “Asymptotic Evaluation of Certain Markov Process Expectations for Large Time. IV.” Communications on Pure and Applied Mathematics 36 (2): 183–212.Figures from https://wiseodd.github.io/techblog/2016/12/21/forward-reverse-kl/.↩︎","categories":[{"name":"Deep_Learning","slug":"Deep-Learning","permalink":"https://wiki.haowen-xu.com/categories/Deep-Learning/"},{"name":"Information_Theoretical","slug":"Deep-Learning/Information-Theoretical","permalink":"https://wiki.haowen-xu.com/categories/Deep-Learning/Information-Theoretical/"}],"tags":[]},{"title":"Graph Auto-Encoder","slug":"Deep_Learning/Graph_Neural_Networks/Graph_Auto_Encoder","date":"2020-03-11T23:38:10.000Z","updated":"2020-06-01T12:15:59.000Z","comments":true,"path":"Deep_Learning/Graph_Neural_Networks/Graph_Auto_Encoder/","link":"","permalink":"https://wiki.haowen-xu.com/Deep_Learning/Graph_Neural_Networks/Graph_Auto_Encoder/","excerpt":"","text":"Auto-Encoding the Feature Matrix Given Adjacency MatrixThe task is to learn the encoder \\(f\\) and the decoder \\(g\\) that: \\[ \\begin{align} \\mathbf{Z} &amp;= f(\\mathbf{A},\\mathbf{X}) \\\\ \\tilde{\\mathbf{X}} &amp;= g(\\mathbf{A},\\mathbf{Z}) \\end{align} \\]Graph U-NetFigure 1: Graph U-Net (view pdf)Figure 2: gUnpool (view pdf)Gao and Ji (2019) proposed the gPool, and gUnpool as the building blocks of graph auto-encoder. It also mimics the hierarchical structure of U-Net (Ronneberger, Fischer, and Brox 2015), thus named as Graph U-Net.gPool: is similar to that proposed by Cangea et al. (2018), see Graph Convolution Network.gUnpool: the gUnpool layer that restores the input of a gUnpool layer can be formulated as; \\[ \\tilde{\\mathbf{X}}^{(l)} = \\mathop{\\mathrm{distribute}}\\left( \\mathbf{X}^{(l+1)}, \\mathbf{idx}^{(l)} \\right) \\] where \\(\\mathbf{X}^{(l+1)}\\) is output of the \\(l\\)-th gPool layer, \\(\\mathbf{idx}^{(l)}\\) is the top-\\(k\\) nodes of the \\(l\\)-th gPool layer. The function \\(\\text{distribute}(\\cdot)\\) assigns back each row of \\(\\mathbf{X}^ {(l+1)}\\) to the \\(\\tilde{\\mathbf{X}}^{(l)}\\), according to the selection index \\(\\mathbf{idx}^{(l)}\\). The shape of \\(\\tilde{\\mathbf{X}}^{(l)}\\) is the same as the input of the \\(l\\)-th gPool layer (i.e., \\(\\mathbf{X}^{(l)}\\)). The un-assigned rows are zero vectors.ReferencesCangea, Cătălina, Petar Veličković, Nikola Jovanović, Thomas Kipf, and Pietro Liò. 2018. “Towards Sparse Hierarchical Graph Classifiers.” arXiv Preprint arXiv:1811.01287.Gao, Hongyang, and Shuiwang Ji. 2019. “Graph U-Nets.” arXiv:1905.05178 [Cs, Stat], May. http://arxiv.org/abs/1905.05178.Ronneberger, Olaf, Philipp Fischer, and Thomas Brox. 2015. “U-Net: Convolutional Networks for Biomedical Image Segmentation.” arXiv:1505.04597 [Cs], May. http://arxiv.org/abs/1505.04597.","categories":[{"name":"Deep_Learning","slug":"Deep-Learning","permalink":"https://wiki.haowen-xu.com/categories/Deep-Learning/"},{"name":"Graph_Neural_Networks","slug":"Deep-Learning/Graph-Neural-Networks","permalink":"https://wiki.haowen-xu.com/categories/Deep-Learning/Graph-Neural-Networks/"}],"tags":[]},{"title":"Image Segmentation","slug":"Deep_Learning/CV/Image_Segmentation","date":"2020-03-11T17:39:40.000Z","updated":"2020-06-01T12:15:58.996Z","comments":true,"path":"Deep_Learning/CV/Image_Segmentation/","link":"","permalink":"https://wiki.haowen-xu.com/Deep_Learning/CV/Image_Segmentation/","excerpt":"","text":"OverviewFor each pixel \\(x_{ij}\\) on an image, predict its segmentation class \\(c_{ij}\\).Supervised MethodsFully convolutional networks for semantic segmentationFigure 1: The Architecture of \"Fully convolutional networks for semantic segmentation\" (view pdf)Long, Shelhamer, and Darrell (2015) proposed to use deconvolutional layers to up-sample intermediate feature maps at different levels from a pre-trained convolutional neural network, in order to compose the pixel-wise classification output.U-NetFigure 2: The Architecture of \"U-Net\" (view pdf)Hierarchical deconvolution at different levels.Weighted cross-entropy loss: to balance the loss of different classes, and to enforce the net to put emphasis on the cell boundaries. \\[ \\begin{align} \\mathcal{L} &amp;= \\sum w(\\mathbf{x}) \\log p(\\mathbf{x}) \\\\ w(\\mathbf{x}) &amp;= w_c(\\mathbf{x}) + w_0 \\cdot \\exp \\left( -\\frac{d_1(\\mathbf{x}) + d_2(\\mathbf{x})^2}{2 \\sigma^2} \\right) \\end{align} \\] where \\(w_c(\\mathbf{x})\\) is the normalizing weight for the class of the pixel, while \\(w_0\\) is a hyper-parameter. \\(d_1\\) and \\(d_2\\) is the distance from the background pixel \\(\\mathbf{x}\\) to the closest and second closest cell.EvaluationMetricsLet \\(n_{ij}\\) be the number of pixels of class \\(i\\) being predicted to belong to class \\(j\\). Suppose there are \\(k\\) different classes, and \\(t_i = \\sum_{j} n_{ij}\\) be the total number of pixels of class \\(i\\). Then we have the following metrics for image segmentation (Long, Shelhamer, and Darrell 2015):Pixel accuracy \\[ \\text{Pixel Acc} = \\frac{\\sum_i n_{ii}}{\\sum_i t_i} \\]Mean accuracy \\[ \\text{Mean Acc} = \\frac{1}{k} \\sum_i \\frac{n_{ii}}{t_i} \\]Mean IoU (Intersection over Union): \\[ \\text{Mean IoU} = \\frac{1}{k} \\sum_{i} \\frac{n_ii}{t_i + \\sum_j n_{ji} - n_{ii}} \\]Weighted IoU \\[ \\text{Weighted IoU} = \\frac{1}{\\sum_j t_j} \\cdot\\frac{\\sum_i t_i n_{ii}}{t_i + \\sum_j n_{ji} -n_{ii}} \\]ReferencesLong, Jonathan, Evan Shelhamer, and Trevor Darrell. 2015. “Fully Convolutional Networks for Semantic Segmentation.” In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 3431–40.","categories":[{"name":"Deep_Learning","slug":"Deep-Learning","permalink":"https://wiki.haowen-xu.com/categories/Deep-Learning/"},{"name":"CV","slug":"Deep-Learning/CV","permalink":"https://wiki.haowen-xu.com/categories/Deep-Learning/CV/"}],"tags":[]},{"title":"Graph Convolution Network","slug":"Deep_Learning/Graph_Neural_Networks/Graph_Convolution_Network","date":"2020-03-07T19:02:45.000Z","updated":"2020-06-01T12:15:59.004Z","comments":true,"path":"Deep_Learning/Graph_Neural_Networks/Graph_Convolution_Network/","link":"","permalink":"https://wiki.haowen-xu.com/Deep_Learning/Graph_Neural_Networks/Graph_Convolution_Network/","excerpt":"","text":"OverviewTo spread the node features across the graph, according to the graph structure (typically local connectivity among the nodes).The features after applying the \\(l\\)-th graph convolution layer can be denoted as \\(\\mathbf{H}^{(l)}\\), where it should be a \\(n \\times k\\) matrix, with \\(n\\) being the number of nodes in the graph, and \\(k\\) being the number of feature dimensions. \\(\\mathbf{h}^{(l)}_i\\) refers to the \\(i\\)-th row of \\(\\mathbf{H}^{(l)}\\), denoting the feature of the \\(i\\)-th node after applying the \\(l\\)-th layer.The connectivity among the nodes can be represented by the adjacency matrix \\(\\mathbf{A}\\), an \\(n \\times n\\) matrix, where \\(\\mathbf{A}_{ij}\\) representing the connectivity from node \\(j\\) to \\(i\\). For undirected graph, \\(\\mathbf{A}\\) should be symmetric.Feature MatrixEvery node in the graph requires an initial feature matrix, for GCN layers to propagate along the connections between nodes. Besides using the task-specific features, one may also use:Node degree information (Cangea et al. 2018): one-hot encoding the node degree for all degrees up to a given upper bound.One-hot index vector (Yao, Mao, and Luo 2019): one-hot encoding the index of nodes.Graph ConvolutionBasic FormulationThe GCN layer can be formulated as: \\[ \\mathbf{H}^{(l + 1)} = f(\\hat{\\mathbf{A}} \\mathbf{H}^{(l)}\\mathbf{W}^{(l)}) \\] where \\(\\hat{\\mathbf{A}}\\) is the normalized adjacency matrix. A popular choice for undirected graph may be (Wu et al. 2019): \\[ \\hat{\\mathbf{A}} = \\mathbf{D}^{-1/2} \\mathbf{A} \\,\\mathbf{D}^{-1/2} \\] where \\(\\mathbf{D}_{ii} = \\sum_j \\mathbf{A}_{ij}\\) is the degree of node \\(i\\).Another choice for \\(\\hat{\\mathbf{A}}\\), which not only can be used for undirected grpah, but also can be used for directed ones, can be formulated as: \\[ \\hat{\\mathbf{A}} = \\mathbf{D}^{-1} \\mathbf{A} \\] Some work may add auxiliary self-loop to the adjacency matrix, such that the normalized adjacency matrix becomes: \\[ \\hat{\\mathbf{A}} = \\mathbf{D}^{-1/2} (\\mathbf{A} + \\lambda\\,\\mathbf{I}) \\,\\mathbf{D}^{-1/2} \\] or (used by Zhang et al. (2018)): \\[ \\hat{\\mathbf{A}} = \\mathbf{D}^{-1} (\\mathbf{A} + \\lambda\\,\\mathbf{I}) \\]Hetergeneous Graph with Various Edge RelationshipIf the link between nodes represent different relationships (\\(R = \\{r\\}\\)), each relationship may have their own kernel \\(\\mathbf{W}_r^{(l)}\\), such that the GCN layer can be formulated as (Schlichtkrull et al. 2017): \\[ \\mathbf{h}^{(l+1)}_i = f\\left(\\sum_{r \\in R} \\sum_{j \\in \\mathcal{N}^r_i} \\frac{1}{c_{ij}} \\mathbf{W}_r^{(l)} \\mathbf{h}^{(l)}_j + \\mathbf{W}_0^{(l)} \\mathbf{h}^{(l)}_i \\right) \\]where \\(c_{ij} = \\left| \\mathcal{N}_i^r \\right|\\) is the normalizing factor.Hetergeneuos Graph with Various Node TypeIf the node are grouped into different types, i.e., node \\(v_i\\) has type \\(t_i\\), then each type can have their own kernel \\(\\mathbf{W}_{t}^{(l)}\\), and the GCN layer can be formulated as: \\[ \\begin{align} \\mathbf{h}^{(l+1)}_i &amp;= f\\left( \\sum_{j \\in \\mathcal{N}_{i}} c_{ij} \\, g(\\mathbf{h}_j^{(l)}) + g(\\mathbf{h}_i^{(l)}) \\right) \\\\ g(\\mathbf{h}_i^{(l)}) &amp;= \\mathbf{W}_{t_i}^{(l)} \\mathbf{h}_i^{(l)} \\end{align} \\] as is proposed by Wang et al. (2019). For the normalizing factor \\(c_{ij}\\), Wang et al. (2019) also proposed to learn the attention score by self-attention: \\[ \\begin{align} c_{ij}^{(l)} &amp;= \\frac{\\exp\\left( s_{ij} \\right)}{\\sum_{\\tilde{j} \\in \\mathcal{N}_i^{(l)}} \\exp\\left( s_{i\\tilde{j}} \\right)} \\\\ s_{ij} &amp;= g(\\mathbf{h}_i^{(l)})^{\\top} \\cdot g(\\mathbf{h}_j^{(l)}) \\end{align} \\] where \\(s_{ij}\\) is the attention score.LSTM Aggregation with Random Permuation of Neighborhood NodesHamilton, Ying, and Leskovec (2017) proposed a candidate neighborhoold information aggregation method, based on LSTM with random permutation of neighborhood nodes.Neighborhood Sampling Instead of SummationHamilton, Ying, and Leskovec (2017) proposed to use neighborhood sampling to reduce the training computation time.Monte-Carlo Approximation Instead of SummationChen, Ma, and Xiao (2018) proposed to view the summation over nodes within one GCN layer as an expectation, formulated as: \\[ \\mathbf{h}^{(l+1)}(v) = f\\left( \\int \\hat{\\mathbf{A}}(v,u) \\,\\mathbf{h}^{(l)}(u)\\,\\mathbf{W}^{(l)}\\,dP(u) \\right) \\] where \\(u\\) and \\(v\\) represent the nodes. \\(P(u)\\) is regarded as a uniform distribution. According to this formulation, the authors proposed to use a proposal distribution: \\[ q(u) = \\left\\| \\hat{\\mathbf{A}}(:,u) \\right\\|^2 / \\sum_{u'} \\left\\| \\hat{\\mathbf{A}}(:,u') \\right\\|^2 \\] which is the power of node \\(u\\)'s out-degree, divided by the sum of the power of all nodes' out-degree. The proposal distribution is used to sample \\(t_l\\) i.i.d. nodes for each GCN layer, namely \\(u_1^{(l)}, \\dots, u_{t_l}^{(l)}\\), and estimate \\(\\mathbf{h}^{(l+1)}\\) by: \\[ \\mathbf{h}^{(l+1)}(v) \\approx f\\left( \\frac{1}{t_l}\\sum_{j=1}^{t_l} \\frac{\\hat{\\mathbf{A}(v,u_j^{(l)})}\\,\\mathbf{h}^{(l)}(u_j^{(l)})\\,\\mathbf{W}^{(l)}}{q(u_j^{(l)})} \\right) \\]Learning the Weight of Message PassingSelf-AttentionInstead of relying on the fixed adjacency matrix to weight the neighborhood information in graph convolution networks, Veličković et al. (2017) proposed to use a self-attention mechanism to weight the neighborhood information.The attention coefficients is calculated by: \\[ e_{ij} = a\\left(\\mathbf{W} \\mathbf{h}_i, \\mathbf{W} \\mathbf{h}_j\\right) \\] where \\(a(\\cdot)\\) is the attention network. \\(e_{ij}\\) is formulated by Veličković et al. (2017) as follows: \\[ e_{ij} = a\\left(\\mathbf{W} \\mathbf{h}_i, \\mathbf{W} \\mathbf{h}_j\\right) = \\mathrm{LeakyReLU}\\left({\\mathbf{a}}^{\\top}\\left[ \\mathbf{W} \\mathbf{h}_i \\big\\| \\mathbf{W} \\mathbf{h}_j \\right]\\right) \\] This attention score is further normalized as: \\[ \\alpha_{ij} = \\frac{\\exp\\left(e_{ij}\\right)}{\\sum_{k \\in \\mathcal{N}_i} \\exp\\left(e_{ik}\\right)} = \\frac{\\exp\\left(\\mathrm{LeakyReLU}\\left({\\mathbf{a}}^{\\top}\\left[ \\mathbf{W} \\mathbf{h}_i \\big\\| \\mathbf{W} \\mathbf{h}_j \\right]\\right)\\right)}{\\sum_{k \\in \\mathcal{N}_i} \\exp\\left(\\mathrm{LeakyReLU}\\left({\\mathbf{a}}^{\\top}\\left[ \\mathbf{W} \\mathbf{h}_i \\big\\| \\mathbf{W} \\mathbf{h}_k \\right]\\right)\\right)} \\] The final output features of this layer is: \\[ \\mathbf{h}' = \\sigma\\left( \\sum_{j \\in \\mathcal{N}_i} \\alpha_{ij} \\mathbf{W} \\mathbf{h}_j \\right) \\] If using mult-head attention, the output features can be formulated as either one of the following (i.e., concat and average multi-head attention): \\[ \\begin{align} \\mathbf{h}' &amp;= \\mathop{\\bigg\\|}_{k=1}^K \\sigma\\left( \\sum_{j \\in \\mathcal{N}_i} \\alpha^k_{ij} \\mathbf{W}^k \\mathbf{h}_j \\right) \\\\ \\mathbf{h}' &amp;= \\sigma\\left( \\frac{1}{K} \\sum_{k=1}^K \\sum_{j \\in \\mathcal{N}_i} \\alpha^k_{ij} \\mathbf{W}^k \\mathbf{h}_j \\right) \\end{align} \\]where the second formulation is adopted as the output layer.Node EmbeddingThe feature vector \\(\\mathbf{h}^{(L)}\\) after \\(L\\)-th GCN layer can be used as the node embedding (Hamilton, Ying, and Leskovec 2017; Schlichtkrull et al. 2017). The unsupervised loss, using negative sampling, should be: \\[ \\mathcal{L} = \\sum_{v_i \\in V} \\sum_{v_j \\in N(v_i)} \\left[ \\log \\frac{\\exp(\\mathbf{h}^{(L)}_i \\cdot \\mathbf{h}^{(L)}_j)}{\\exp(\\mathbf{h}^{(L)}_i \\cdot \\mathbf{h}^{(L)}_j) + 1} + \\sum_{\\tilde{v}_n \\sim q(v) \\\\ n = 1 \\dots k} \\log\\frac{1}{\\exp(\\mathbf{h}^{(L)}_i \\cdot \\tilde{\\mathbf{h}}^{(L)}_n) + 1} \\right] \\] same as the negative sampling method for node embedding. This embedding loss can be used along in a fully unsupervised task; or as a augumented/regularization term for a supervised task, along with the main objective for the specific task (Hamilton, Ying, and Leskovec 2017).PoolinggPoolCangea et al. (2018) proposed the projection score to sort the nodes, and choose the top-\\(\\lceil kN \\rceil\\) nodes after pooling. \\(k \\in (0, 1]\\) is the pooling ratio, while \\(N\\) is the numberr of input nodes. The projection score \\(\\mathbf{y}\\) and the pooling method is formulated as: \\[ \\mathbf{Z} = \\frac{\\mathbf{X} \\mathbf{p}}{\\left\\| \\mathbf{p} \\right\\|_2} \\qquad \\overrightarrow{i} = \\mathop{\\text{top-rank}}\\left( \\mathbf{Z}, \\lceil kN \\rceil \\right) \\qquad \\mathbf{X}' = \\left( \\mathbf{X} \\odot \\tanh(\\mathbf{Z}) \\right)_{\\overrightarrow{i}} \\qquad \\mathbf{A}' = \\mathbf{A}_{\\overrightarrow{i},\\overrightarrow{i}} \\] where \\(\\mathbf{X}\\) is feature matrix of input nodes, \\(\\mathbf{p}\\) is the learnable parameter for the projection. \\(\\cdot_{\\overrightarrow{i}}\\) is the indexing operation.Cangea et al. (2018) proposed a similar architecture, named gPool, with \\(\\tanh(\\mathbf{Z})\\) in \\(\\mathbf{X}' = \\left( \\mathbf{X} \\odot \\tanh(\\mathbf{Z}) \\right)_{\\overrightarrow{i}}\\) replaced by \\(\\text{sigmoid}(\\mathbf{\\mathbf{Z}})\\), mimic the often adopted gate mechanism.Note: \\(\\tanh(\\mathbf{Z})\\) or \\(\\text{sigmoid}(\\mathbf{Z})\\) enables the gradient to be passed along \\(\\mathbf{p}\\). Without this term, \\(\\mathbf{p}\\) produces a solely discrete element selection, which cannot be trained by stochastic gradient descent.Self-Attention Graph PoolingLee, Lee, and Kang (2019) proposed to improve the projection score based pooling by adding a self-attention mechanism to calculate the projection score. The new projection score is derived as: \\[ \\mathbf{Z} = f(\\mathop{\\mathrm{GNN}}(\\mathbf{X},\\mathbf{A})) \\] where \\(f\\) is the activation function, \\(\\mathbf{X}\\) is the node feature matrix to be pooled, \\(\\mathbf{A}\\) is the adjacency matrix, which may be normalized. \\(\\mathop{\\mathrm{GNN}}\\) indicates a general graph network, and authors proposed the following variants of this GNN network:\\(\\mathbf{Z} = f\\left(\\tilde{\\mathbf{D}}^{-1/2}\\,\\tilde{\\mathbf{A}}\\,\\tilde{\\mathbf{D}}^{1/2}\\,\\mathbf{X}\\,\\mathbf{p}\\right)\\), where \\(\\tilde{\\mathbf{A}} = \\mathbf{A}+\\mathbf{I}\\) is the normalized adjacency matrix with self-loop, and \\(\\tilde{\\mathbf{D}}\\) is the degree matrix of \\(\\tilde{\\mathbf{A}}\\).\\(\\mathbf{Z} = f\\left(\\mathop{\\mathrm{GNN}}(\\mathbf{X},\\mathbf{A}+\\mathbf{A}^2)\\right)\\), which considers the second-order neighborhood by augument 2-hop nodes in the adjacency matrix \\(\\mathbf{A}\\).\\(\\mathbf{Z} = f\\left(\\mathop{\\mathrm{GNN}_2}(\\mathop{\\mathrm{GNN}_1}(\\mathbf{X},\\mathbf{A}),\\mathbf{A})\\right)\\), which considers the second-order neighborhood by stacking GNN layers.\\(\\mathbf{Z}=\\frac{1}{M}\\sum_{m} f\\left( \\mathop{\\mathrm{GNN}_m}(\\mathbf{X},\\mathbf{A}) \\right))\\), which using an average of multiple attention scores.Hierarchical Pooling vs Global PoolingFigure 1: Global Pooling vs Hierarchical Pooling (view pdf)Lee, Lee, and Kang (2019) found that hierarchical pooling seems to work better on large graphs, where global pooling works better on small graphs.ReadoutCombine the Global Average and Max PoolingFigure 2: Combine the Global Average and Max Pooling (view pdf)Cangea et al. (2018) proposed a graph readout layer that combines both the global average and global max pooling techniques. Specifically, for the output graph of every \\(l\\)-th pooling layer \\((\\mathbf{X}^{(l)}, \\mathbf{A}^{(l)})\\), the readout layer produces the summarized output by: \\[ \\mathbf{s}^{(l)} = \\mathop{\\mathrm{Concat}}\\left(\\frac{1}{N^{(l)}} \\sum_{i=1}^{N^{(l)}} \\mathbf{x}^{(l)}_i \\Bigg\\|\\max_{i=1}^{N^{(l)}} \\mathbf{x}^{(l)}_i\\right) \\] And the summarized output of all pooling layers \\(\\mathbf{s}^{(1)}, \\dots, \\mathbf{s}^{(L)}\\) are then summed together: \\[ \\mathbf{s} = \\sum_{l=1}^L \\mathbf{s}^{(l)} \\] as the final aggregated feature of the graph. The whole structure is demonstrated as fig.&nbsp;2.Practical GCN ArchitecturesModeling Relational Data with Graph Convolutional Networks [2017]Schlichtkrull et al. (2017) proposed the following formulation for their GCN layer with regarding to the node relationship: \\[ \\mathbf{h}^{(l+1)}_i = f\\left(\\sum_r \\sum_{j \\in \\mathcal{N}^r_i} \\frac{1}{c_{ij}} \\mathbf{W}_r^{(l)} \\mathbf{h}^{(l)}_j + \\mathbf{W}_0^{(l)} \\mathbf{h}^{(l)}_i \\right) \\] where the term \\(\\mathbf{W}_0^{(l)} \\mathbf{h}^{(l)}_i\\) represents the information passed along the self-loop from \\(l\\)-th layer to the \\((l+1)\\)-layer. \\(c_{ij}\\) is the normalizing factor, and is chosen to be \\(\\left| \\mathcal{N}_i^r \\right|\\) (i.e., the in-coming degree of node \\(i\\)) in their paper.To avoid having too many parameters, which may be potentially prone to over-fitting, Schlichtkrull et al. (2017) also proposed two methods to regularize the weights \\(\\mathbf{W}^{(l)}_r\\) of different relationships.For link prediction, Schlichtkrull et al. (2017) proposed to use the GCN output at \\(L\\)-th layer \\(\\mathbf{h}_i^{(L)}\\) as the embedding \\(\\mathbf{e}_i\\) of nodes, deriving the score function as: \\[ f(i,r,j) = \\mathbf{e}_i^{\\top}\\mathbf{R}_r\\mathbf{e}_j \\] where \\(\\mathbf{R}_r\\) is a learned diagonal matrix, to represent the \"dot-product\" between \\(\\mathbf{e}_i\\) and \\(\\mathbf{e}_j\\) under the relationship \\(r\\). The whole graph is then trained by negative sampling. See the loss function for this situation.ReferencesCangea, Cătălina, Petar Veličković, Nikola Jovanović, Thomas Kipf, and Pietro Liò. 2018. “Towards Sparse Hierarchical Graph Classifiers.” arXiv Preprint arXiv:1811.01287.Chen, Jie, Tengfei Ma, and Cao Xiao. 2018. “FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling.” arXiv:1801.10247 [Cs], January. http://arxiv.org/abs/1801.10247.Hamilton, Will, Zhitao Ying, and Jure Leskovec. 2017. “Inductive Representation Learning on Large Graphs.” In Advances in Neural Information Processing Systems, 1024–34.Lee, Junhyun, Inyeop Lee, and Jaewoo Kang. 2019. “Self-Attention Graph Pooling.” arXiv Preprint arXiv:1904.08082.Schlichtkrull, Michael, Thomas N. Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and Max Welling. 2017. “Modeling Relational Data with Graph Convolutional Networks.” arXiv:1703.06103 [Cs, Stat], October. http://arxiv.org/abs/1703.06103.Veličković, Petar, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. 2017. “Graph Attention Networks.” arXiv Preprint arXiv:1710.10903.Wang, Yueyang, Ziheng Duan, Binbing Liao, Fei Wu, and Yueting Zhuang. 2019. “Heterogeneous Attributed Network Embedding with Graph Convolutional Networks.” In Proceedings of the AAAI Conference on Artificial Intelligence, 33:10061–2.Wu, Zonghan, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and Philip S. Yu. 2019. “A Comprehensive Survey on Graph Neural Networks.” arXiv:1901.00596 [Cs, Stat], August. http://arxiv.org/abs/1901.00596.Yao, Liang, Chengsheng Mao, and Yuan Luo. 2019. “Graph Convolutional Networks for Text Classification.” In Proceedings of the AAAI Conference on Artificial Intelligence, 33:7370–7.Zhang, Muhan, Zhicheng Cui, Marion Neumann, and Yixin Chen. 2018. “An End-to-End Deep Learning Architecture for Graph Classification.” In Thirty-Second AAAI Conference on Artificial Intelligence.","categories":[{"name":"Deep_Learning","slug":"Deep-Learning","permalink":"https://wiki.haowen-xu.com/categories/Deep-Learning/"},{"name":"Graph_Neural_Networks","slug":"Deep-Learning/Graph-Neural-Networks","permalink":"https://wiki.haowen-xu.com/categories/Deep-Learning/Graph-Neural-Networks/"}],"tags":[]},{"title":"Node Embedding","slug":"Deep_Learning/Graph_Neural_Networks/Node_Embedding","date":"2020-03-06T01:07:30.000Z","updated":"2020-06-01T12:15:59.008Z","comments":true,"path":"Deep_Learning/Graph_Neural_Networks/Node_Embedding/","link":"","permalink":"https://wiki.haowen-xu.com/Deep_Learning/Graph_Neural_Networks/Node_Embedding/","excerpt":"","text":"OverviewA graph is defined by \\(G = V \\times E\\), where \\(V = \\{v_i\\}\\) is the set of all vertex (nodes), and \\(E=\\{e_i\\}\\) is the set of all edges.For directed graph, the task is often to learn one embedding for a particular node \\(v_i\\), namely, \\(\\mathbf{u}_i\\).For undirected graph, the task is often to learn two embeddings for a particular node \\(v_i\\), the out-going (source) embedding \\(\\mathbf{s}_i\\), and the in-going (destination) embedding \\(\\mathbf{t}_i\\).Objective for Learning the EmbeddingLearning Embedding that Represents the Local Connectivity (homophily) of NodesUndirected GraphFor undirected graph, the connectivity from \\(v_i\\) to \\(v_j\\) can be defined as the probability of predicting \\(v_j\\) given \\(v_i\\): \\[ p(v_j|v_i) = \\frac{\\exp(\\mathbf{u}_i \\cdot \\mathbf{u}_j)}{\\sum_{v_n \\in G} \\exp(\\mathbf{u}_i \\cdot \\mathbf{u}_n)} \\] The dot product can be seen as a score to indicate the connectivity from \\(v_i\\) to \\(v_j\\). The training objective is to maximize \\(p(v_j|v_i)\\) for every \\(v_j\\) among the neighbors of \\(v_i\\) (i.e., \\(N(v_i)\\), and all the nodes \\(v_i\\) in \\(V\\). \\[ \\mathcal{L} = \\mathbb{E}_{v_i \\in V} \\mathbb{E}_{v_j \\in N(v_i)} \\left[ \\log p(v_j|v_i) \\right] = \\mathbb{E}_{v_i \\in V} \\mathbb{E}_{v_j \\in N(v_i)} \\left[ \\log\\frac{\\exp(\\mathbf{u}_i \\cdot \\mathbf{u}_j)}{\\sum_{v_n \\in G} \\exp(\\mathbf{u}_i \\cdot \\mathbf{u}_n)} \\right] \\] The partition function of \\(p(v_j|v_i)\\) is hard to evaluate. For learning an embedding, negative sampling could be adopted to train the above objective. If \\(p(v_i)\\) and \\(p(v_j|v_i)\\) are uniform, we have: \\[ \\tilde{\\mathcal{L}} = \\sum_{v_i \\in V} \\sum_{v_j \\in N(v_i)} \\left[ \\log \\frac{\\exp(\\mathbf{u}_i \\cdot \\mathbf{u}_j)}{\\exp(\\mathbf{u}_i \\cdot \\mathbf{u}_j) + 1} + \\sum_{\\tilde{v}_n \\sim q(v) \\\\ n = 1 \\dots k} \\log\\frac{1}{\\exp(\\mathbf{u}_i \\cdot \\tilde{\\mathbf{u}}_n) + 1} \\right] \\] where \\(q(v)\\) is a noise distribution that samples negative node samples for a given pair \\((v_i,v_j)\\). \\(k\\) controls the number of negative samples for each pair \\((v_i,v_j)\\). The noise distribution is usually also uniform.Directed GraphIn directed graph, the connectivity from \\(v_i\\) to \\(v_j\\) can be defined as the probability of predicting \\(v_j\\) given \\(v_i\\): \\[ p(v_j|v_i) = \\frac{\\exp(\\mathbf{s}_i \\cdot \\mathbf{t}_j)}{\\sum_{v_n \\in G} \\exp(\\mathbf{s}_i \\cdot \\mathbf{t}_n)} \\]Multi-Relational LinkA relational matrix \\(\\mathbf{R}_r, \\, r \\in R\\) can be used to replace the dot-product in homogeneous graphs, which brings the probability of predicting \\(v_j\\) given \\(v_i\\):\\[ p(v_j|v_i,r) = \\frac{\\exp(\\mathbf{s}_i^{\\top} \\mathbf{R}_r \\mathbf{t}_j)}{\\sum_{\\tilde{r} \\in R, v_n \\in G} \\exp(\\mathbf{s}_i^{\\top} \\mathbf{R}_{\\tilde{r}} \\mathbf{t}_n)} \\]The negative-sampling based training objective then becomes:\\[ \\tilde{\\mathcal{L}} = \\sum_{v_i \\in V} \\sum_{r \\in R} \\sum_{v_j \\in N_r(v_i)} \\left[ \\log \\frac{\\exp(\\mathbf{u}_i^{\\top} \\mathbf{R}_r \\mathbf{u}_j)}{\\exp(\\mathbf{u}_i^{\\top} \\mathbf{R}_r \\mathbf{u}_j) + 1} + \\sum_{\\tilde{r}_n \\in R \\\\ \\tilde{v}_n \\sim q_{\\tilde{r}_n}(v) \\\\ n = 1 \\dots k} \\log\\frac{1}{\\exp(\\mathbf{u}_i^{\\top} \\mathbf{R}_{\\tilde{r}_n} \\tilde{\\mathbf{u}}_n) + 1} \\right] \\]where Yang et al. (2014) and Schlichtkrull et al. (2017) used diagonal \\(\\mathbf{R}_r\\) to reduce the parameters.Random WalkMonte-Carlo End-Point sampling methodBalancing DFS and BFSnode2vec: Scalable Feature Learning for NetworksPath sharing techniques: for a sampled random-walk path \\(v_1, \\dots, v_L\\), consider thatSuffixes: \\(v_i, \\dots, v_L\\) are valid paths.Prefixes: \\(v_1, \\dots, v_i\\) are valid paths. [Powerwalk: Scalable personalized pagerank via random walks with vertexcentric decomposition.]Sliding windows: \\(v_i, \\dots, v_{i+W}\\) are valid paths. (Grover and Leskovec 2016)ReferencesGrover, Aditya, and Jure Leskovec. 2016. “Node2vec: Scalable Feature Learning for Networks.” arXiv:1607.00653 [Cs, Stat], July. http://arxiv.org/abs/1607.00653.Schlichtkrull, Michael, Thomas N. Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and Max Welling. 2017. “Modeling Relational Data with Graph Convolutional Networks.” arXiv:1703.06103 [Cs, Stat], October. http://arxiv.org/abs/1703.06103.Yang, Bishan, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. 2014. “Embedding Entities and Relations for Learning and Inference in Knowledge Bases.” arXiv Preprint arXiv:1412.6575.","categories":[{"name":"Deep_Learning","slug":"Deep-Learning","permalink":"https://wiki.haowen-xu.com/categories/Deep-Learning/"},{"name":"Graph_Neural_Networks","slug":"Deep-Learning/Graph-Neural-Networks","permalink":"https://wiki.haowen-xu.com/categories/Deep-Learning/Graph-Neural-Networks/"}],"tags":[]},{"title":"Softmax Speedup","slug":"Deep_Learning/Confronting_Partition_Function/Softmax_Speedup","date":"2020-03-04T02:59:36.000Z","updated":"2020-06-01T12:15:59.000Z","comments":true,"path":"Deep_Learning/Confronting_Partition_Function/Softmax_Speedup/","link":"","permalink":"https://wiki.haowen-xu.com/Deep_Learning/Confronting_Partition_Function/Softmax_Speedup/","excerpt":"","text":"Problem StatementTo optimize \\(\\theta\\) for a softmax classification model \\(p(y|\\mathbf{x};\\theta)\\) (or formulated as \\(p(w|\\mathbf{c};\\theta)\\) in many NLP literature) without estimating the partition function \\(Z(\\theta)\\):\\[ \\begin{align} p(y=i|\\mathbf{x};\\theta) &amp;= \\frac{\\tilde{p}(y=i|\\mathbf{x};\\theta)}{Z(\\theta)} \\\\ Z(\\mathbf{x};\\theta) &amp;= \\sum_{j=1}^m \\tilde{p}(y=j|\\mathbf{x};\\theta) \\end{align} \\] where the unnormalized \\(\\tilde{p}(y|\\mathbf{x};\\theta)\\) is typically formulated as \\(\\tilde{p}(y=i|\\mathbf{x};\\theta)=\\exp\\left( u_i(\\mathbf{x},y;\\theta) \\right)\\), where \\(i=1 \\dots m\\) is the \\(k\\) classes.Hierarchical SoftmaxEncode each class \\(y\\) by a binary sequence \\(b_1(y), b_2(y), \\dots, b_k(y)\\) , where \\(b_i(y) \\in \\{0, 1\\}\\), and decompose \\(p(y|\\mathbf{x};\\theta)\\) as: \\[ p(y|\\mathbf{x};\\theta) = \\prod_{i=1}^k p(b_i(y)|b_{i-1}(y),\\dots,b_1(y),\\mathbf{x};\\theta) \\] such that each of the decomposed term is a binary classifier.Further SpeedupHoffman coding can be used to construct the binary tree, such that more frequently visited classes \\(y\\) can have a shorter binary sequence.Noise Contrastive EstimationThe NCE method starts by choosing a noise distribution \\(q(y)\\), and modify the original \\(m\\)-target classification problem as a binary classfication problem.The binary classifier is defined as: \\[ \\begin{align} p(D=1|\\mathbf{x},y;\\theta) &amp;= \\frac{p(y|\\mathbf{x};\\theta)}{p(y|\\mathbf{x};\\theta) + k\\, q(y)} = \\frac{\\tilde{p}(y|\\mathbf{x};\\theta) / Z(\\mathbf{x};\\theta)}{\\tilde{p}(y|\\mathbf{x};\\theta) / Z(\\mathbf{x};\\theta) + k\\, q(y)} \\\\ p(D=0|\\mathbf{x},y;\\theta) &amp;= \\frac{k\\, q(y)}{p(y|\\mathbf{x};\\theta) + k\\, q(y)} = \\frac{k\\, q(y)}{\\tilde{p}(y|\\mathbf{x};\\theta)/Z(\\mathbf{x};\\theta) + k\\, q(y)} \\end{align} \\] and the NCE objective (which should be maximized) is defined as: \\[ \\begin{align} \\mathcal{L}_{\\mathrm{NCE}} &amp;= \\mathbb{E}_{(\\mathbf{x},y) \\sim p_d(\\mathbf{x},y)} \\left[ \\log p(D=1|\\mathbf{x},y;\\theta) + k\\, \\mathbb{E}_{\\bar{y} \\sim q(y)}\\left[ \\log p(D=0|\\mathbf{x},\\bar{y}) \\right] \\right] \\\\ &amp;\\approx \\sum_{\\mathbf{x},y } \\left[ \\log p(D=1|\\mathbf{x},y;\\theta) + k\\cdot\\frac{1}{k}\\sum_{j = 1 \\dots k \\\\ \\bar{y}_j \\sim q(y)} \\log p(D=0|\\mathbf{x},\\bar{y}_j;\\theta) \\right] \\\\ &amp;= \\sum_{\\mathbf{x},y } \\left[ \\log p(D=1|\\mathbf{x},y;\\theta) + \\sum_{j = 1}^k \\log p(D=0|\\mathbf{x},\\bar{y}_j;\\theta) \\right] \\end{align} \\] where \\(\\sum_{\\mathbf{x},y}\\) is a summation over the train data in a mini-batch, and \\(\\frac{1}{k}\\sum_{j = 1 \\dots k \\\\ \\bar{y}_j \\sim q(y)}\\) is a Monte Carlo estimator of \\(\\mathbb{E}_{\\bar{y} \\sim q(y)}\\).A necessary condition for NCE to work is that, \\(q(y) \\neq 0\\) wherever \\(p(y|\\mathbf{x};\\theta) \\neq 0\\).The Gradient of NCE Loss\\[ \\begin{align} \\frac{\\partial \\mathcal{L}_{\\mathrm{NCE}}}{\\partial \\theta} &amp;= \\frac{\\partial}{\\partial \\theta} \\mathbb{E}_{p_d(\\mathbf{x},y)} \\left[ \\log \\frac{p(y|\\mathbf{x};\\theta)}{p(y|\\mathbf{x};\\theta) + k\\, q(y)} + k\\, \\mathbb{E}_{q(y)}\\left[ \\log \\frac{k\\, q(y)}{p(y|\\mathbf{x};\\theta) + k\\, q(y)} \\right] \\right] \\\\ &amp;= \\frac{\\partial}{\\partial \\theta} \\mathbb{E}_{p_d(\\mathbf{x})} \\left[ \\mathbb{E}_{p_d(y|\\mathbf{x})} \\left[ \\log \\frac{p(y|\\mathbf{x};\\theta)}{p(y|\\mathbf{x};\\theta) + k\\, q(y)} \\right] + k\\, \\mathbb{E}_{q(y)}\\left[ \\log \\frac{k\\, q(y)}{p(y|\\mathbf{x};\\theta) + k\\, q(y)} \\right] \\right] \\\\ &amp;= \\mathbb{E}_{p_d(\\mathbf{x})} \\left[ \\mathbb{E}_{p_d(y|\\mathbf{x})} \\left[ \\frac{1}{p(y|\\mathbf{x};\\theta)} \\cdot \\frac{k\\,q(y)}{p(y|\\mathbf{x};\\theta) + k\\, q(y)} \\cdot \\frac{\\partial p(y|\\mathbf{x};\\theta)}{\\partial \\theta} \\right] - k\\, \\mathbb{E}_{q(y)}\\left[ \\frac{1}{p(y|\\mathbf{x};\\theta) + k\\, q(y)} \\cdot \\frac{\\partial p(y|\\mathbf{x};\\theta)}{\\partial \\theta} \\right] \\right] \\\\ &amp;= \\mathbb{E}_{p_d(\\mathbf{x})} \\left[ \\mathbb{E}_{p_d(y|\\mathbf{x})} \\left[ \\frac{k\\,q(y)}{p(y|\\mathbf{x};\\theta) + k\\, q(y)} \\cdot \\frac{\\partial \\log p(y|\\mathbf{x};\\theta)}{\\partial \\theta} \\right] - k\\, \\mathbb{E}_{q(y)}\\left[ \\frac{p(y|\\mathbf{x};\\theta)}{p(y|\\mathbf{x};\\theta) + k\\, q(y)} \\cdot \\frac{\\partial \\log p(y|\\mathbf{x};\\theta)}{\\partial \\theta} \\right] \\right] \\\\ &amp;= \\mathbb{E}_{p_d(\\mathbf{x})} \\sum_y \\left[ \\frac{k\\,q(y)}{p(y|\\mathbf{x};\\theta) + k\\, q(y)} \\cdot \\bigg( p_d(y|\\mathbf{x}) - p(y|\\mathbf{x};\\theta) \\bigg) \\cdot \\frac{\\partial \\log p(y|\\mathbf{x};\\theta)}{\\partial \\theta} \\right] \\end{align} \\]Note that:\\[ \\sum_y \\left[ p(y|\\mathbf{x};\\theta) \\cdot \\frac{\\partial \\log p(y|\\mathbf{x};\\theta)}{\\partial \\theta} \\right] = \\sum_y \\frac{\\partial p(y|\\mathbf{x};\\theta)}{\\partial \\theta} = \\frac{\\partial}{\\partial \\theta} \\sum_y p(y|\\mathbf{x};\\theta) = \\frac{\\partial 1}{\\partial \\theta} = 0 \\]Thus, when \\(k \\to \\infty\\), \\(\\frac{\\partial \\mathcal{L}_{\\mathrm{NCE}}}{\\partial \\theta}\\) equals to \\(\\frac{\\partial}{\\partial \\theta} \\mathbb{E}_{p_d(\\mathbf{x},y)}\\left[ \\log p(y|\\mathbf{x};\\theta) \\right]\\), which is the gradient for the original classification problem.The Global Optimum of NCE Loss(This section is added by me. The original NCE paper (Gutmann and Hyvärinen 2010) has a similar theorem, but did not give the proof. I use Euler's formula from Calculus of Variations, but in fact the condition of using this theorem is not strictly satisfied. Thus this proof may only be seen as a discussion.) \\[ \\begin{align} \\frac{\\partial \\mathcal{L}_{\\mathrm{NCE}}}{\\partial p(y|\\mathbf{x};\\theta)} &amp;= \\mathbb{E}_{p_d(\\mathbf{x})} \\sum_y \\left[ \\frac{k\\,q(y)}{p(y|\\mathbf{x};\\theta) + k\\,q(y)} \\cdot \\bigg( p_d(y|\\mathbf{x}) - p(y|\\mathbf{x};\\theta) \\bigg) \\cdot \\frac{1}{p(y|\\mathbf{x};\\theta)} \\right] \\end{align} \\] According to Euler's formula: \\[ \\begin{align} \\frac{\\partial \\mathcal{L}_{\\mathrm{NCE}}}{\\partial p(y|\\mathbf{x};\\theta)} = 0 &amp;\\Leftrightarrow \\frac{k\\,q(y)}{p(y|\\mathbf{x};\\theta) + k\\,q(y)} \\cdot \\bigg( p_d(y|\\mathbf{x}) - p(y|\\mathbf{x};\\theta) \\bigg) \\cdot \\frac{1}{p(y|\\mathbf{x};\\theta)} = 0 \\end{align} \\] Since \\(q(y)\\) is arbitrary, the only solution for \\(\\mathcal{L}_{\\mathrm{NCE}}\\) to attains its global optimum is: \\[ p(y|\\mathbf{x};\\theta) \\equiv p_d(y|\\mathbf{x}) \\]Dealing with \\(Z(\\mathbf{x};\\theta)\\)For NCE, \\(Z(\\mathbf{x};\\theta)\\) can be learned instead of estimated. To avoid having extra free parameters, Mnih and Teh (2012) suggested to let \\(Z(\\mathbf{x};\\theta) \\equiv 1\\), and found it works well. Having a fixed \\(Z(\\mathbf{x};\\theta) \\equiv 1\\) will likely to induce a normalized \\(p(y|\\mathbf{x};\\theta)\\).Negative SamplingNegative sampling (Mikolov et al. 2013) can be seen as a simplified version of noise contrastive estimation. The binary classifier is defined as: \\[ \\begin{align} p(D=1|\\mathbf{x},y;\\theta) &amp;= \\frac{p(y|\\mathbf{x};\\theta)}{p(y|\\mathbf{x};\\theta) + 1} \\\\ p(D=0|\\mathbf{x},y;\\theta) &amp;= \\frac{1}{p(y|\\mathbf{x};\\theta) + 1} \\end{align} \\] that is, \\(q(y) \\equiv \\frac{1}{k}\\). The loss is derived as: \\[ \\begin{align} \\mathcal{L}_{\\mathrm{NEG}} &amp;= \\mathbb{E}_{(\\mathbf{x},y) \\sim p_d(\\mathbf{x},y)} \\left[ \\log p(D=1|\\mathbf{x},y;\\theta) + \\mathbb{E}_{\\bar{y} \\sim q(y)}\\left[ \\log p(D=0|\\mathbf{x},\\bar{y}) \\right] \\right] \\\\ &amp;= \\mathbb{E}_{p_d(\\mathbf{x})} \\left[ \\mathbb{E}_{ p(y|\\mathbf{x};\\theta)}\\left[ \\log \\frac{p(y|\\mathbf{x};\\theta)}{p(y|\\mathbf{x};\\theta) + 1} \\right] + k\\,\\mathbb{E}_{q(y)}\\left[ \\log \\frac{1}{p(y|\\mathbf{x};\\theta) + 1} \\right] \\right] \\\\ &amp;\\approx \\frac{1}{N} \\sum_{\\mathbf{x},y } \\left[ \\log \\frac{p(y|\\mathbf{x};\\theta)}{p(y|\\mathbf{x};\\theta) + 1} + \\sum_{j = 1}^k \\log \\frac{1}{p(\\bar{y}_j|\\mathbf{x};\\theta) + 1} \\right] \\end{align} \\]The Normalizing FactorThe negative sampling does not train a properly normalized \\(p(y|\\mathbf{x};\\theta)\\) (Dyer 2014), unless \\(k = m\\), where \\(q(y) \\equiv \\frac{1}{m}\\) is a normalized uniform distribution, since: \\[ \\begin{align} \\mathcal{L}_{\\mathrm{NEG}} &amp;= \\mathbb{E}_{p_d(\\mathbf{x})} \\left[ \\mathbb{E}_{ p_d(y|\\mathbf{x})}\\left[ \\log \\frac{p(y|\\mathbf{x};\\theta)}{p(y|\\mathbf{x};\\theta) + 1} \\right] + k\\,\\mathbb{E}_{q(y)}\\left[ \\log \\frac{1}{p(y|\\mathbf{x};\\theta) + 1} \\right] \\right] \\\\ &amp;= \\mathbb{E}_{p_d(\\mathbf{x})} \\left[ \\mathbb{E}_{ p_d(y|\\mathbf{x}) }\\left[ \\log \\frac{p(y|\\mathbf{x};\\theta) / \\frac{m}{k}}{p(y|\\mathbf{x};\\theta) / \\frac{m}{k} + k \\cdot \\left( \\frac{1}{k} \\cdot \\frac{k}{m} \\right)} \\right] + k\\,\\mathbb{E}_{q(y)}\\left[ \\log \\frac{k \\cdot \\left( \\frac{1}{k} \\cdot \\frac{k}{m} \\right)}{p(y|\\mathbf{x};\\theta) / \\frac{m}{k} + k \\cdot \\left( \\frac{1}{k} \\cdot \\frac{k}{m} \\right)} \\right] \\right] \\end{align} \\] If we believe \\(q(y) \\equiv \\frac{1}{m}\\) (which is often indeed the case for uniform sampling of the noise samples), then this should be a standard NCE loss, and the learned \"probability distribution\" \\(p(y|\\mathbf{x};\\theta)\\) could potentially be converted into a truely normalized distribution, by scaling it with the normalizing factor \\(Z(\\mathbf{x};\\theta) \\equiv \\frac{m}{k}\\). (This assertion is made by me)ReferencesDyer, Chris. 2014. “Notes on Noise Contrastive Estimation and Negative Sampling.” arXiv Preprint arXiv:1410.8251.Gutmann, Michael, and Aapo Hyvärinen. 2010. “Noise-Contrastive Estimation: A New Estimation Principle for Unnormalized Statistical Models.” In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, 297–304.Mikolov, Tomas, Ilya Sutskever, Kai Chen, Greg S. Corrado, and Jeff Dean. 2013. “Distributed Representations of Words and Phrases and Their Compositionality.” In Advances in Neural Information Processing Systems, 3111–9.Mnih, Andriy, and Yee Whye Teh. 2012. “A Fast and Simple Algorithm for Training Neural Probabilistic Language Models.” arXiv:1206.6426 [Cs], June. http://arxiv.org/abs/1206.6426.","categories":[{"name":"Deep_Learning","slug":"Deep-Learning","permalink":"https://wiki.haowen-xu.com/categories/Deep-Learning/"},{"name":"Confronting_Partition_Function","slug":"Deep-Learning/Confronting-Partition-Function","permalink":"https://wiki.haowen-xu.com/categories/Deep-Learning/Confronting-Partition-Function/"}],"tags":[]},{"title":"Energy Function in Probabilistic Models","slug":"Deep_Learning/Energy_Based_Models/Energy_Function_in_Probabilistic_Models","date":"2020-02-28T22:43:51.000Z","updated":"2020-06-01T12:15:59.000Z","comments":true,"path":"Deep_Learning/Energy_Based_Models/Energy_Function_in_Probabilistic_Models/","link":"","permalink":"https://wiki.haowen-xu.com/Deep_Learning/Energy_Based_Models/Energy_Function_in_Probabilistic_Models/","excerpt":"","text":"This post summarizes the relationship between energy function and the deduced probabilistic model by a specified energy function.Common FormulationThe probabilistic model deduced from an energy function can have the following formulations.Gibbs DistributionGiven an energy function \\(U(\\mathbf{x};\\theta)\\) with parameters \\(\\theta\\), the probability distribution can be deduced as: \\[ \\begin{align} p(\\mathbf{x};\\theta) &amp;= \\frac{1}{Z(\\theta)}\\,\\exp\\left( -U(\\mathbf{x};\\theta) \\right) \\\\ Z(\\theta) &amp;= \\int \\exp\\left( -U(\\mathbf{x};\\theta) \\right)\\,\\mathrm{d}\\mathbf{x} \\end{align} \\] The gradient of \\(\\mathbb{E}_{p_D(\\mathbf{x})}\\left[ -\\log p(\\mathbf{x};\\theta) \\right]\\) (i.e., the expectation of the negative log-likelihood \\(-\\log p(\\mathbf{x})\\) over data distribution \\(p_D(\\mathbf{x})\\)) is then derived as: \\[ \\begin{align} \\nabla \\mathbb{E}_{p_D(\\mathbf{x};\\theta)}\\left[ -\\log p(\\mathbf{x};\\theta) \\right] &amp;= \\mathbb{E}_{p_D(\\mathbf{x})}\\left[ -\\nabla \\log p(\\mathbf{x};\\theta) \\right] \\\\ &amp;= \\mathbb{E}_{p_D(\\mathbf{x})} \\left[ \\nabla U(\\mathbf{x};\\theta) + \\nabla \\log Z(\\theta) \\right] \\\\ &amp;= \\mathbb{E}_{p_D(\\mathbf{x})} \\left[ \\nabla U(\\mathbf{x};\\theta) \\right] + \\nabla \\log Z(\\theta) \\end{align} \\]where \\(\\nabla \\log Z(\\theta)\\) is: \\[ \\begin{align} \\nabla \\log Z(\\theta) &amp;= \\frac{\\nabla Z(\\theta)}{Z(\\theta)} \\\\ &amp;= \\frac{1}{Z(\\theta)} \\int \\nabla \\exp\\left( -U(\\mathbf{x};\\theta) \\right)\\,\\mathrm{d}\\mathbf{x} \\\\ &amp;= \\int \\frac{\\exp\\left( -U(\\mathbf{x};\\theta) \\right) / Z(\\theta)}{\\exp\\left( -U(\\mathbf{x};\\theta) \\right)} \\nabla \\exp\\left( -U(\\mathbf{x};\\theta) \\right) \\,\\mathrm{d}\\mathbf{x} \\\\ &amp;= \\int p(\\mathbf{x};\\theta) \\, \\nabla \\log \\exp\\left( -U(\\mathbf{x};\\theta) \\right) \\,\\mathrm{d}\\mathbf{x} \\\\ &amp;= -\\int p(\\mathbf{x};\\theta) \\, \\nabla U(\\mathbf{x};\\theta) \\,\\mathrm{d}\\mathbf{x} \\\\ &amp;= -\\mathbb{E}_{p(\\mathbf{x};\\theta)} \\left[ \\nabla U(\\mathbf{x};\\theta) \\right] \\end{align} \\] thus the final gradient can be derived as: \\[ \\nabla \\mathbb{E}_{p_D(\\mathbf{x};\\theta)}\\left[ -\\log p(\\mathbf{x};\\theta) \\right] = \\mathbb{E}_{p_D(\\mathbf{x})} \\left[ \\nabla U(\\mathbf{x};\\theta) \\right] - \\mathbb{E}_{p(\\mathbf{x};\\theta)} \\left[ \\nabla U(\\mathbf{x};\\theta) \\right] \\]Positive and Negative PhaseThe above gradient consists of the positive phase term \\(\\mathbb{E}_{p_D(\\mathbf{x})} \\left[ \\nabla U(\\mathbf{x};\\theta) \\right]\\), and the negative phase term \\(\\mathbb{E}_{p(\\mathbf{x};\\theta)} \\left[ \\nabla U(\\mathbf{x};\\theta) \\right]\\). The gradient reaches zero (which indicates a local minima) when these two terms are equal.If the path of the gradient to \\(\\mathbb{E}_{p(\\mathbf{x};\\theta)}\\) is blocked (this sentence is added by me), then the positive phase term can be seen as minimizing the energy on \"positive samples\" from data distribution, and the negative phase can be seen as maximizing the energy on \"negative samples\" from model distribution. (Kim and Bengio 2016)Sampling from \\(\\mathbb{E}_{p(\\mathbf{x};\\theta)}\\) often requires MCMC techniques, for example, the Contrastive Divergence algorithm.Conditional and IndependenceDefinitionThe distribution \\(p(\\mathbf{x},\\mathbf{y},\\mathbf{z})\\) deduced by energy function \\(U(\\mathbf{x},\\mathbf{y},\\mathbf{z})\\): \\[ p(\\mathbf{x},\\mathbf{y},\\mathbf{z}) = \\frac{\\exp\\left( -U(\\mathbf{x},\\mathbf{y},\\mathbf{z}) \\right)}{\\iiint \\exp\\left( -U(\\mathbf{x}^*,\\mathbf{y}^*,\\mathbf{z}^*) \\right) \\, \\mathrm{d}\\mathbf{z}^* \\,\\mathrm{d}\\mathbf{y}^* \\,\\mathrm{d}\\mathbf{x}^*} \\] Also, the conditional distribution \\(p(\\mathbf{y},\\mathbf{z}|\\mathbf{x})\\) is defined as: \\[ p(\\mathbf{y},\\mathbf{z}|\\mathbf{x}) = \\frac{\\exp\\left( -U(\\mathbf{x},\\mathbf{y},\\mathbf{z}) \\right)}{\\iint \\exp\\left( -U(\\mathbf{x},\\mathbf{y}^*,\\mathbf{z}^*) \\right) \\, \\mathrm{d}\\mathbf{z}^* \\,\\mathrm{d}\\mathbf{y}^*} \\]Theorem 1If \\(U(\\mathbf{x},\\mathbf{y},\\mathbf{z}) = f(\\mathbf{x},\\mathbf{y}) + g(\\mathbf{x},\\mathbf{z}) + h(\\mathbf{x})\\), then: \\[ \\begin{align} p(\\mathbf{y}|\\mathbf{x}) &amp;= \\frac{\\exp\\left( -f(\\mathbf{x},\\mathbf{y}) \\right)}{\\int \\exp\\left( -f(\\mathbf{x},\\mathbf{y}^*) \\right) \\,\\mathrm{d}\\mathbf{y}^*} \\\\ p(\\mathbf{z}|\\mathbf{x}) &amp;= \\frac{\\exp\\left( -g(\\mathbf{x},\\mathbf{z}) \\right)}{\\int \\exp\\left( -g(\\mathbf{x},\\mathbf{z}^*) \\right) \\,\\mathrm{d}\\mathbf{z}^*} \\end{align} \\] Proof: \\[ \\begin{align} p(\\mathbf{y}|\\mathbf{x}) &amp;= \\int p(\\mathbf{x},\\mathbf{y},\\mathbf{z})\\,\\mathrm{d}\\mathbf{z} \\\\ &amp;= \\int \\frac{\\exp\\left( -U(\\mathbf{x},\\mathbf{y},\\mathbf{z}) \\right)}{\\iint \\exp\\left( -U(\\mathbf{x},\\mathbf{y}^*,\\mathbf{z}^*) \\right) \\, \\mathrm{d}\\mathbf{z}^* \\,\\mathrm{d}\\mathbf{y}^*}\\,\\mathrm{d}\\mathbf{z} \\\\ &amp;= \\frac{\\exp\\left( -h(\\mathbf{x}) \\right)\\cdot\\exp\\left( -f(\\mathbf{x},\\mathbf{y}) \\right) \\int \\exp\\left( -g(\\mathbf{x},\\mathbf{z}) \\right)\\,\\mathrm{d}\\mathbf{z}}{\\iint \\exp\\left( -h(\\mathbf{x}) \\right)\\cdot\\exp\\left( -f(\\mathbf{x},\\mathbf{y}^*)\\right) \\cdot\\exp\\left( -g(\\mathbf{x},\\mathbf{z}^*) \\right)\\,\\mathrm{d}\\mathbf{z}^*\\,\\mathrm{d}\\mathbf{y}^*} \\\\ &amp;= \\frac{\\exp\\left( -h(\\mathbf{x}) \\right)\\cdot\\exp\\left( -f(\\mathbf{x},\\mathbf{y}) \\right) \\int \\exp\\left( -g(\\mathbf{x},\\mathbf{z}) \\right)\\,\\mathrm{d}\\mathbf{z}}{\\exp\\left( -h(\\mathbf{x}) \\right)\\cdot\\left( \\int \\exp\\left( -f(\\mathbf{x},\\mathbf{y}^*)\\right)\\,\\mathrm{d}\\mathbf{y}^* \\right) \\cdot \\left( \\int \\exp\\left( -g(\\mathbf{x},\\mathbf{z}^*) \\right)\\,\\mathrm{d}\\mathbf{z}^* \\right)} \\\\ &amp;= \\frac{\\exp\\left( -f(\\mathbf{x},\\mathbf{y}) \\right)}{\\int \\exp\\left( -f(\\mathbf{x},\\mathbf{y}^*)\\right)\\,\\mathrm{d}\\mathbf{y}^*} \\end{align} \\] \\(p(\\mathbf{z}|\\mathbf{x}) = \\frac{\\exp\\left( -g(\\mathbf{x},\\mathbf{z}) \\right)}{\\int \\exp\\left( -g(\\mathbf{x},\\mathbf{z}^*) \\right) \\,\\mathrm{d}\\mathbf{z}^*}\\) can be proven in the same way.Collary 1If \\(U(\\mathbf{x},\\mathbf{y},\\mathbf{z}) = f(\\mathbf{x},\\mathbf{y}) + g(\\mathbf{x},\\mathbf{z}) + h(\\mathbf{x})\\), then \\(\\mathbf{y} \\perp\\!\\!\\!\\perp \\mathbf{z} \\mid \\mathbf{x}\\).Proof: \\[ \\begin{align} p(\\mathbf{y},\\mathbf{z}|\\mathbf{x}) &amp;= \\frac{\\exp\\left( -U(\\mathbf{x},\\mathbf{y},\\mathbf{z}) \\right)}{\\iint \\exp\\left( -U(\\mathbf{x},\\mathbf{y}^*,\\mathbf{z}^*) \\right) \\, \\mathrm{d}\\mathbf{z}^* \\,\\mathrm{d}\\mathbf{y}^*} \\\\ &amp;= \\frac{\\exp\\left( -f(\\mathbf{x},\\mathbf{y}) \\right)}{\\int \\exp\\left( -f(\\mathbf{x},\\mathbf{y}^*) \\right) \\,\\mathrm{d}\\mathbf{y}^*} \\cdot \\frac{\\exp\\left( -g(\\mathbf{x},\\mathbf{z}) \\right)}{\\int \\exp\\left( -g(\\mathbf{x},\\mathbf{z}^*) \\right) \\,\\mathrm{d}\\mathbf{z}^*} \\\\ &amp;= p(\\mathbf{y}|\\mathbf{x}) \\cdot p(\\mathbf{z}|\\mathbf{x}) \\end{align} \\] which implies \\(\\mathbf{y} \\perp\\!\\!\\!\\perp \\mathbf{z} \\mid \\mathbf{x}\\).Collary 2If \\(U(\\mathbf{y},\\mathbf{z}) = f(\\mathbf{y}) + g(\\mathbf{z})\\), then: \\[ \\begin{align} p(\\mathbf{y}) &amp;= \\frac{\\exp\\left( -f(\\mathbf{y}) \\right)}{\\int \\exp\\left( -f(\\mathbf{y}^*) \\right) \\,\\mathrm{d}\\mathbf{y}^*} \\\\ p(\\mathbf{z}) &amp;= \\frac{\\exp\\left( -g(\\mathbf{z}) \\right)}{\\int \\exp\\left( -g(\\mathbf{z}^*) \\right) \\,\\mathrm{d}\\mathbf{z}^*} \\end{align} \\] Proof: similar to Theorem 1.Collary 3If \\(U(\\mathbf{y},\\mathbf{z}) = f(\\mathbf{y}) + g(\\mathbf{z})\\), then \\(\\mathbf{y} \\perp\\!\\!\\!\\perp \\mathbf{z}\\).Proof: according to Collary 2, and similar to Collary 1.ReferencesKim, Taesup, and Yoshua Bengio. 2016. “Deep Directed Generative Models with Energy-Based Probability Estimation.” arXiv Preprint arXiv:1606.03439.","categories":[{"name":"Deep_Learning","slug":"Deep-Learning","permalink":"https://wiki.haowen-xu.com/categories/Deep-Learning/"},{"name":"Energy_Based_Models","slug":"Deep-Learning/Energy-Based-Models","permalink":"https://wiki.haowen-xu.com/categories/Deep-Learning/Energy-Based-Models/"}],"tags":[]},{"title":"Restricted Boltzmann Machine","slug":"Deep_Learning/Energy_Based_Models/Restricted_Boltzmann_Machine","date":"2020-02-28T18:56:31.000Z","updated":"2020-06-01T12:15:59.000Z","comments":true,"path":"Deep_Learning/Energy_Based_Models/Restricted_Boltzmann_Machine/","link":"","permalink":"https://wiki.haowen-xu.com/Deep_Learning/Energy_Based_Models/Restricted_Boltzmann_Machine/","excerpt":"","text":"Standard RBMThe formuation of a standard Restricted Boltzmann Machine (RBM) consists of an observed binary variable \\(\\mathbf{v}\\) and a latent binary variable \\(\\mathbf{h}\\), as well as an energy function defined as: \\[ E(\\mathbf{v},\\mathbf{h}) = -\\mathbf{a}^{\\top} \\mathbf{v} - \\mathbf{b}^{\\top}\\mathbf{h}-\\mathbf{v}^{\\top}\\mathbf{W}\\,\\mathbf{h} \\] the probability of the model is defined as: \\[ P(\\mathbf{v},\\mathbf{h}) = \\frac{1}{Z} \\, \\exp\\left( -E(\\mathbf{v},\\mathbf{h}) \\right) \\] where \\(Z\\) is the partition function.The conditional distributions are: \\[ \\begin{align} P(\\mathbf{v}|\\mathbf{h}) &amp;= \\frac{1}{Z(\\mathbf{h})} \\, \\exp\\left(\\left( \\mathbf{a} + \\mathbf{W}\\mathbf{h} \\right)^{\\top}\\,\\mathbf{v}\\right) \\\\ P(\\mathbf{h}|\\mathbf{v}) &amp;= \\frac{1}{Z(\\mathbf{v})} \\, \\exp\\left(\\left( \\mathbf{b}^{\\top} + \\mathbf{v}^{\\top}\\mathbf{W} \\right)\\mathbf{h}\\right) \\end{align} \\]It is easy to verify that \\(h_i\\) is independent of \\(h_j\\), for \\(i \\neq j\\), given an observed \\(\\mathbf{v}\\). This is also true for \\(v_i\\), \\(v_j\\) given \\(\\mathbf{h}\\). Thus sampling from \\(P(\\mathbf{v},\\mathbf{h})\\) could be achieved by sampling from the two conditional distributions alternatively (i.e., a block Gibbs sampler).The parameters \\(\\mathbf{a},\\mathbf{b},\\mathbf{W}\\) of \\(E(\\mathbf{v},\\mathbf{h})\\) can be optimized by Contrastive Divergence algorithm, with the energy function \\(U(\\mathbf{z})\\) for \\(\\mathbf{v}\\), satisfying \\(U(\\mathbf{v}) + \\log Z = \\log P(\\mathbf{v})\\). Whereas it is more simply to deduce the energy function \\(U(\\mathbf{v})\\) if we consider the conditional independence of \\(h_i\\), \\(h_j\\) beforehand, and use the element-wise notations, I provide here the deduction using vector notation.Deduction of \\(U(\\mathbf{v})\\) using Vector NotationThe marginal distribution \\(P(\\mathbf{v})\\) for the training data is: \\[ P(\\mathbf{v}) = \\sum_{\\mathbf{h}} P(\\mathbf{v},\\mathbf{h}) = \\frac{1}{Z} \\, \\exp\\left( \\mathbf{a}^{\\top}\\mathbf{v} \\right) \\cdot \\sum_{\\mathbf{h}} \\exp\\left( \\left( \\mathbf{b}^{\\top}+\\mathbf{v}^{\\top}\\mathbf{W} \\right)\\mathbf{h} \\right) \\] which results in the following energy function for \\(\\mathbf{v}\\): \\[ U(\\mathbf{v}) = \\log P(\\mathbf{v}) = \\mathbf{a}^{\\top}\\mathbf{v} + \\log\\sum_{\\mathbf{h}}\\exp\\left( \\left( \\mathbf{b}^{\\top}+\\mathbf{v}^{\\top}\\mathbf{W} \\right)\\mathbf{h} \\right) \\] If the number of elements of the vector \\(\\mathbf{h}\\) is k, we can further get: \\[ \\begin{align} \\sum_{\\mathbf{h}}\\exp\\left( \\left( \\mathbf{b}^{\\top}+\\mathbf{v}^{\\top}\\mathbf{W} \\right)\\mathbf{h} \\right) &amp;= \\sum_{h_1,h_2,\\dots,h_k} \\exp\\left( \\sum_{j=1}^k(b_j + \\mathbf{v}^{\\top} \\mathbf{W}_j)\\,h_j \\right) \\\\ &amp;= \\sum_{h_1,h_2,\\dots,h_k} \\prod_{j=1}^k \\exp\\left( (b_j+\\mathbf{v}^{\\top} \\mathbf{W}_j) \\,h_j \\right) \\\\ &amp;= \\prod_{j=1}^k \\sum_{h_j} \\exp\\left( (b_j+\\mathbf{v}^{\\top} \\mathbf{W}_j) \\,h_j \\right) \\end{align} \\] where \\(\\mathbf{W}_j\\) is the \\(j\\)-th column of the matrix \\(\\mathbf{W}\\). We then have: \\[ U(\\mathbf{v}) = \\mathbf{a}^{\\top}\\mathbf{v} + \\sum_{j=1}^k \\log \\sum_{h_j} \\exp\\left( (b_j+\\mathbf{v}^{\\top} \\mathbf{W}_j) \\,h_j \\right) \\] Given that \\(h_j\\) is a binary variable, \\(\\sum_{h_j}\\) can be it further deduced into: \\[ U(\\mathbf{v}) = \\mathbf{a}^{\\top}\\mathbf{v} + \\sum_{j=1}^k \\log \\left( 1 + \\exp\\left( b_j+\\mathbf{v}^{\\top} \\mathbf{W}_j \\right) \\right) \\]","categories":[{"name":"Deep_Learning","slug":"Deep-Learning","permalink":"https://wiki.haowen-xu.com/categories/Deep-Learning/"},{"name":"Energy_Based_Models","slug":"Deep-Learning/Energy-Based-Models","permalink":"https://wiki.haowen-xu.com/categories/Deep-Learning/Energy-Based-Models/"}],"tags":[]},{"title":"Overview","slug":"Deep_Learning/Energy_Based_Models/Overview","date":"2020-02-25T08:45:00.000Z","updated":"2020-06-01T12:15:59.000Z","comments":true,"path":"Deep_Learning/Energy_Based_Models/Overview/","link":"","permalink":"https://wiki.haowen-xu.com/Deep_Learning/Energy_Based_Models/Overview/","excerpt":"","text":"Energy based models (EBM) consists of three ingredients:The energy function: to assign a scalar to each configuration of the variables.Inference: given a set of observed variables, to find values of the remaining variables that minimizes the energy.Learning: to find an energy function that assigns low energy values to correct configurations of the variables, and high energy values to incorrect configurations of the variables.Loss function: minimized during training, which measures to quality of the energy function.Loss functionsThe general form of the training objective of energy based model can be formulated as (LeCun et al. 2006): \\[ \\mathcal{L}(E,\\mathcal{S})=\\frac{1}{P}\\sum_{i=1}^P L(Y^i, E(W,\\mathcal{Y},X^i)) + R(W) \\]where \\(L\\) is the loss function that should \"pull-down\" the energy function at \"correct\" configurations (i.e., the training data \\((X^i, Y^i)\\)), and \"pull-up\" the energy function at incorrect configurations. \\(R\\) is the constraint on the model parameter \\(W\\), which may effectively restrict the VC-dimension of the model.Here are some popular choices of loss functions for energy based models:Energy loss\\[ L=E(W,Y^i,X^i) \\]which requires \\(L\\) to satisfy the property that: \"pull-down\" the energy at correct configurations will automatically \"pull-up\" the energy at incorrect configurations.Generalized Perceptron Loss\\[ L = E(W,Y^i,X^i) - \\min_{Y \\in \\mathcal{Y}} E(W,Y,X^i) \\]There is no mechanism for creating an energy gap between the correct configurations and the incorrect ones, which may potentially produce (almost) flat energy surfaces if the architecture allows it.Generalized Margin LossesHere is some common definitions of this section:\\(\\bar{Y}^i = \\arg\\min_{Y\\in \\mathcal{Y},\\left\\| Y-Y^i \\right\\| &gt; \\epsilon} E(W,Y,X^i)\\)Hinge loss\\[ L=\\max\\left(0, m+E(W,Y^i,X^i)-E(W,\\bar{Y}^i,X^i)\\right) \\]Log Loss\\[ L = \\log\\left( 1 + \\exp\\left(E(W,Y^i,X^i) - E(W,\\bar{Y}^i,X^i)\\right) \\right) \\]LVQ2 Loss\\[ L = \\max\\left( 0, m+E(W,Y^i,X^i) - E(W,\\bar{Y}^i, X^i) \\right) \\]MCE Loss\\[ L = \\sigma\\left( E(W,Y^i,X^i) - E(W,\\bar{Y}^i,X^i) \\right) \\]Square-Square Loss\\[ L = E(W,Y^i,X^i) + \\left( \\max\\left(0, m-E(W,\\bar{Y}^i,X^i)\\right)^2 \\right) \\]Square-Exponential Loss\\[ L = E(W,Y^i,X^i)^2 + \\gamma \\exp\\left( -E(W,\\bar{Y}^i,X^i) \\right) \\]Negative Log-Likelihood Loss (NLL Loss)\\[ \\begin{align} L &amp;= E(W,Y^i,X^i) + \\mathcal{F}_{\\beta}(W,\\mathcal{Y},X^i) \\\\ \\mathcal{F}_{\\beta}(W,\\mathcal{Y},X^i) &amp;= \\frac{1}{\\beta} \\log\\left( \\int_{y\\in \\mathcal{Y}}\\exp\\left( -\\beta E(W,y,X^i)\\right) \\,dy \\right) \\end{align} \\]where \\[ P(Y|X^i,W) = \\frac{\\exp\\left( -\\beta E(W,Y,X^i) \\right)}{\\int_{y\\in \\mathcal{Y}}\\exp\\left( -\\beta E(W,y,X^i)\\right) \\, dy} \\] Some authors have argued that the NLL loss puts too much emphasis on mistakes, which inspires the MEE loss.Minimum Empirical Error Loss (MEE Loss)\\[ L = 1 - P(Y|X^i,W) = 1 - \\frac{\\exp\\left( -\\beta E(W,Y,X^i) \\right)}{\\int_{y\\in \\mathcal{Y}}\\exp\\left( -\\beta E(W,y,X^i)\\right) \\, dy} \\]Learning with Approximate InferenceMany of the losses or algorithms used by energy based models does not guarantee that the energy for \\(E(W,Y,X^i)\\) is pull-up propertly at all \\(Y \\in \\mathcal{Y}, Y \\neq Y^i\\), such that \\(E(W,Y^i,X^i)\\) should be a global minimum.LeCun et al. (2006) justifies that, if learning is driven by approximated inference, such that all the constrastive samples found by the approximated inference algorithm was pull-up, then there is no need to worry about the far away samples which cannot be found by the inference algorithm (which I personally disagree with this, where it might cause some \"out-distribution\" problems).An example of such approximated inference driven learning algorithm is Contrastive Divergence:Contrastive Divergence\\[ W \\leftarrow W - \\eta \\left( \\frac{\\partial E(W,Y^i,X^i)}{\\partial W} - \\frac{\\partial E(W,\\bar{Y}^i,X)}{\\partial W} \\right) \\]ReferencesLeCun, Yann, Sumit Chopra, Raia Hadsell, M. Ranzato, and F. Huang. 2006. “A Tutorial on Energy-Based Learning.” Predicting Structured Data 1 (0).","categories":[{"name":"Deep_Learning","slug":"Deep-Learning","permalink":"https://wiki.haowen-xu.com/categories/Deep-Learning/"},{"name":"Energy_Based_Models","slug":"Deep-Learning/Energy-Based-Models","permalink":"https://wiki.haowen-xu.com/categories/Deep-Learning/Energy-Based-Models/"}],"tags":[]},{"title":"Sequential VAE","slug":"Deep_Learning/Variational_Autoencoder/Sequential_VAE","date":"2019-11-04T06:52:00.000Z","updated":"2020-06-01T12:15:59.012Z","comments":true,"path":"Deep_Learning/Variational_Autoencoder/Sequential_VAE/","link":"","permalink":"https://wiki.haowen-xu.com/Deep_Learning/Variational_Autoencoder/Sequential_VAE/","excerpt":"","text":"Sequential VAEs model observed sequence \\(\\mathbf{x}_1, \\mathbf{x}_2, \\dots, \\mathbf{x}_T\\) using latent sequence \\(\\mathbf{z}_1, \\mathbf{z}_2, \\dots, \\mathbf{z}_T\\).In this article, we use \\(\\mathbf{x}_{1:T}\\) and \\(\\mathbf{z}_{1:T}\\) to denote \\(\\mathbf{x}_1, \\mathbf{x}_2, \\dots, \\mathbf{x}_T\\) and \\(\\mathbf{z}_1, \\mathbf{z}_2, \\dots, \\mathbf{z}_T\\), respectively. Also, we use \\(\\mathbf{x}_{\\neg t}\\) to denote \\(\\mathbf{x}_1,\\dots,\\mathbf{x}_{t-1},\\mathbf{x}_{t+1},\\dots,\\mathbf{x}_T\\), and \\(\\mathbf{z}_{\\neg t}\\) likewise.The Future Dependency ProblemIn this section, we shall discuss the future dependency problem of \\(\\mathbf{z}_t\\) on \\(\\mathbf{x}_{t:T}\\) in the variational distribution \\(q_{\\phi}(\\mathbf{z}_{1:T}|\\mathbf{x}_{1:T})\\), via a simple hidden state markov chain model.Suppose \\(\\mathbf{z}_{1:T}\\) is a Markov chain, and serves as a sequence of hidden states that determines the \\(\\mathbf{x}_{1:T}\\) sequence. Formally, the probabilistic formulation of such a model can be written as: \\[ \\begin{align} p_{\\lambda}(\\mathbf{z}_{1:T}) &amp;= \\prod_{t=1}^T p_{\\lambda}(\\mathbf{z}_t|\\mathbf{z}_{t-1}) \\\\ p_{\\theta}(\\mathbf{x}_{1:T}|\\mathbf{z}_{1:T}) &amp;= \\prod_{t=1}^T p_{\\theta}(\\mathbf{x}_t|\\mathbf{z}_t) \\end{align} \\] This formulation can also be visualized as the following Bayesian network diagram:Figure 1: Hidden State Markov Chain ModelSurprisingly, the posterior distribution, i.e., \\(p_{\\theta}(\\mathbf{z}_{1:T}|\\mathbf{x}_{1:T})\\), exhibits future dependency, as is noticed by Fraccaro et al. (2016). Using d-separation, one can easily figure out that: \\[ p_{\\theta}(\\mathbf{z}_{1:T}|\\mathbf{x}_{1:T}) = \\prod_{t=1}^T p_{\\theta}(\\mathbf{z}_t|\\mathbf{z}_{t-1}, \\mathbf{x}_{t:T}) \\] Such independence relationship is also illustrated in the following diagram. Clearly, the observation of \\(\\mathbf{z}_{t-1}\\) blocks the dependency of \\(\\mathbf{z}_t\\) on \\(\\mathbf{x}_{1:(t-1)}\\) and \\(\\mathbf{z}_{1:(t-1)}\\), however, does not block its dependence on the future \\(\\mathbf{x}_{(t+1):T}\\).Figure 2: d-separation illustration for \\(\\mathbf{z}_t\\)The future dependency seems counter-intuitive at the first glance. However, this dependency can be naturally explained in the information theoretic perspective. Since the sequence \\(\\mathbf{x}_{t:T}\\) is generated with the influence of \\(\\mathbf{z}_t\\), i.e., the information of \\(\\mathbf{z}_t\\) flows into \\(\\mathbf{x}_{t:T}\\), then it should be not suprising that knowing the information of \\(\\mathbf{x}_{t:T}\\) is helpful to infer \\(\\mathbf{z}_t\\).VRNNChung et al. (2015) proposed to embed a variational autoencoder into each step of an LSTM or GRU recurrent network, formalized as: \\[ \\begin{align} p_{\\lambda}(\\mathbf{z}_t|\\mathbf{h}_{t-1}) &amp;= \\mathcal{N}(\\mathbf{z}_t|\\boldsymbol{\\mu}_{\\lambda,t}, \\boldsymbol{\\sigma}_{\\lambda,t}^2) \\\\ p_{\\theta}(\\mathbf{x}_t|\\mathbf{z}_t,\\mathbf{h}_{t-1}) &amp;= \\mathcal{N}(\\mathbf{x}_t| \\boldsymbol{\\mu}_{\\theta,t}, \\boldsymbol{\\sigma}_{\\theta,t}^2) \\\\ q_{\\phi}(\\mathbf{z}_t|\\mathbf{x}_t,\\mathbf{h}_{t-1}) &amp;= \\mathcal{N}(\\mathbf{z}_t| \\boldsymbol{\\mu}_{\\phi,t}, \\boldsymbol{\\sigma}_{\\phi,t}^2) \\\\ \\mathbf{h}_t &amp;= f_{\\text{rnn}}(f_{\\mathbf{x}}(\\mathbf{x}_t),f_{\\mathbf{z}}(\\mathbf{z}_t),\\mathbf{h}_{t-1}) \\\\ [\\boldsymbol{\\mu}_{\\lambda,t}, \\boldsymbol{\\sigma}_{\\lambda,t}] &amp;= f_{\\text{prior}}(\\mathbf{h}_{t-1}) \\\\ [\\boldsymbol{\\mu}_{\\theta,t}, \\boldsymbol{\\sigma}_{\\theta,t}] &amp;= f_{\\text{dec}}(f_{\\mathbf{z}}(\\mathbf{z}_t), \\mathbf{h}_{t-1}) \\\\ [\\boldsymbol{\\mu}_{\\phi,t}, \\boldsymbol{\\sigma}_{\\phi,t}] &amp;= f_{\\text{enc}}(f_{\\mathbf{x}}(\\mathbf{x}_t), \\mathbf{h}_{t-1}) \\end{align} \\] where \\(f_{\\mathbf{z}}\\) and \\(f_{\\mathbf{x}}\\) are feature networks shared among the prior, the encoder and the decoder, which are “crucial for learning complex sequences” according to Chung et al. (2015). \\(f_{\\text{rnn}}\\) is the transition kernel of the recurrent network, and \\(\\mathbf{h}_{1:T}\\) are the deterministic hidden states of the recurrent network. \\(f_{\\text{prior}}\\), \\(f_{\\text{enc}}\\) and \\(f_{\\text{dec}}\\) are the neural networks in the prior, the encoder and the decoder, respectively.The overall architecture of VRNN can be illustrated as the following figure, given by Chung et al. (2015):Figure 3: Overall architecture of VRNNThe following figure may provide a more clear illustration of the dependency among \\(\\mathbf{h}_{1:T}\\), \\(\\mathbf{x}_{1:T}\\) and \\(\\mathbf{z}_{1:T}\\) in the generative part:Figure 4: Dependency graph of VRNN in the generative partThe authors did not provide a theoretical analysis of the dependency relationship in their variational posterior \\(q_{\\phi}(\\mathbf{z}_{1:T}|\\mathbf{x}_{1:T})\\), but according to d-separation, we can easily figure out the correct dependency for the true posterior should be: \\[ p_{\\theta}(\\mathbf{z}_t|\\mathbf{z}_{1:(t-1)},\\mathbf{x}_{1:T},\\mathbf{h}_{1:T}) = p_{\\theta}(\\mathbf{z}_t|\\mathbf{h}_{t-1},\\mathbf{x}_t,\\mathbf{h}_t) \\] The dependency of \\(\\mathbf{z}_t\\) on future state \\(\\mathbf{h}_t\\) brings trouble for posterior inference. Chung et al. (2015) simply neglected this dependency. On the other hand, Fraccaro et al. (2016) considered such dependency and proposed SRNN, which brought us to a theoretically more reasonable factorization.SRNNFraccaro et al. (2016) proposed to factorize \\(\\mathbf{z}_{1:T}\\) as a state-space machine, depending on deterministic hidden state \\(\\mathbf{h}_{1:T}\\) of a recurrent network, and potentially the input \\(\\mathbf{u}_{1:T}\\) of each time step. The observation \\(\\mathbf{x}_t\\) of each time step is then assumed to depend only on \\(\\mathbf{d}_t\\) and \\(\\mathbf{z}_t\\). The overall architecture of SRNN is illustrated in the following figure (Fraccaro et al. 2016):Figure 5: Overall architecture of SRNNGenerative PartThe initial state is chosen to be \\(\\mathbf{z}_0 = \\mathbf{0}\\) and \\(\\mathbf{d}_0 = \\mathbf{0}\\). According to d-separation, the generative part is formulated as: \\[ \\begin{align} p_{\\lambda}(\\mathbf{z}_{1:T},\\mathbf{d}_{1:T}|\\mathbf{u}_{1:T},\\mathbf{z}_0,\\mathbf{d}_0) &amp;= \\prod_{t=1}^T p_{\\lambda}(\\mathbf{z}_t|\\mathbf{z}_{t-1},\\mathbf{d}_t) \\, p_{\\lambda}(\\mathbf{d}_t|\\mathbf{d}_{t-1},\\mathbf{u}_t) \\\\ p_{\\theta}(\\mathbf{x}_{1:T}|\\mathbf{z}_{1:T},\\mathbf{d}_{1:T},\\mathbf{u}_{1:T},\\mathbf{z}_0,\\mathbf{d}_0) &amp;= \\prod_{t=1}^T p_{\\theta}(\\mathbf{x}_t|\\mathbf{z}_t,\\mathbf{d}_t) \\end{align} \\]\\(p_{\\lambda}(\\mathbf{d}_t|\\mathbf{d}_{t-1},\\mathbf{u}_t)\\) is a dirac distribution, derived by \\(\\text{RNN}^{(p)}\\), a recurrent network: \\[ \\begin{align} p_{\\lambda}(\\mathbf{d}_t|\\mathbf{d}_{d-1},\\mathbf{u}_t) &amp;= \\delta(\\mathbf{d}_t-\\widetilde{\\mathbf{d}}_t) \\\\ \\widetilde{\\mathbf{d}}_t &amp;= \\text{RNN}^{(p)}(\\mathbf{d}_{t-1}, \\mathbf{u}_t) \\end{align} \\] \\(p_{\\lambda}(\\mathbf{z}_t|\\mathbf{z}_{t-1},\\mathbf{d}_t)\\) is a state-space machine, given by: \\[ \\begin{align} p_{\\lambda}(\\mathbf{z}_t|\\mathbf{z}_{t-1},\\mathbf{d}_t) &amp;= \\mathcal{N}(\\mathbf{z}_t| \\boldsymbol{\\mu}_{\\lambda}(\\mathbf{z}_{t-1},\\mathbf{d}_t), \\boldsymbol{\\sigma}^2_{\\lambda}(\\mathbf{z}_{t-1},\\mathbf{d}_t)) \\\\ \\boldsymbol{\\mu}_{\\lambda}(\\mathbf{z}_{t-1},\\mathbf{d}_t) &amp;= \\text{NN}^{(p)}_1(\\mathbf{z}_{t-1},\\mathbf{d}_t) \\\\ \\log \\boldsymbol{\\sigma}_{\\lambda}(\\mathbf{z}_{t-1},\\mathbf{d}_t) &amp;= \\text{NN}^{(p)}_2(\\mathbf{z}_{t-1},\\mathbf{d}_t) \\end{align} \\]\\(p_{\\theta}(\\mathbf{x}_t|\\mathbf{z}_t, \\mathbf{d}_t)\\) is derived by: \\[ p_{\\theta}(\\mathbf{x}_t|\\mathbf{z}_t,\\mathbf{d}_t) = \\mathcal{N}(\\mathbf{x}_t| \\boldsymbol{\\mu}_{\\theta}(\\mathbf{z}_t,\\mathbf{d}_t), \\boldsymbol{\\sigma}^2_{\\theta}(\\mathbf{z}_t,\\mathbf{d}_t)) \\]where \\(\\boldsymbol{\\mu}_{\\theta}(\\mathbf{z}_t,\\mathbf{d}_t)\\) and \\(\\boldsymbol{\\sigma}_{\\theta}(\\mathbf{z}_t,\\mathbf{d}_t)\\) use similar parameterization as in \\(p_{\\lambda}(\\mathbf{z}_t|\\mathbf{z}_{t-1},\\mathbf{d}_t)\\).Variational PartThe variational approximated posterior can be factorized as: \\[ \\begin{align} q_{\\phi}(\\mathbf{z}_{1:T},\\mathbf{d}_{1:T}|\\mathbf{x}_{1:T},\\mathbf{u}_{1:T},\\mathbf{d}_0,\\mathbf{u}_0) &amp;= q_{\\phi}(\\mathbf{z}_{1:T}|\\mathbf{d}_{1:T},\\mathbf{x}_{1:T},\\mathbf{u}_{1:T},\\mathbf{d}_0,\\mathbf{u}_0)\\, q_{\\phi}(\\mathbf{d}_{1:T}|\\mathbf{x}_{1:T},\\mathbf{u}_{1:T},\\mathbf{d}_0,\\mathbf{u}_0) \\end{align} \\]Since in the generative part, \\(p_{\\lambda}(\\mathbf{d}_t|\\mathbf{d}_{t-1},\\mathbf{u}_t)\\) is a dirac distribution, Fraccaro et al. (2016) decided to assume the second term in the above equation to be: \\[ q_{\\phi}(\\mathbf{d}_{1:T}|\\mathbf{x}_{1:T},\\mathbf{u}_{1:T},\\mathbf{d}_0,\\mathbf{u}_0) \\equiv p_{\\lambda}(\\mathbf{d}_{1:T}|\\mathbf{u}_{1:T},\\mathbf{d}_0,\\mathbf{u}_0) = \\prod_{t=1}^T p_{\\lambda}(\\mathbf{d}_t|\\mathbf{d}_{t-1},\\mathbf{u}_t) \\] That is, the same recurrent network \\(\\text{RNN}^{(p)}\\) is used to produce the deterministic states \\(\\mathbf{d}_{1:T}\\) in both the generative part and variational part.The first term is factorized according to d-separation, as: \\[ q_{\\phi}(\\mathbf{z}_{1:T}|\\mathbf{d}_{1:T},\\mathbf{x}_{1:T},\\mathbf{u}_{1:T},\\mathbf{d}_0,\\mathbf{u}_0) = \\prod_{t=1}^T q_{\\phi}(\\mathbf{z}_t|\\mathbf{z}_{t-1},\\mathbf{d}_{t:T},\\mathbf{x}_{t:T}) \\] where \\(q_{\\phi}(\\mathbf{z}_t|\\mathbf{z}_{t-1},\\mathbf{d}_{t:T},\\mathbf{x}_{t:T})\\) is derived by a reverse recurrent network \\(\\text{RNN}^{(q)}\\), whose hidden state was denoted as \\(\\mathbf{a}_t\\), as illustrated in Fig.&nbsp;5. The formalization of \\(q_{\\phi}(\\mathbf{z}_t|\\mathbf{z}_{t-1},\\mathbf{d}_{t:T},\\mathbf{x}_{t:T})\\) is: \\[ \\begin{align} q_{\\phi}(\\mathbf{z}_t|\\mathbf{z}_{t-1},\\mathbf{d}_{t:T},\\mathbf{x}_{t:T}) &amp;= q_{\\phi}(\\mathbf{z}_t|\\mathbf{z}_{t-1},\\mathbf{a}_t) \\\\ q_{\\phi}(\\mathbf{z}_t|\\mathbf{z}_{t-1},\\mathbf{a}_t) &amp;= \\mathcal{N}(\\mathbf{z}_t| \\boldsymbol{\\mu}_{\\phi}(\\mathbf{z}_{t-1},\\mathbf{a}_t), \\boldsymbol{\\sigma}_{\\phi}^2(\\mathbf{z}_{t-1},\\mathbf{a}_t)) \\\\ \\boldsymbol{\\mu}_{\\phi}(\\mathbf{z}_{t-1},\\mathbf{a}_t) &amp;= \\boldsymbol{\\mu}_{\\lambda}(\\mathbf{z}_{t-1},\\mathbf{d}_t) + \\text{NN}^{(q)}_1(\\mathbf{z}_{t-1},\\mathbf{a}_t) \\\\ \\log \\boldsymbol{\\sigma}_{\\phi}(\\mathbf{z}_{t-1},\\mathbf{a}_t) &amp;= \\text{NN}^{(q)}_2(\\mathbf{z}_{t-1},\\mathbf{a}_t) \\\\ \\mathbf{a}_t &amp;= \\text{RNN}^{(q)}(\\mathbf{a}_{t+1},[\\mathbf{d}_t,\\mathbf{x}_t]) \\end{align} \\] Notice \\(\\text{NN}^{(q)}_1(\\mathbf{z}_{t-1},\\mathbf{a}_t)\\) is adopted to learn the residual \\(\\boldsymbol{\\mu}_{\\phi}(\\mathbf{z}_{t-1},\\mathbf{a}_t) - \\boldsymbol{\\mu}_{\\lambda}(\\mathbf{z}_{t-1},\\mathbf{d}_t)\\), instead of \\(\\boldsymbol{\\mu}_{\\phi}(\\mathbf{z}_{t-1},\\mathbf{a}_t)\\) directly. Fraccaro et al. (2016) found that this residual parameterization can lead to better performance.ReferencesChung, Junyoung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron C. Courville, and Yoshua Bengio. 2015. “A Recurrent Latent Variable Model for Sequential Data.” In Advances in Neural Information Processing Systems, 2980–8.Fraccaro, Marco, Søren Kaae Sønderby, Ulrich Paquet, and Ole Winther. 2016. “Sequential Neural Models with Stochastic Layers.” In Advances in Neural Information Processing Systems, 2199–2207.","categories":[{"name":"Deep_Learning","slug":"Deep-Learning","permalink":"https://wiki.haowen-xu.com/categories/Deep-Learning/"},{"name":"Variational_Autoencoder","slug":"Deep-Learning/Variational-Autoencoder","permalink":"https://wiki.haowen-xu.com/categories/Deep-Learning/Variational-Autoencoder/"}],"tags":[]},{"title":"Overview","slug":"Deep_Learning/Variational_Autoencoder/Overview","date":"2019-11-03T11:01:00.000Z","updated":"2020-06-01T12:15:59.012Z","comments":true,"path":"Deep_Learning/Variational_Autoencoder/Overview/","link":"","permalink":"https://wiki.haowen-xu.com/Deep_Learning/Variational_Autoencoder/Overview/","excerpt":"","text":"Variational Autoencoder BasisThe Probabilistic Model FormulationVariational autoencoder, first proposed by Kingma and Welling (2014) and Rezende, Mohamed, and Wierstra (2014), was originally a deep Bayesian network composed of latent variable \\(\\mathbf{z}\\) and observed variable \\(\\mathbf{x}\\), formulated as: \\[ p(\\mathbf{x};\\theta,\\lambda) = \\int_{\\mathcal{Z}} p_{\\theta}(\\mathbf{x}|\\mathbf{z})\\,p_{\\lambda}(\\mathbf{z})\\,d\\mathbf{z} \\] where \\(p_{\\lambda}(\\mathbf{z})\\) is the prior for \\(\\mathbf{z}\\), either fixed, or derived by a neural network with parameter \\(\\lambda\\); and \\(p_{\\theta}(\\mathbf{x}|\\mathbf{z})\\) is derived by a neural network with parameter \\(\\theta\\). \\(p_{\\lambda}(\\mathbf{z})\\) is denoted as \\(p_{\\theta}(\\mathbf{z})\\) in some literature, including in its original papers.Variational Approximation of the PosteriorThe most significant design of variational autoencoder, which makes it much different from other generative models, is the use of variational inference to approximate the intractable \\(p_{\\theta}(\\mathbf{z}|\\mathbf{x}) = \\frac{p_{\\theta}(\\mathbf{x}|\\mathbf{z})\\,p_{\\lambda}(\\mathbf{z})}{\\int_{\\mathcal{Z}} p_{\\theta}(\\mathbf{x}|\\mathbf{w})\\,p_{\\lambda}(\\mathbf{w})\\,d\\mathbf{w}}\\), using a separatedly learned \\(q_{\\phi}(\\mathbf{z}|\\mathbf{x})\\), derived by a neural network with parameter \\(\\phi\\). The evidence lower-bound (ELBO) can be used to joinly train these components, formulated as: \\[ \\begin{aligned} \\log p(\\mathbf{x};\\theta,\\lambda) &amp;\\geq \\log p_{\\theta}(\\mathbf{x}) - \\operatorname{D}_{KL}\\big[ q_{\\phi}(\\mathbf{z}|\\mathbf{x})\\|p_{\\theta}(\\mathbf{z}|\\mathbf{x}) \\big] \\\\ &amp;= \\mathbb{E}_{\\mathbf{z} \\sim q_{\\phi}(\\mathbf{z}|\\mathbf{x})}\\big[ \\log p_{\\theta}(\\mathbf{x}) + \\log p_{\\theta}(\\mathbf{z}|\\mathbf{x}) - \\log q_{\\phi}(\\mathbf{z}|\\mathbf{x}) \\big] \\\\ &amp;= \\mathbb{E}_{\\mathbf{z} \\sim q_{\\phi}(\\mathbf{z}|\\mathbf{x})}\\big[ \\log p_{\\theta}(\\mathbf{x}|\\mathbf{z}) + \\log p_{\\lambda}(\\mathbf{z}) - \\log q_{\\phi}(\\mathbf{z}|\\mathbf{x}) \\big] \\\\ &amp;= \\mathbb{E}_{\\mathbf{z} \\sim q_{\\phi}(\\mathbf{z}|\\mathbf{x})}\\big[\\log p_{\\theta}(\\mathbf{x}|\\mathbf{z})\\big] - D_{\\mathrm{KL}}\\left( q_{\\phi}(\\mathbf{z}|\\mathbf{x}) \\| p_{\\lambda}(\\mathbf{z}) \\right) \\\\ &amp;= \\mathcal{L}(\\mathbf{x};\\theta,\\lambda,\\phi) \\end{aligned} \\]Kingma and Welling (2014) proposed to optimize ELBO using SGVB gradient estimator, which requires \\(q_{\\phi}(\\mathbf{z}|\\mathbf{x})\\) to be re-parameterized. Only some of the continuous distributions can be re-parameterized. For non-reparameterizable continuous distributions and discrete distributions, other gradient estimators may be adopted, which are reviewed in variational inference.The Auto-Encoding StructureThe pair of \\(q_{\\phi}(\\mathbf{z}|\\mathbf{x})\\) and \\(p_{\\theta}(\\mathbf{x}|\\mathbf{z})\\) resembles an autoencoder, where \\(q_{\\phi}(\\mathbf{z}|\\mathbf{x})\\) is the encoder, and \\(p_{\\theta}(\\mathbf{x}|\\mathbf{z})\\) is the decoder. In this perspective, \\(D_{\\mathrm{KL}}\\left( q_{\\phi}(\\mathbf{z}|\\mathbf{x}) \\| p_{\\lambda}(\\mathbf{z}) \\right)\\) becomes a regularization term to encourage a meaningful latent coding, which was further discussed in \\(\\beta\\)-VAE (Higgins et al. 2017; Burgess et al. 2018; Mathieu et al. 2018) and others.The auto-encoding structure is even more well-known and widely used than the probabilistic formulation of a variational autoencoder. Because of this, the term variational autoencoder now has been generalized to refer to a family of generative models, which learn stochastic encoders and infer latent variables by variational inference, rather than just the original model.Advanced Model ArchitecturesSome more advanced model architectures, which is composed of more than just one latent variable \\(\\mathbf{z}\\) and one observed variable \\(\\mathbf{x}\\), are reviewed in this section.Sequential VAETraining Variational AutoencoderStochastic gradient descentGradient Estimators for Variational InferenceReferencesBurgess, Christopher P., Irina Higgins, Arka Pal, Loic Matthey, Nick Watters, Guillaume Desjardins, and Alexander Lerchner. 2018. “Understanding Disentangling in $\\beta$-VAE.” arXiv:1804.03599 [Cs, Stat], April. http://arxiv.org/abs/1804.03599.Higgins, Irina, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. 2017. “Beta-Vae: Learning Basic Visual Concepts with a Constrained Variational Framework.” In International Conference on Learning Representations. Vol. 3.Kingma, Diederik P, and Max Welling. 2014. “Auto-Encoding Variational Bayes.” In Proceedings of the International Conference on Learning Representations.Mathieu, Emile, Tom Rainforth, N. Siddharth, and Yee Whye Teh. 2018. “Disentangling Disentanglement in Variational Autoencoders.” arXiv:1812.02833 [Cs, Stat], December. http://arxiv.org/abs/1812.02833.Rezende, Danilo Jimenez, Shakir Mohamed, and Daan Wierstra. 2014. “Stochastic Backpropagation and Approximate Inference in Deep Generative Models.” In Proceedings of the 31st International Conference on International Conference on Machine Learning - Volume 32, II–1278–II–1286. ICML’14. Beijing, China: JMLR.org.","categories":[{"name":"Deep_Learning","slug":"Deep-Learning","permalink":"https://wiki.haowen-xu.com/categories/Deep-Learning/"},{"name":"Variational_Autoencoder","slug":"Deep-Learning/Variational-Autoencoder","permalink":"https://wiki.haowen-xu.com/categories/Deep-Learning/Variational-Autoencoder/"}],"tags":[]},{"title":"Langevin Dynamics","slug":"Deep_Learning/Monte_Carlo_Methods/Langevin_Dynamics","date":"2019-10-21T07:08:00.000Z","updated":"2020-06-01T12:15:59.008Z","comments":true,"path":"Deep_Learning/Monte_Carlo_Methods/Langevin_Dynamics/","link":"","permalink":"https://wiki.haowen-xu.com/Deep_Learning/Monte_Carlo_Methods/Langevin_Dynamics/","excerpt":"","text":"Problem StatementTo sample from \\(p(\\mathbf{x})\\), where \\(p(\\mathbf{x})\\) is not easy to sample from, but the gradient \\(\\nabla_\\mathbf{x} \\log p(\\mathbf{x})\\) is tractable.Langevin DynamicsThe transition kernel \\(T\\) of Langevin dynamics is given by the following equation: \\[ \\begin{align} &amp;\\mathbf{x}^{(t+1)} = \\mathbf{x}^{(t)} + \\frac{\\epsilon^2}{2}\\cdot \\nabla_{\\mathbf{x}} \\log p(\\mathbf{x}^{(t)}) + \\epsilon\\cdot \\mathbf{z}^{(t)} \\\\ &amp;\\text{where}\\;\\,\\mathbf{z}^{(t)} \\sim \\mathcal{N}(\\mathbf{0},\\mathbf{I}) \\end{align} \\] and then Metropolis-Hastings algorithm is adopted to determine whether or not the new sample \\(\\mathbf{x}^{(t+1)}\\) should be accepted. The acceptance rate is given by: \\[ \\begin{align} \\rho(\\mathbf{x}^{(t)}, \\mathbf{x}^{(t+1)}) &amp;= \\min\\bigg\\{ 1, \\exp\\bigg( -\\log p(\\mathbf{x}^{(t)}) + \\log p(\\mathbf{x}^{(t+1)}) \\\\ &amp;\\qquad\\qquad\\qquad\\qquad+\\frac{1}{2} \\left\\| \\mathbf{z}^{(t)} \\right\\|^2_2 -\\frac{1}{2} \\left\\| \\mathbf{z}^{(t+1)} \\right\\|^2_2 \\bigg) \\bigg\\} \\\\ \\mathbf{z}^{(t+1)} &amp;= -\\mathbf{z}^{(t)} - \\frac{\\epsilon}{2}\\cdot \\left[ \\nabla_{\\mathbf{x}} \\log p(\\mathbf{x}^{(t)}) + \\nabla_{\\mathbf{x}} \\log p(\\mathbf{x}^{(t+1)}) \\right] \\end{align} \\]Langevin Dynamics as Metropolis-Hastings AlgorithmOne may also substitute out \\(\\mathbf{z}^{(t)}\\) and \\(\\mathbf{z}^{(t+1)}\\) using \\(\\mathbf{x}^{(t)}\\) and \\(\\mathbf{x}^{(t+1)}\\), such that the whole Langevin dynamics can be viewed as a strict Metropolis-Hastings transition kernel, as follows: \\[ \\begin{align} \\mathbf{z}^{(t)} &amp;= \\frac{1}{\\epsilon}\\left[ \\mathbf{x}^{(t+1)} - \\mathbf{x}^{(t)} - \\frac{\\epsilon^2}{2} \\nabla_{\\mathbf{x}} \\log p(\\mathbf{x}^{(t)}) \\right] \\\\ \\mathbf{z}^{(t+1)} &amp;= \\frac{1}{\\epsilon}\\left[ \\mathbf{x}^{(t)} - \\mathbf{x}^{(t+1)} - \\frac{\\epsilon^2}{2} \\nabla_{\\mathbf{x}} \\log p(\\mathbf{x}^{(t+1)}) \\right] \\end{align} \\] Also, we can write \\(\\rho(\\mathbf{x}^{(t)}, \\mathbf{x}^{(t+1)})\\) as: \\[ \\rho(\\mathbf{x}^{(t)}, \\mathbf{x}^{(t+1)}) = \\min\\left\\{ 1, \\frac{\\exp\\left( \\log p(\\mathbf{x}^{(t+1)}) \\right)}{\\exp\\left( \\log p(\\mathbf{x}^{(t)}) \\right)} \\cdot \\frac{\\exp\\left( -\\frac{1}{2}\\left\\| \\mathbf{z}^{(t+1)} \\right\\|^2_2 \\right)}{\\exp\\left( -\\frac{1}{2}\\left\\| \\mathbf{z}^{(t)} \\right\\|^2_2 \\right)} \\right\\} \\] where: \\[ \\begin{align} \\exp\\left( -\\frac{1}{2}\\left\\| \\mathbf{z}^{(t+1)} \\right\\|^2_2 \\right) &amp;= \\exp\\left( -\\frac{1}{2\\epsilon^2} \\left\\| \\mathbf{x}^{(t)} - \\left( \\mathbf{x}^{(t+1)} + \\frac{\\epsilon^2}{2} \\nabla_{\\mathbf{x}} \\log p(\\mathbf{x}^{(t+1)}) \\right) \\right\\|^2_2 \\right) \\\\ \\exp\\left( -\\frac{1}{2}\\left\\| \\mathbf{z}^{(t)} \\right\\|^2_2 \\right) &amp;= \\exp\\left( -\\frac{1}{2\\epsilon^2} \\left\\| \\mathbf{x}^{(t+1)} - \\left( \\mathbf{x}^{(t)} + \\frac{\\epsilon^2}{2} \\nabla_{\\mathbf{x}} \\log p(\\mathbf{x}^{(t)}) \\right) \\right\\|^2_2 \\right) \\end{align} \\] The ratio \\(\\exp\\left( -\\frac{1}{2}\\left\\| \\mathbf{z}^{(t+1)} \\right\\|^2_2 \\right) / \\exp\\left( -\\frac{1}{2}\\left\\| \\mathbf{z}^{(t)} \\right\\|^2_2 \\right)\\) can be viewed as \\(q(\\mathbf{x}^{(t)}|\\mathbf{x}^{(t+1)}) / q(\\mathbf{x}^{(t+1)}|\\mathbf{x}^{(t)})\\), where: \\[ \\begin{align} q(\\mathbf{x}^{(t)}|\\mathbf{x}^{(t+1)}) &amp;= \\mathcal{N}\\left(\\mathbf{x}^{(t)}\\,\\bigg|\\,\\mathbf{x}^{(t+1)} + \\frac{\\epsilon^2}{2} \\nabla_{\\mathbf{x}} \\log p(\\mathbf{x}^{(t+1)}), \\epsilon^2\\right) \\\\ q(\\mathbf{x}^{(t+1)}|\\mathbf{x}^{(t)}) &amp;= \\mathcal{N}\\left(\\mathbf{x}^{(t+1)}\\,\\bigg|\\,\\mathbf{x}^{(t)} + \\frac{\\epsilon^2}{2} \\nabla_{\\mathbf{x}} \\log p(\\mathbf{x}^{(t)}), \\epsilon^2\\right) \\end{align} \\] which are two Normal distributions with mean in the form of \\(\\mathbf{x}+\\frac{\\epsilon^2}{2}\\nabla_{\\mathbf{x}} \\log p(\\mathbf{x})\\) and variance being \\(\\epsilon^2\\). Also, since \\(p(\\mathbf{x}) = \\exp\\left( \\log p(\\mathbf{x}) \\right)\\), we finally obtain: \\[ \\rho(\\mathbf{x}^{(t)}, \\mathbf{x}^{(t+1)}) = \\min\\left\\{ 1, \\frac{p(\\mathbf{x}^{(t+1)})}{p(\\mathbf{x}^{(t)})} \\cdot \\frac{q(\\mathbf{x}^{(t)}|\\mathbf{x}^{(t+1)})}{q(\\mathbf{x}^{(t+1)}|\\mathbf{x}^{(t)})} \\right\\} \\] which is exactly the acceptance rate of Metropolis-Hastings algorithm, with target distribution \\(p(\\mathbf{x})\\) and proposal distribution \\(q(\\mathbf{x}^{\\star}|\\mathbf{x})\\).Relationship with Hamiltonian Monte CarloLangevin dyanmics is actually a Hamiltonian dynamics system, with the potential and kinetic energy chosen as: \\[ \\begin{align} U(q) &amp;= -\\log \\pi(q) \\\\ K(p|q) &amp;= K(p) = -\\log \\mathcal{N}(p|0,I) = \\frac{1}{2} p^\\top p + \\text{const} = \\frac{1}{2}\\left\\| p \\right\\|^2_2 + \\text{const} \\end{align} \\] and the integrator for the Hamiltonian equations chosen as one-step LeapFrog method.The Hamiltonian equations, under this assumption, are given by: \\[ \\begin{align} \\frac{dq}{dt} &amp;= \\frac{\\partial K}{\\partial p} = p \\\\ \\frac{dp}{dt} &amp;= -\\frac{\\partial U}{\\partial q} = \\nabla \\log \\pi(q) \\end{align} \\]Using one-step LeapFrog integrator, we obtain: \\[ \\begin{align} p_{0.5} &amp;= p_0 - \\frac{\\epsilon}{2}\\cdot \\frac{\\partial U}{\\partial q}(q_0) \\\\ q_1 &amp;= q_0 + \\epsilon \\cdot \\frac{\\partial K}{\\partial p}(p_{0.5}) = q_0 - \\frac{\\epsilon^2}{2} \\cdot \\frac{\\partial U}{\\partial q}(q_0) + \\epsilon \\cdot p_0 \\\\ p_1 &amp;= p_{0.5} - \\frac{\\epsilon}{2} \\cdot \\frac{\\partial U}{\\partial q}(q_1) = p_0 - \\frac{\\epsilon}{2} \\cdot \\frac{\\partial U}{\\partial q}(q_0) - \\frac{\\epsilon}{2} \\cdot \\frac{\\partial U}{\\partial q}(q_1) \\end{align} \\] Negate \\(p_1\\), we obtain the candidate state \\((q_1,-p_1)\\). The Metropolis-Hastings acceptance rate is then given by:\\[ \\begin{align} \\rho((q_0,p_0),(q_1,-p_1)) &amp;= \\min\\left\\{ 1, \\frac{\\pi(q_1,-p_1)}{\\pi(q_0,p_0)} \\right\\} \\\\ &amp;= \\min\\left\\{ 1, \\frac{\\exp\\left( -H(q_1,-p_1) \\right)}{\\exp\\left( -H(q_0,p_0) \\right)} \\right\\} \\\\ &amp;= \\min\\left\\{ 1, \\exp\\left( U(q_0) - U(q_1) + K(p_0) - K(-p_1) \\right) \\right\\} \\\\ &amp;= \\min\\left\\{ 1, \\exp\\left( -\\log \\pi(q_0) + \\log \\pi(q_1) + \\frac{1}{2} \\left\\| p_0 \\right\\|^2_2 - \\frac{1}{2} \\left\\| -p_1 \\right\\|^2_2 \\right) \\right\\} \\end{align} \\] Substitute \\(q_0 = \\mathbf{x}^{(t)}\\), \\(q_1 = \\mathbf{x}^{(t+1)}\\) and \\(p_0=\\mathbf{z}^{(t)}\\), \\(-p_1=\\mathbf{z}^{(t+1)}\\), we obtain the transition kernel and the acceptance rate for Langevin dynamics in the previous section.Further Reading MaterialsSee Neal (2011).ReferencesNeal, Radford M. 2011. “MCMC Using Hamiltonian Dynamics.” Handbook of Markov Chain Monte Carlo 2 (11).","categories":[{"name":"Deep_Learning","slug":"Deep-Learning","permalink":"https://wiki.haowen-xu.com/categories/Deep-Learning/"},{"name":"Monte_Carlo_Methods","slug":"Deep-Learning/Monte-Carlo-Methods","permalink":"https://wiki.haowen-xu.com/categories/Deep-Learning/Monte-Carlo-Methods/"}],"tags":[]},{"title":"Hamiltonian Dynamics","slug":"Deep_Learning/Monte_Carlo_Methods/Hamiltonian_Dynamics","date":"2019-10-19T07:09:00.000Z","updated":"2020-06-01T12:15:59.008Z","comments":true,"path":"Deep_Learning/Monte_Carlo_Methods/Hamiltonian_Dynamics/","link":"","permalink":"https://wiki.haowen-xu.com/Deep_Learning/Monte_Carlo_Methods/Hamiltonian_Dynamics/","excerpt":"","text":"Problem StatementTo sample from \\(\\pi(q)\\), where \\(\\pi(q)\\) is not easy to sample from, but given a sample \\(q\\), the density of \\(\\pi(q)\\) is easy to evaluate, or at least the unnormalized density \\(\\tilde{\\pi}(q)\\) is easy to calculate.Note: as a convention of the Hamiltonian Monte Carlo method literature, the notations for vectors and matrices are not bold.Hamiltonian DynamicsBasic ElementsThe Hamiltonian Monte Carlo method uses Hamiltonian dynamics system to derive the transition kernel \\(T\\). It auguments the state space \\(q\\) with a momentum \\(p\\), forming the extended phase space \\((q, p)\\). A Hamiltonian system is then constructed for this phase space, with Hamiltonian function \\(H(q,p)\\), i.e., the total energy of this system.The density for the phase space, \\(\\pi(q,p)\\) is then given by: \\[ \\begin{align} \\pi(q, p) &amp;= \\exp\\left( -H(q,p) \\right) \\end{align} \\] satisfying \\(\\int \\pi(q,p)\\,dp = \\pi(q)\\).Since \\(H(q,p)\\) is the energy function of a Hamiltonian system, it can further be decomposed into kinetic and potential energies: \\[ \\begin{align} H(q,p) &amp;= -\\log \\pi(q,p) \\\\ &amp;= -\\log \\pi(p|q) - \\log \\pi(q) \\\\ &amp;= K(p,q) + U(q) \\end{align} \\] where \\(K(p,q) = -\\log \\pi(p|q)\\) is the Kinetic energy, and \\(U(q) = -\\log \\pi(q)\\) is the potential energy.Transition KernelThe evolution of a Hamiltonian system over time is given by the following differential equations: \\[ \\begin{align} \\frac{d q}{dt} &amp;= \\frac{\\partial H}{\\partial p} = \\frac{\\partial K}{\\partial p} \\\\ \\frac{d p}{dt} &amp;= -\\frac{\\partial H}{\\partial q} = -\\frac{\\partial K}{\\partial q} - \\frac{\\partial U}{\\partial q} \\end{align} \\] Let \\(z=(q,p)\\), the Hamiltonian equations can also be rewritten in the following form: \\[ \\begin{align} \\frac{dz}{\\mathrm{d}t} = J \\,\\nabla H(z) \\end{align} \\] where \\(\\nabla H\\) is the gradient of \\(H\\), and: \\[ J = \\begin{bmatrix} 0 &amp; I \\\\ -I &amp; 0 \\\\ \\end{bmatrix} \\]The transition kernel \\(T\\) is then given by the following procedure:Lift \\(q^{(t)}\\) to \\((q^{(t)},p^{(t)})\\), by sampling \\(p^{(t)} \\sim \\pi(p|q^{(t)})\\).Simulate the Hamiltonian system, by integrating the Hamiltonian differential equations, to obtain candidate state \\((q^*, p^*)\\).For reversibility, negate \\(p^{\\star}\\), such that the candidate state becomes \\((q^{\\star},-p^{\\star})\\) (see below).To correct numerical errors, apply Metropolis-Hastings acceptance rate on the candidate state \\((q^{\\star}, -p^{\\star})\\), to obtain the final state \\((q^{(t+1)},p^{(t+1)})\\).Discard \\(p^{(t+1)}\\) and use \\(q^{(t+1)}\\) as the final sample of the Markov chain.Choices for \\(K(p,q)\\) (or equivalently, \\(\\pi(p|q)\\))1. Euclidean-Gaussian Kinetic EnergyThis is perhaps the most commonly used kinetic energy, given by:\\[ \\begin{align} K(p|q) &amp;= \\frac{1}{2} p^T M^{-1} p + \\log \\left| M \\right| + \\text{const} \\\\ \\pi(p|q) &amp;\\propto \\exp\\left( -\\frac{1}{2} p^T M^{-1} p \\right) \\end{align} \\]Note \\(\\pi(p|q) = \\mathcal{N}(p\\,|\\,0,M)\\) is a Gaussian distribution independent of \\(q\\).2. Riemannian-Gaussian Kinetic Energy \\[ \\begin{align} K(p|q) &amp;= \\frac{1}{2} \\cdot p^T \\cdot \\Sigma^{-1}(q) \\cdot p + \\log \\left| \\Sigma(q) \\right| + \\text{const} \\\\ \\pi(p|q) &amp;\\propto \\exp\\left( -\\frac{1}{2} p^T \\cdot \\Sigma(q)^{-1}\\cdot p \\right) \\end{align} \\]Note \\(\\pi(p|q) = \\mathcal{N}(p|0,\\Sigma(q))\\) is a Gaussian distribution dependent of \\(q\\). Such dependence can reflect the local geometry of the target distribution, thus can be better than Euclidean-Gaussian Kinetic Energy.Numerical Integrator for the Hamiltonian EquationsNumerical integrators are adopted to simulate a Hamiltonian dynamics system. Given the size of simulation time step (denoted as \\(\\epsilon\\)), and the total number of simulation steps to run (denoted as \\(n\\)), a numerical integrator transforms an initial state \\((q(t),p(t))\\) to \\((q(t+n\\epsilon), p(t+n\\epsilon))\\).For simplicity, in this section, the initial state fed into the numerical integrator is denoted as \\((q_0, p_0)\\) (which is \\((q(t),p(t))\\)), while the transformed state after the \\(k\\)-th simulation step is denoted as \\((q_k, p_k)\\) (which is \\((q(t+k\\epsilon),p(t+k\\epsilon))\\)).1. Euler's MethodEach step of Euler's method is given by: \\[ \\begin{align} p_{k+1} &amp;= p_k + \\epsilon \\cdot \\frac{dp}{dt}(t + k\\epsilon) = p_k - \\epsilon\\cdot\\left[ \\frac{\\partial K}{\\partial q}(p_k, q_k) + \\frac{\\partial U}{\\partial q}(q_k) \\right] \\\\ q_{k+1} &amp;= q_k + \\epsilon \\cdot \\frac{dq}{dt}(t+k\\epsilon) = q_k + \\epsilon\\cdot\\frac{\\partial K}{\\partial p}(p_k,q_k) \\end{align} \\]However, Euler's method is not sympletic, and is prone to divergence.2. LeapFrog MethodWhen \\(K(p,q)\\) is independent of \\(q\\), i.e., \\(K(p,q) = K(p)\\) and \\(\\pi(p|q) = \\pi(p)\\), LeapFrog method can be used, which is sympletic and better than Euler's method. We first state the Hamiltonian equations under this new assumption: \\[ \\begin{align} \\frac{d q}{dt} &amp;= \\frac{\\partial H(q,p)}{\\partial p} = \\frac{\\partial K}{\\partial p} \\\\ \\frac{d p}{dt} &amp;= -\\frac{\\partial H}{\\partial q} = -\\frac{\\partial U}{\\partial q} \\end{align} \\] Each step of the LeapFrog method is then given by: \\[ \\begin{align} p_{k+0.5} &amp;= p_k + \\frac{\\epsilon}{2} \\cdot \\frac{dp}{dt}(t+k\\epsilon) = p_k - \\frac{\\epsilon}{2} \\cdot \\frac{\\partial U}{\\partial q}(q_k) \\\\ q_{k+1} &amp;= q_k + \\epsilon \\cdot \\frac{dq}{dt}(t+(k+0.5)\\epsilon) = q_k + \\epsilon \\cdot \\frac{\\partial K}{\\partial p}(p_{k+0.5}) \\\\ p_{k+1} &amp;= p_{k+0.5} + \\frac{\\epsilon}{2} \\cdot \\frac{dp}{dt}(t+\\epsilon) = p_{k+0.5} - \\frac{\\epsilon}{2} \\cdot \\frac{\\partial U}{\\partial q}(q_{k+1}) \\end{align} \\] Note in a practical implementation, when the desired number of steps \\(n\\) is larger than 1, the intermediate states \\(p_k\\) can be omitted, with only \\(p_{k+0.5}\\) actually computed. The whole LeapFrog method with \\(n\\) simulation steps can be given by:\\(p_{0.5} = p_0 - \\frac{\\epsilon}{2} \\cdot \\frac{\\partial U}{\\partial q}(q_0)\\)For each \\(k = 0 \\dots n - 2\\)\\(q_{k+1} = q_k + \\epsilon \\cdot \\frac{\\partial K}{\\partial p}(p_{k+0.5})\\)\\(p_{k+1.5} = p_{k+0.5} - \\epsilon \\cdot \\frac{\\partial U}{\\partial q}(q_{k+1})\\)\\(q_n = q_{n-1} + \\epsilon \\cdot \\frac{\\partial K}{\\partial p}(p_{n-0.5})\\)\\(p_n = p_{n-0.5} - \\frac{\\epsilon}{2} \\cdot \\frac{\\partial U}{\\partial q}(q_n)\\)Metropolis-Hastings Acceptance RateAfter applying a reversible numerical integrator, such that the candidate \\((q^{(t)},p^{(t)}) \\to (q^{\\star},-p^{\\star})\\) has been proposed, Metropolis-Hastings acceptance rate should be calculated, in order to determine whether or not to accept the new state. The rate is given by: \\[ \\begin{align} \\rho((q^{(t)}, p^{(t)}), (q^{\\star}, -p^{\\star})) &amp;= \\min\\left\\{ 1, \\frac{\\pi(q^{\\star},-p^{\\star})}{\\pi(q,p)} \\frac{Q(q^{(t)},p^{(t)}|q^{\\star},-p^{\\star})}{Q(q^{\\star},-p^{\\star}|q^{(t)},p^{(t)})} \\right\\} \\\\ &amp;= \\min\\left\\{ 1, \\frac{\\pi(q^{\\star},-p^{\\star})}{\\pi(q,p)} \\right\\} \\\\ &amp;= \\min\\left\\{ 1, \\frac{\\exp\\left(-H(q^{\\star},-p^{\\star}) \\right)}{\\exp\\left(-H(q^{(t)},p^{(t)}) \\right)} \\right\\} \\\\ &amp;= \\min\\left\\{ 1, \\exp\\left( H(q^{(t)},p^{(t)}) - H(q^{\\star},-p^{\\star}) \\right) \\right\\} \\end{align} \\]The proposal distribution \\(Q(q^{(t)},p^{(t)}|q^{\\star},-p^{\\star}) \\equiv 1\\) and \\(Q(q^{\\star},-p^{\\star}|q^{(t)},p^{(t)}) \\equiv 1\\), since the integrator is reversible and deterministic.Further Reading MaterialsSee Betancourt (2017), Neal (2011), Carroll (2019) and Young (n.d.).ReferencesBetancourt, Michael. 2017. “A Conceptual Introduction to Hamiltonian Monte Carlo.” arXiv:1701.02434 [Stat], January. http://arxiv.org/abs/1701.02434.Carroll, Colin. 2019. “Hamiltonian Monte Carlo from Scratch.” Colin Carroll. https://colindcarroll.com/2019/04/11/hamiltonian-monte-carlo-from-scratch/.Neal, Radford M. 2011. “MCMC Using Hamiltonian Dynamics.” Handbook of Markov Chain Monte Carlo 2 (11).Young, Peter. n.d. “The Leapfrog Method and Other ‘Symplectic’ Algorithms for Integrating Newton’s Laws of Motion,” 15.","categories":[{"name":"Deep_Learning","slug":"Deep-Learning","permalink":"https://wiki.haowen-xu.com/categories/Deep-Learning/"},{"name":"Monte_Carlo_Methods","slug":"Deep-Learning/Monte-Carlo-Methods","permalink":"https://wiki.haowen-xu.com/categories/Deep-Learning/Monte-Carlo-Methods/"}],"tags":[]},{"title":"Gibbs Sampler","slug":"Deep_Learning/Monte_Carlo_Methods/Gibbs_Sampler","date":"2019-10-18T09:11:02.000Z","updated":"2020-06-01T12:15:59.008Z","comments":true,"path":"Deep_Learning/Monte_Carlo_Methods/Gibbs_Sampler/","link":"","permalink":"https://wiki.haowen-xu.com/Deep_Learning/Monte_Carlo_Methods/Gibbs_Sampler/","excerpt":"","text":"Problem StatementTo sample from \\(p(\\mathbf{x})\\) where \\(\\mathbf{x}\\) is a \\(D\\)-dimensional vector. \\(p(\\mathbf{x})\\) is not easy to sample from directly, but each factorized conditional distribution \\(p(x_i|\\mathbf{x}_{\\neg i})\\), where \\(\\mathbf{x}_{\\neg i} = (x_1, \\dots, x_{i-1}, x_{i+1}, \\dots, x_D)\\), is easy to sample from.Gibbs SamplerGiven \\(\\mathbf{x}^{(t)}\\), \\(\\mathbf{X}^{(t+1)}\\) is generated by the following procedure: \\[ \\begin{align} X_1^{(t+1)} &amp;\\sim p(x_1|x_2^{(t)}, \\dots, x_D^{(t)}) \\\\ X_2^{(t+1)} &amp;\\sim p(x_2|x_1^{(t+1)}, x_3^{(t)}, \\dots, x_D^{(t)}) \\\\ &amp; \\vdots \\\\ X_D^{(t+1)} &amp;\\sim p(x_D|x_1^{(t+1)}, \\dots, x_{D-1}^{(t+1)}) \\end{align} \\]Irreducibility of Gibbs SamplerThe Gibbs sampler might not always be irreducible for a given state distribution \\(p(\\mathbf{x})\\). A counterexample is given in \"Monte Carlo Statistical Methods\", Example 7.1.10.1 A sufficient condition for the chain to be irreducible is the positivity condition:Let \\((X_1, \\dots, X_D) \\sim p(x_1, \\dots, x_D)\\), where \\(p_i\\) denotes the marginal distribution of \\(X_i\\). If \\(p_i(x_i) &gt; 0\\) for all \\(i = 1,\\dots,D\\) implies that \\(p(x_1,\\dots,x_D) &gt; 0\\), then \\(p\\) satisfies the positivity condition.And we further have:Gibbs sampler derived from \\(p\\) satisfying the positivity condition is irreducible.Robert, C. P., &amp; Casella, G. (2005). Monte Carlo Statistical Methods (Springer Texts in Statistics). Secaucus, NJ, USA: Springer-Verlag New York, Inc.↩︎","categories":[{"name":"Deep_Learning","slug":"Deep-Learning","permalink":"https://wiki.haowen-xu.com/categories/Deep-Learning/"},{"name":"Monte_Carlo_Methods","slug":"Deep-Learning/Monte-Carlo-Methods","permalink":"https://wiki.haowen-xu.com/categories/Deep-Learning/Monte-Carlo-Methods/"}],"tags":[]},{"title":"Metropolis-Hastings Algorithm","slug":"Deep_Learning/Monte_Carlo_Methods/Metropolis_Hastings_Algorithm","date":"2019-10-18T09:11:01.000Z","updated":"2020-06-01T12:15:59.008Z","comments":true,"path":"Deep_Learning/Monte_Carlo_Methods/Metropolis_Hastings_Algorithm/","link":"","permalink":"https://wiki.haowen-xu.com/Deep_Learning/Monte_Carlo_Methods/Metropolis_Hastings_Algorithm/","excerpt":"","text":"Problem StatementTo sample from \\(p(x)\\), where \\(p(x)\\) is not easy to sample from, but given a sample \\(x\\), the density of \\(p(x)\\) is easy to evaluate, or at least the unnormalized density \\(\\tilde{p}(x)\\) is easy to calculate.Metropolis-Hastings AlgorithmGiven \\(x^{(t)}\\), \\(X^{(t+1)}\\) is generated by the following procedure, with a proposal distribution \\(q(y|x)\\):\\[ \\begin{align} Y^{(t)} &amp;\\sim q(y|x^{(t)}) \\\\ X^{(t+1)} &amp;= \\begin{cases} Y^{(t)} &amp; \\text{with probability} &amp; \\rho(x^{(t)}, Y^{(t)}) \\\\ x^{(t)} &amp; \\text{with probability} &amp; 1 - \\rho(x^{(t)}, Y^{(t)}) \\end{cases} \\end{align} \\] where \\[ \\rho(x,y) = \\min\\left\\{ 1, \\frac{p(y)}{p(x)} \\frac{q(x|y)}{q(y|x)} \\right\\} \\] Note the transition kernel of such a chain can be formulated as: \\[ \\begin{align} T(x,y) &amp;= \\rho(x,y)\\,q(y|x) + (1-r(x))\\,\\delta_x(y) \\\\ r(x) &amp;= \\int \\rho(x,y)\\,q(y|x)\\,dy \\end{align} \\] where \\(\\delta_x\\) is the Dirac mass in \\(x\\). It is easy to verify that \\(T\\) is reversible.Metropolis-Hastings algorithm is easy to implement in multiple situations, in any one of the following situations:When \\(q\\) is reversible, i.e., \\(q(x|y) = q(y|x)\\), and \\(p(x)\\) or \\(\\tilde{p}(x)\\) is easy to calculate.When \\(p(y) / q(y|x)\\) is independent of \\(x\\), and easy to calculate.","categories":[{"name":"Deep_Learning","slug":"Deep-Learning","permalink":"https://wiki.haowen-xu.com/categories/Deep-Learning/"},{"name":"Monte_Carlo_Methods","slug":"Deep-Learning/Monte-Carlo-Methods","permalink":"https://wiki.haowen-xu.com/categories/Deep-Learning/Monte-Carlo-Methods/"}],"tags":[]},{"title":"Markov Chain","slug":"Deep_Learning/Monte_Carlo_Methods/Markov_Chain","date":"2019-10-18T09:11:00.000Z","updated":"2020-06-01T12:15:59.008Z","comments":true,"path":"Deep_Learning/Monte_Carlo_Methods/Markov_Chain/","link":"","permalink":"https://wiki.haowen-xu.com/Deep_Learning/Monte_Carlo_Methods/Markov_Chain/","excerpt":"","text":"Elements of Markov ChainA measurable state space \\(\\mathcal{X}\\).A transition kernel \\(T: \\mathcal{X} \\mapsto \\mathcal{X}\\).For discrete state space \\(\\mathcal{X}\\), the transition kernel can also be written as a transition matrix, where \\(P_{ij}\\) is the probability of state \\(i\\) transit to state \\(j\\).Given an initial state distribution \\(\\pi_0(x)\\), the distribution after \\(k\\)-cycle MCMC transition is \\(\\pi_k(x)\\), defined as: \\[ \\pi_k(x) = (T \\pi_{k-1})(x) = \\int_{\\mathcal{X}} \\pi_{k-1}(y) \\,T(y, x)\\,dy \\] We shall use \\(T^k\\) to denote such a \\(k\\)-cycle transition, such that: \\[ \\pi_k(x) = (T^k \\pi_0)(x) = \\int_{\\mathcal{X}} \\pi_0(y) \\,T^k(y, x)\\,dy \\]Stationary distribution: if there exists \\(\\pi(x)\\) such that \\(\\pi = T\\pi\\), then \\(\\pi\\) is a stationary distribution of the Markov chain derived by \\(T\\), denoted as \\(\\pi^{\\star}(x)\\).Ergodicity of Markov ChainTODO: Write about ergodicity, instead of separated \"irreducible\" and \"aperiodic\".A state i is said to be ergodic if it is aperiodic and positive recurrent.-- https://en.wikipedia.org/wiki/Markov_chain#ErgodicityExistence and Uniqueness of the Stationary DistributionThe necessary condition for a unique stationary distribution \\(\\pi^{\\star}(x)\\) of a Markov chain to exist is that the chain is irreducible and aperiodic.Irreducible:A Markov chain is irreducible if \\(\\forall x,\\,y\\), there exists \\(k \\in \\mathbb{N}\\), such that \\(T^k(x,y) &gt; 0\\).Aperiodic:The period of a state \\(x\\) is defined as: \\(\\mathrm{gcd}\\left\\{k &gt; 0 : T^k(x,x) &gt; 0\\right\\}\\).A state \\(x\\) is aperiodic if the period of \\(x\\) is 1.A Markov chain is aperiodic if every state of this chain is aperiodic.If there is an aperiodic state in an irreducible Markov chain, then all states of this Markov chain is aperiodic.Detailed Balance ConditionIn practice, the transition kernel is often chosen to satisfy the detailed balance condition. If there exists a state distribution \\(\\pi(x)\\), such that \\(\\forall x, \\, y\\), \\[ \\pi(x) \\, T(x,y) = \\pi(y) \\, T(y, x) \\]We say that \\(T\\) satisfies the detailed balance condition. Furthermore, in such situation, \\(\\pi\\) is the stationary distribution of the Markov chain, and the chain is reversible.If the Markov chain is further irreducible and aperiodic, then \\(\\pi\\) is the unique stationary distribution.Hybird Markov ChainIf \\(T_1\\) and \\(T_2\\) are two kernels with the same stationary distribution \\(\\pi^{\\star}\\), and if \\(T_1\\) produces an irreducible Markov chain, then the mixture kernel: \\[ T = \\alpha T_1 + (1-\\alpha) T_2 \\qquad\\qquad (0 &lt; \\alpha &lt; 1) \\] is also irreducible.","categories":[{"name":"Deep_Learning","slug":"Deep-Learning","permalink":"https://wiki.haowen-xu.com/categories/Deep-Learning/"},{"name":"Monte_Carlo_Methods","slug":"Deep-Learning/Monte-Carlo-Methods","permalink":"https://wiki.haowen-xu.com/categories/Deep-Learning/Monte-Carlo-Methods/"}],"tags":[]},{"title":"Accept-Reject Sampling","slug":"Deep_Learning/Monte_Carlo_Methods/Accept_Reject_Sampling","date":"2019-10-18T06:15:01.000Z","updated":"2020-06-01T12:15:59.008Z","comments":true,"path":"Deep_Learning/Monte_Carlo_Methods/Accept_Reject_Sampling/","link":"","permalink":"https://wiki.haowen-xu.com/Deep_Learning/Monte_Carlo_Methods/Accept_Reject_Sampling/","excerpt":"","text":"Problem StatementTo sample from \\(p(x)\\), where \\(p(x)\\) is not easy to sample from, but given a sample \\(x\\), the density of \\(p(x)\\) is easy to evaluate.Accept-Reject SamplingGiven that \\(p(x) \\leq M q(x)\\) for \\(0 &lt; M &lt; \\infty\\), we can sample \\(x \\sim p(x)\\) by:Sample \\(X \\sim q(x)\\), \\(U \\sim \\mathcal{U}[0,1]\\).Accept \\(Y=X\\) if \\(U &lt; p(X) / M q(X)\\).Return to 1 otherwise.The correctness of this method can be proven by:\\[ \\begin{aligned} P(Y\\leq y) &amp;= P\\left(X \\leq y \\,\\Big|\\, U \\leq \\frac{p(X)}{M q(X)}\\right) = \\frac{P\\left(X \\leq y, U \\leq \\frac{p(X)}{M q(X)}\\right)}{P\\left(U \\leq \\frac{p(X)}{M q(X)}\\right)} \\\\ &amp;= \\frac{\\int_{-\\infty}^y \\int_0^{p(x)/M q(x)} \\mathrm{d}{u}\\,q(x)\\,\\mathrm{d}{x}} {\\int_{-\\infty}^{\\infty} \\int_0^{p(x)/M q(x)} \\mathrm{d}{u}\\,q(x)\\,\\mathrm{d}{x}} = \\frac{\\frac{1}{M}\\,\\int_{-\\infty}^y p(x)\\,\\mathrm{d}{x}} {\\frac{1}{M}\\,\\int_{-\\infty}^{\\infty} p(x)\\,\\mathrm{d}{x}} = \\int_{-\\infty}^y p(x)\\,\\mathrm{d}{x} \\end{aligned} \\]The average acceptance rate \\(\\propto 1/M\\), so a smaller \\(M\\) can lead to lower time consumption. On the other hand, the target distribution \\(p(x)\\) need not be normalized; \\(M\\) can be often estimated again by sampling from \\(q(x)\\) in applications, by \\(M = \\max p(x) / q(x)\\).","categories":[{"name":"Deep_Learning","slug":"Deep-Learning","permalink":"https://wiki.haowen-xu.com/categories/Deep-Learning/"},{"name":"Monte_Carlo_Methods","slug":"Deep-Learning/Monte-Carlo-Methods","permalink":"https://wiki.haowen-xu.com/categories/Deep-Learning/Monte-Carlo-Methods/"}],"tags":[]},{"title":"Monte Carlo Integration","slug":"Deep_Learning/Monte_Carlo_Methods/Monte_Carlo_Integration","date":"2019-10-18T06:15:00.000Z","updated":"2020-06-01T12:15:59.008Z","comments":true,"path":"Deep_Learning/Monte_Carlo_Methods/Monte_Carlo_Integration/","link":"","permalink":"https://wiki.haowen-xu.com/Deep_Learning/Monte_Carlo_Methods/Monte_Carlo_Integration/","excerpt":"","text":"Problem StatementTo estimate \\(\\mathbb{E}_{p(x)}\\left[ f(x) \\right]\\).Naive EstimatorWhen \\(p(x)\\) are easy to sample from, it is straightforward to estimate \\(\\mathbb{E}_{p(x)}\\left[ f(x) \\right]\\) by: \\[ \\mathbb{E}_{p(x)}\\left[ f(x) \\right] \\approx \\frac{1}{K} \\sum_{i=1}^K f(x^{(i)}) \\] where \\(x^{(i)}, \\, i = 1 \\dots K\\) are i.i.d. samples from \\(p(x)\\).Importance SamplingWhen \\(p(x)\\) is not easy to sample from, or when the above estimator has too large variance, one may use the importance sampling estimator: \\[ \\begin{align} \\mathbb{E}_{p(x)}\\left[ f(x) \\right] = \\mathbb{E}_{q(x)}\\left[ \\frac{f(x)\\,p(x)}{q(x)} \\right] \\approx \\frac{1}{K} \\sum_{i=1}^K \\frac{f(x^{(i)})\\,p(x^{(i)})}{q(x^{(i)})} \\end{align} \\] where \\(x^{(i)}, \\, i = 1 \\dots K\\) are i.i.d. samples from \\(q(x)\\), the proposal distribution. The theoretical optimal proposal distribution \\(q^{\\star}(x)\\), which gives the smallest variance to the estimator, is given by: \\[ q^{\\star}(x) = \\frac{\\left| f(x) \\right|\\,p(x)}{\\int \\left| f(\\xi) \\right|\\,p(\\xi)\\,d\\xi} \\]Note the following condition must hold: \\[ q(x) \\neq 0, \\;\\, \\forall x \\;\\, \\text{satisfying} \\;\\, p(x) \\neq 0 \\]","categories":[{"name":"Deep_Learning","slug":"Deep-Learning","permalink":"https://wiki.haowen-xu.com/categories/Deep-Learning/"},{"name":"Monte_Carlo_Methods","slug":"Deep-Learning/Monte-Carlo-Methods","permalink":"https://wiki.haowen-xu.com/categories/Deep-Learning/Monte-Carlo-Methods/"}],"tags":[]},{"title":"Overview","slug":"Deep_Learning/Monte_Carlo_Methods/Overview","date":"2019-10-18T06:03:00.000Z","updated":"2020-06-01T12:15:59.008Z","comments":true,"path":"Deep_Learning/Monte_Carlo_Methods/Overview/","link":"","permalink":"https://wiki.haowen-xu.com/Deep_Learning/Monte_Carlo_Methods/Overview/","excerpt":"","text":"Monte Carlo methods are simulation methods to estimate quantities based on, or take random samples from, a given distribution \\(p(x)\\), where the exact form of \\(p(x)\\) may be either known or unknown. The latter case may typically require Markov Chain Monte Carlo (MCMC) methods.Simple Monte Carlo MethodsEstimating \\(\\mathbb{E}_{p(x)}\\left[ f(x) \\right]\\)Monte Carlo IntegrationSampling from \\(p(x)\\)Accept-Reject SamplingMarkov Chain Monte Carlo methodsMarkov ChainSampling from \\(p(x)\\)Metropolis-Hastings AlgorithmGibbs SamplerHamiltonian DynamicsLangevin Dynamics","categories":[{"name":"Deep_Learning","slug":"Deep-Learning","permalink":"https://wiki.haowen-xu.com/categories/Deep-Learning/"},{"name":"Monte_Carlo_Methods","slug":"Deep-Learning/Monte-Carlo-Methods","permalink":"https://wiki.haowen-xu.com/categories/Deep-Learning/Monte-Carlo-Methods/"}],"tags":[]},{"title":"Score Matching","slug":"Deep_Learning/Confronting_Partition_Function/Score_Matching","date":"2019-10-16T08:05:00.000Z","updated":"2020-06-01T12:15:59.000Z","comments":true,"path":"Deep_Learning/Confronting_Partition_Function/Score_Matching/","link":"","permalink":"https://wiki.haowen-xu.com/Deep_Learning/Confronting_Partition_Function/Score_Matching/","excerpt":"","text":"Problem StatementTo find the optimum \\(\\theta\\) that minimizes \\(U(\\mathbf{X};\\theta)\\) w.r.t. the given dataset \\(\\mathbf{X} = \\{\\mathbf{x}_1,\\dots,\\mathbf{x}_N\\}\\) without estimating \\(Z(\\theta)\\):\\[ \\begin{align} U(\\mathbf{X};\\theta) &amp;= -\\frac{1}{N} \\sum_{i=1}^N \\log p_m(\\mathbf{x}_i;\\theta) = \\log Z(\\theta) - \\frac{1}{N} \\sum_{i=1}^N \\log \\tilde{p}_m(\\mathbf{x}_i;\\theta) \\\\ p_m(\\mathbf{x};\\theta) &amp;= \\frac{1}{Z(\\theta)} \\, \\tilde{p}_m(\\mathbf{x};\\theta) \\\\ Z(\\theta) &amp;= \\int \\tilde{p}_m(\\mathbf{x};\\theta)\\,\\mathrm{d}\\mathbf{x} \\end{align} \\]Score MatchingSuppose each \\(\\mathbf{x}\\) is a \\(k\\)-dimensional vector. The score function of the model \\(p_m(\\mathbf{x};\\theta)\\) is defined as: \\[ \\mathbf{s}(\\mathbf{x};\\theta) = \\begin{pmatrix} \\frac{\\partial \\log p_m(\\mathbf{x};\\theta)}{\\partial x_1} \\\\ \\vdots \\\\ \\frac{\\partial \\log p_m(\\mathbf{x};\\theta)}{\\partial x_k} \\end{pmatrix} = \\begin{pmatrix} \\mathbf{s}_1(\\mathbf{x};\\theta) \\\\ \\vdots \\\\ \\mathbf{s}_k(\\mathbf{x};\\theta) \\end{pmatrix} = \\nabla_{\\mathbf{x}} \\log p_m(\\mathbf{x};\\theta) \\] It is easy to see that \\(\\mathbf{s}(\\mathbf{x};\\theta) = \\nabla_{\\mathbf{x}} \\log \\tilde{p}_m(\\mathbf{x};\\theta)\\), since: \\[ \\begin{align} \\mathbf{s}(\\mathbf{x};\\theta) &amp;= \\nabla_{\\mathbf{x}} \\log p_m(\\mathbf{x};\\theta) = \\frac{1}{p_m(\\mathbf{x};\\theta)} \\nabla_{\\mathbf{x}} p_m(\\mathbf{x};\\theta) \\\\ &amp;= \\frac{1}{\\tilde{p}_m(\\mathbf{x};\\theta) / Z(\\theta)} \\cdot \\nabla_{\\mathbf{x}} \\frac{\\tilde{p}_m(\\mathbf{x};\\theta)}{Z(\\theta)} = \\frac{1}{\\tilde{p}_m(\\mathbf{x};\\theta)} \\nabla_{\\mathbf{x}} \\tilde{p}_m(\\mathbf{x};\\theta) = \\nabla_{\\mathbf{x}} \\log \\tilde{p}_m(\\mathbf{x};\\theta) \\end{align} \\]And if we denote the score function for the empirical distribution \\(p_d(\\mathbf{x})\\) as \\(\\mathbf{s}_d(\\mathbf{x})\\), then the objective function is to match \\(\\mathbf{s}(\\mathbf{x};\\theta)\\) against \\(\\mathbf{s}_d(\\mathbf{x})\\), using squared loss: \\[ J(\\theta) = \\frac{1}{2} \\int p_d(\\mathbf{x}) \\, \\left\\| \\mathbf{s}(\\mathbf{x};\\theta) - \\mathbf{s}_d(\\mathbf{x}) \\right\\|_2^2 \\,d\\mathbf{x} \\]where, under some weak regularity conditions, \\(J(\\theta)\\) can be expressed as: \\[ \\begin{align} &amp; J(\\theta) = \\int p_d(\\mathbf{x})\\sum_{i=1}^k \\left[ \\frac{\\partial \\mathbf{s}_i(\\mathbf{x};\\theta)}{\\partial x_i} + \\frac{1}{2} \\mathbf{s}_i(\\mathbf{x};\\theta)^2 \\right]\\,d\\mathbf{x} + const \\\\ &amp; \\text{where} \\;\\, \\frac{\\partial \\mathbf{s}_i(\\mathbf{x};\\theta)}{\\partial x_i} = \\frac{\\partial^2 \\log \\tilde{p}_m(\\mathbf{x};\\theta)}{\\partial x_i^2} \\end{align} \\] or equivalently, using expectation and matrix notation: \\[ \\begin{align} &amp; J(\\theta) = \\mathbb{E}_{p_d(\\mathbf{x})}\\left[ \\operatorname{tr}\\left( \\nabla_{\\mathbf{x}} \\mathbf{s}(\\mathbf{x};\\theta) \\right) + \\frac{1}{2} \\left\\| \\mathbf{s}(\\mathbf{x};\\theta) \\right\\|^2_2 \\right] \\\\ &amp; \\text{where} \\;\\, \\nabla_{\\mathbf{x}} \\mathbf{s}(\\mathbf{x};\\theta) = \\nabla^2_{\\mathbf{x}} \\log \\tilde{p}_m(\\mathbf{x};\\theta) \\;\\, \\text{is the Hessian matrix} \\end{align} \\]For further materials, see Hyvärinen (2005).Score Estimation in Implicit Generative ModelsIn implicit generative models (i.e., GANs), the sampling process may be explicitly given by:\\[ \\mathbf{x} = g(\\boldsymbol{\\epsilon};\\theta) \\]where \\(\\boldsymbol{\\epsilon}\\) is a random variable independent of \\(\\theta\\). The true density \\(p_m(\\mathbf{x};\\theta)\\) is not tractable.In this case, the score \\(\\nabla_{\\mathbf{x}} \\log p_m(\\mathbf{x};\\theta)\\) can be estimated by a dedicated score network \\(\\mathbf{s}(\\mathbf{x};\\phi)\\), trained by minimizing the score-matching objective: \\[ J(\\phi) = \\mathbb{E}_{p_m(\\mathbf{x};\\theta)}\\left[ \\operatorname{tr}\\left( \\nabla_{\\mathbf{x}} \\mathbf{s}(\\mathbf{x};\\phi) \\right) + \\frac{1}{2} \\left\\| \\mathbf{s}(\\mathbf{x};\\phi) \\right\\|^2_2 \\right] \\] The learned score network can be used to estimate certain quantities involving the score \\(\\nabla_{\\mathbf{x}} \\log p_m(\\mathbf{x};\\theta)\\). For example, the gradient of the entropy of \\(p_m(\\mathbf{x};\\theta)\\), *i.e., \\(\\nabla_{\\theta} H\\left(p_m(\\mathbf{x};\\theta)\\right)\\): \\[ \\begin{align} \\nabla_{\\theta} H\\left( p_m(\\mathbf{x};\\theta) \\right) &amp;= -\\nabla_{\\theta} \\mathbb{E}_{p_m(\\mathbf{x};\\theta)}\\left[ \\log p_m(\\mathbf{x};\\theta) \\right] = -\\nabla_{\\theta} \\mathbb{E}_{p(\\boldsymbol{\\epsilon})}\\left[ \\log p_m(g(\\boldsymbol{\\epsilon};\\theta);\\theta) \\right] \\\\ &amp;= -\\mathbb{E}_{p(\\boldsymbol{\\epsilon})}\\left[ \\nabla_{\\theta} \\log p_m(\\mathbf{x};\\theta) + \\nabla_{\\mathbf{x}} \\log p_m(\\mathbf{x};\\theta)\\big|_{\\mathbf{x}=g(\\boldsymbol{\\epsilon;\\theta})} \\cdot \\nabla_{\\theta} g(\\boldsymbol{\\epsilon};\\theta) \\right] \\\\ &amp;= -\\mathbb{E}_{p_m(\\mathbf{x};\\theta)}\\left[ \\nabla_{\\theta} \\log p_m(\\mathbf{x};\\theta) \\right] - \\mathbb{E}_{p(\\boldsymbol{\\epsilon})}\\left[ \\nabla_{\\mathbf{x}} \\log p_m(\\mathbf{x};\\theta)\\big|_{\\mathbf{x}=g(\\boldsymbol{\\epsilon;\\theta})} \\cdot \\nabla_{\\theta} g(\\boldsymbol{\\epsilon};\\theta) \\right] \\\\ &amp;= - \\mathbb{E}_{p(\\boldsymbol{\\epsilon})}\\left[ \\nabla_{\\mathbf{x}} \\log p_m(\\mathbf{x};\\theta)\\big|_{\\mathbf{x}=g(\\boldsymbol{\\epsilon;\\theta})} \\cdot \\nabla_{\\theta} g(\\boldsymbol{\\epsilon};\\theta) \\right] \\end{align} \\] where \\(\\mathbb{E}_{p_m(\\mathbf{x};\\theta)}\\left[ \\nabla_{\\theta} \\log p_m(\\mathbf{x};\\theta) \\right]=0\\), since: \\[ \\begin{align} \\mathbb{E}_{p_m(\\mathbf{x};\\theta)}\\left[ \\nabla_{\\theta} \\log p_m(\\mathbf{x};\\theta) \\right] &amp;= \\int p_m(\\mathbf{x};\\theta) \\cdot {\\nabla_{\\theta} p_m(\\mathbf{x};\\theta) \\over p_m(\\mathbf{x};\\theta)} \\,d\\mathbf{x} \\\\ = \\int \\nabla_{\\theta} p_m(\\mathbf{x};\\theta)\\,d\\mathbf{x} &amp;= \\nabla_{\\theta} \\int p_m(\\mathbf{x};\\theta)\\,d\\mathbf{x} = \\nabla_{\\theta} 1 = 0 \\end{align} \\]For further materials, see Li and Turner (2017).Reducing Computation Cost\\(\\nabla_{\\mathbf{x}} \\mathbf{s}(\\mathbf{x};\\theta)\\) is the Hessian matrix of \\(\\log \\tilde{p}_m(\\mathbf{x};\\theta)\\),ReferencesHyvärinen, Aapo. 2005. “Estimation of Non-Normalized Statistical Models by Score Matching.” Journal of Machine Learning Research 6 (Apr): 695–709.Li, Yingzhen, and Richard E. Turner. 2017. “Gradient Estimators for Implicit Models.” arXiv:1705.07107 [Cs, Stat], May. http://arxiv.org/abs/1705.07107.","categories":[{"name":"Deep_Learning","slug":"Deep-Learning","permalink":"https://wiki.haowen-xu.com/categories/Deep-Learning/"},{"name":"Confronting_Partition_Function","slug":"Deep-Learning/Confronting-Partition-Function","permalink":"https://wiki.haowen-xu.com/categories/Deep-Learning/Confronting-Partition-Function/"}],"tags":[]},{"title":"Contrastive Divergence","slug":"Deep_Learning/Confronting_Partition_Function/Contrastive_Divergence","date":"2019-10-16T07:56:00.000Z","updated":"2020-06-01T12:15:59.000Z","comments":true,"path":"Deep_Learning/Confronting_Partition_Function/Contrastive_Divergence/","link":"","permalink":"https://wiki.haowen-xu.com/Deep_Learning/Confronting_Partition_Function/Contrastive_Divergence/","excerpt":"","text":"Problem StatementTo find the optimum \\(\\theta\\) that minimizes \\(U(\\mathbf{X};\\theta)\\) w.r.t. the given dataset \\(\\mathbf{X} = \\{\\mathbf{x}_1,\\dots,\\mathbf{x}_N\\}\\) without estimating \\(Z(\\theta)\\):\\[ \\begin{align} U(\\mathbf{X};\\theta) &amp;= -\\frac{1}{N} \\sum_{i=1}^N \\log p_m(\\mathbf{x}_i;\\theta) = \\log Z(\\theta) - \\frac{1}{N} \\sum_{i=1}^N \\log \\tilde{p}_m(\\mathbf{x}_i;\\theta) \\\\ p_m(\\mathbf{x};\\theta) &amp;= \\frac{1}{Z(\\theta)} \\, \\tilde{p}_m(\\mathbf{x};\\theta) \\\\ Z(\\theta) &amp;= \\int \\tilde{p}_m(\\mathbf{x};\\theta)\\,\\mathrm{d}\\mathbf{x} \\end{align} \\]Contrastive DivergenceThe gradient for \\(U(\\mathbf{X};\\theta)\\) is: \\[ \\begin{align} \\frac{\\partial U(\\mathbf{X};\\theta)}{\\partial \\theta} &amp;= \\frac{\\partial \\log Z(\\theta)}{\\partial \\theta} - \\frac{1}{N} \\sum_{i=1}^N \\frac{\\partial \\log \\tilde{p}_m(\\mathbf{x}_i;\\theta)}{\\partial \\theta} \\end{align} \\]The first term, \\(\\frac{\\partial \\log Z(\\theta)}{\\partial \\theta}\\), can be derived as:\\[ \\begin{align} \\frac{\\partial \\log Z(\\theta)}{\\partial \\theta} &amp;= \\frac{1}{Z(\\theta)} \\cdot \\frac{\\partial Z(\\theta)}{\\partial \\theta} = \\frac{1}{Z(\\theta)} \\cdot \\int \\frac{\\partial \\tilde{p}_m(\\mathbf{x};\\theta)}{\\partial \\theta}\\,\\mathrm{d}\\mathbf{x} = \\frac{1}{Z(\\theta)} \\cdot \\int \\tilde{p}_m(\\mathbf{x};\\theta)\\,\\frac{\\partial \\log \\tilde{p}_m(\\mathbf{x};\\theta)}{\\partial \\theta}\\,\\mathrm{d}\\mathbf{x} \\\\ &amp;= \\int p_m(\\mathbf{x};\\theta)\\,\\frac{\\partial \\log \\tilde{p}_m(\\mathbf{x};\\theta)}{\\partial \\theta}\\,\\mathrm{d}\\mathbf{x} = \\mathbb{E}_{p_m(\\mathbf{x};\\theta)}\\left[\\frac{\\partial \\log \\tilde{p}_m(\\mathbf{x};\\theta)}{\\partial \\theta}\\right] = \\left&lt;\\frac{\\partial \\log \\tilde{p}_m(\\mathbf{x};\\theta)}{\\partial \\theta}\\right&gt;_{\\mathbf{X}} \\end{align} \\]where \\(\\left&lt;\\frac{\\partial \\log \\tilde{p}_m(\\mathbf{x};\\theta)}{\\partial \\theta}\\right&gt; _ {\\mathbf{X}}\\) is the expectation of \\(\\frac{\\partial \\log \\tilde{p}_m(\\mathbf{x};\\theta)}{\\partial \\theta}\\) over \\(p_m(\\mathbf{x};\\theta)\\). Also see Energy Function in Probabilistic Models for another deduction.Unfortunately, directly sampling from \\(p_m(\\mathbf{x};\\theta)\\) is generally not possible. Thus, Hinton suggested to start with the empirical distribution (i.e., dataset \\(\\mathbf{X}\\)), and use MCMC chain to approach \\(p_m(\\mathbf{x};\\theta)\\). Let \\(\\mathbf{X}^0 \\equiv \\mathbf{X}\\) be the samples from the empirical distribution \\(p_d(\\mathbf{x})\\), \\(\\mathbf{X}^k\\) be the samples derived by \\(k\\)-cycle MCMC chain, and \\(\\mathbf{X}^{\\infty}\\) be the samples from the limit distribution of MCMC chain (i.e., \\(p_m(\\mathbf{x};\\theta)\\)), we then have: \\[ \\begin{align} \\frac{\\partial U(\\mathbf{X};\\theta)}{\\partial \\theta} &amp;= \\left&lt;\\frac{\\partial \\log \\tilde{p}_m(\\mathbf{x};\\theta)}{\\partial \\theta}\\right&gt;_{\\mathbf{X}^{\\infty}} - \\left&lt;\\frac{\\partial \\log \\tilde{p}_m(\\mathbf{x};\\theta)}{\\partial \\theta}\\right&gt;_{\\mathbf{X}^0} \\\\ &amp;\\approx \\left&lt;\\frac{\\partial \\log \\tilde{p}_m(\\mathbf{x};\\theta)}{\\partial \\theta}\\right&gt;_{\\mathbf{X}^k} - \\left&lt;\\frac{\\partial \\log \\tilde{p}_m(\\mathbf{x};\\theta)}{\\partial \\theta}\\right&gt;_{\\mathbf{X}^0} \\end{align} \\]This algorithm is called CD-k algorithm when using \\(k\\)-cycle MCMC chain. For further materials, see Woodford (n.d.).Hinton (2002) has found that even with 1-cycle MCMC chain, it is sufficient for the training to converge.Interestingly, although the 1-cycle MCMC chain (or few cycles MCMC chain) is not expected to be a good estimator of the true \\(p_m(\\mathbf{x};\\theta)\\), it can be viewed as a neighborhood of a particular given training data \\(\\mathbf{x}\\) with relatively high log-likelihood (since by design, the chain starts from a given training data). Then optimizing the contrastive divergence loss can be viewed as \"pull-down\" the energy of some energy function \\(E(\\mathbf{x};\\theta)\\) at the given train data, and \"pull-up\" the energy at the sampled neighborhood data, if we can write \\(p_{m}(\\mathbf{x};\\theta) = \\frac{\\exp(-\\beta E(\\mathbf{x};\\theta))}{\\int \\exp(-\\beta E(\\mathbf{x'};\\theta))\\,dx'}\\). In this perspective, the CD-k algorithm can be viewed as a form of energy based learning, with approximated constrastive samples (see Energy Based Models).Persistent Contrastive DivergenceTieleman (2008) proposed to use the final samples from the previous MCMC chain at each mini-batch instead of the training points, as the initial state of the MCMC chain at each mini-batch.ReferencesHinton, Geoffrey E. 2002. “Training Products of Experts by Minimizing Contrastive Divergence.” Neural Computation 14 (8): 1771–1800.Tieleman, Tijmen. 2008. “Training Restricted Boltzmann Machines Using Approximations to the Likelihood Gradient.” In Proceedings of the 25th International Conference on Machine Learning - ICML ’08, 1064–71. Helsinki, Finland: ACM Press. https://doi.org/10.1145/1390156.1390290.Woodford, Oliver. n.d. “Notes on Contrastive Divergence,” 3.","categories":[{"name":"Deep_Learning","slug":"Deep-Learning","permalink":"https://wiki.haowen-xu.com/categories/Deep-Learning/"},{"name":"Confronting_Partition_Function","slug":"Deep-Learning/Confronting-Partition-Function","permalink":"https://wiki.haowen-xu.com/categories/Deep-Learning/Confronting-Partition-Function/"}],"tags":[]},{"title":"Overview","slug":"Deep_Learning/Confronting_Partition_Function/Overview","date":"2019-10-16T07:55:00.000Z","updated":"2020-06-01T12:15:59.000Z","comments":true,"path":"Deep_Learning/Confronting_Partition_Function/Overview/","link":"","permalink":"https://wiki.haowen-xu.com/Deep_Learning/Confronting_Partition_Function/Overview/","excerpt":"","text":"The density \\(p_m(\\mathbf{x};\\theta)\\) of an undirected graphical model is often given by a tractable, but unnormalized density function \\(\\tilde{p}_m(\\mathbf{x};\\theta)\\):\\[ p_m(\\mathbf{x};\\theta) = \\frac{1}{Z(\\theta)} \\, \\tilde{p}_m(\\mathbf{x};\\theta) \\]where \\(Z(\\theta)=\\int \\tilde{p}_m(\\mathbf{x};\\theta)\\,d\\mathbf{x}\\) is the partition function of \\(\\tilde{p}_m(\\mathbf{x};\\theta)\\), usually not exactly tractable.The expectation of the energy function \\(U(\\mathbf{x};\\theta)\\) over a given dataset \\(\\mathbf{X} = \\{\\mathbf{x}_1,\\dots,\\mathbf{x}_N\\}\\) is the average negative log-likelihood, i.e.: \\[ U(\\mathbf{X};\\theta) = -\\frac{1}{N} \\sum_{i=1}^N \\log p_m(\\mathbf{x}_i;\\theta) = \\log Z(\\theta) - \\frac{1}{N} \\sum_{i=1}^N \\log \\tilde{p}_m(\\mathbf{x}_i;\\theta) \\] Usually, one would expect to obtain the optimal parameters \\(\\theta^{\\star}\\), which minimizes \\(U(\\mathbf{X};\\theta)\\). However, since \\(Z(\\theta)\\) cannot be exactly computed, the estimation of \\(U(\\mathbf{X};\\theta)\\) requires special techniques, which briefly falls into the following categories:To approximate \\(Z(\\theta)\\), e.g., by using Monte Carlo methods.To directly find the optimum \\(\\theta^{\\star}\\) of \\(U(\\mathbf{X};\\theta)\\) without having to estimate \\(Z(\\theta)\\).Estimating \\(Z(\\theta)\\)Find the Optimum \\(\\theta^{\\star}\\) without Estimating \\(Z(\\theta)\\)Contrastive DivergenceScore Matching","categories":[{"name":"Deep_Learning","slug":"Deep-Learning","permalink":"https://wiki.haowen-xu.com/categories/Deep-Learning/"},{"name":"Confronting_Partition_Function","slug":"Deep-Learning/Confronting-Partition-Function","permalink":"https://wiki.haowen-xu.com/categories/Deep-Learning/Confronting-Partition-Function/"}],"tags":[]},{"title":"Theoretical Facts about VAEs","slug":"Deep_Learning/Variational_Autoencoder/Theoretical_Facts","date":"2019-09-21T07:04:00.000Z","updated":"2020-06-01T12:15:59.028Z","comments":true,"path":"Deep_Learning/Variational_Autoencoder/Theoretical_Facts/","link":"","permalink":"https://wiki.haowen-xu.com/Deep_Learning/Variational_Autoencoder/Theoretical_Facts/","excerpt":"","text":"\\(\\beta\\)-VAE\\(\\beta\\)-VAE objective is equivalent to a standard ELBO with the alternative prior \\(f_{\\beta}(\\mathbf{z}) \\propto p(\\mathbf{z})^{\\beta}\\) (Mathieu et al. 2018):for Gaussian \\(p(\\mathbf{z})\\):\\(\\beta\\)-VAE simply anneals the latent space by \\(1/\\sqrt{\\beta}\\), i.e., \\(f_{\\beta}(\\mathbf{z}) = \\mathcal{N}(\\mathbf{z};0,\\Sigma/\\beta)\\).\\(\\beta\\)-VAE objective is invariant to rotations of the latent space.ReferencesMathieu, Emile, Tom Rainforth, N. Siddharth, and Yee Whye Teh. 2018. “Disentangling Disentanglement in Variational Autoencoders.” arXiv:1812.02833 [Cs, Stat], December. http://arxiv.org/abs/1812.02833.","categories":[{"name":"Deep_Learning","slug":"Deep-Learning","permalink":"https://wiki.haowen-xu.com/categories/Deep-Learning/"},{"name":"Variational_Autoencoder","slug":"Deep-Learning/Variational-Autoencoder","permalink":"https://wiki.haowen-xu.com/categories/Deep-Learning/Variational-Autoencoder/"}],"tags":[]},{"title":"Convex Optimization","slug":"Mathematics/Optimization/Convex_Optimization","date":"2019-07-11T11:47:00.000Z","updated":"2020-06-01T12:15:59.028Z","comments":true,"path":"Mathematics/Optimization/Convex_Optimization/","link":"","permalink":"https://wiki.haowen-xu.com/Mathematics/Optimization/Convex_Optimization/","excerpt":"","text":"This page is a summarization of the convex optimization topic. If not pointed out, all pages and propositions can be found in Bertsekas (2009).Convex Functionsoperations that preserves convexity/closeness:page 12, prop 1.1.4:\\(F(\\mathbf{x}) = f(\\mathbf{A}\\mathbf{x})\\), then \\(f\\) is convex/closed =&gt; \\(F\\) is convex/closedpage 13, prop 1.1.5:\\(F(\\mathbf{x}) = \\sum_{i=1}^m \\gamma_i f_i(\\mathbf{x})\\), then \\(f_i\\) are convex/closed and \\(\\gamma_i\\) are positive =&gt; \\(F\\) is convex/closedpage 13, prop 1.1.6: \\(F(\\mathbf{x}) = \\sup_{i \\in I} f_i(\\mathbf{x})\\), then \\(f_i\\) are convex/closed =&gt; \\(F\\) is convex/closedfor differentiable \\(f\\), assuming \\(C \\sube \\mathbb{R}^n\\) is a nonempty convex set:page 14, prop 1.1.7:\\(f\\) is convex over \\(C\\) &lt;=&gt; \\(f(\\mathbf{z}) \\geq \\nabla f(\\mathbf{x})^{\\top} (\\mathbf{z}-\\mathbf{x}), \\quad\\forall \\mathbf{x},\\mathbf{z} \\in C\\).\\(f\\) is strictly convex over \\(C\\) &lt;=&gt; the above inequality is strict whenever \\(\\mathbf{x} \\neq \\mathbf{z}\\).page 17, prop 1.1.8:\\(x^* \\in C\\) minimizes \\(f\\) over \\(C\\) &lt;=&gt; \\(\\nabla f(\\mathbf{x}^*)^{\\top} (\\mathbf{z}-\\mathbf{x}^*) \\geq 0, \\quad \\forall \\mathbf{z} \\in C\\).page 17, prop 1.1.9: projection theoremthere exists a unique \\(\\mathbf{z} \\in \\mathbb{R}^n\\) that minimizes \\(\\|\\mathbf{z}-\\mathbf{x}\\|\\) over \\(\\mathbf{x} \\in C\\).\\(\\mathbf{x}^*\\) is the projection of \\(\\mathbf{z}\\) on C &lt;=&gt; \\((\\mathbf{z}-\\mathbf{x}^*)^{\\top}(\\mathbf{x}-\\mathbf{x}^*) \\leq 0, \\quad \\forall \\mathbf{x}\\in C\\).for twice differentiable \\(f\\), assuming \\(C \\sube \\mathbb{R}^n\\) is a nonempty convex set\\(\\nabla^2 f(x)\\) positive semidefinite for all \\(x \\in C\\) =&gt; \\(f\\) is convex over \\(C\\).\\(\\nabla^2 f(x)\\) positive definite for all \\(x \\in C\\) =&gt; \\(f\\) is strictly convex over \\(C\\).\\(C\\) is open, and \\(f\\) is convex over \\(C\\) =&gt; \\(\\nabla^2 f(x)\\) positive semidefinite for all \\(x \\in C\\).ReferencesBertsekas, Dimitri P. 2009. Convex Optimization Theory. Optimization and Computation Series 1. Belmont, Massachusetts: Athena Scientific.","categories":[{"name":"Mathematics","slug":"Mathematics","permalink":"https://wiki.haowen-xu.com/categories/Mathematics/"},{"name":"Optimization","slug":"Mathematics/Optimization","permalink":"https://wiki.haowen-xu.com/categories/Mathematics/Optimization/"}],"tags":[]},{"title":"Differential Equations on Probability Distributions","slug":"Mathematics/Differential_Equations/Probability_Distribution_Equations","date":"2019-05-23T03:19:00.000Z","updated":"2020-06-01T12:15:59.028Z","comments":true,"path":"Mathematics/Differential_Equations/Probability_Distribution_Equations/","link":"","permalink":"https://wiki.haowen-xu.com/Mathematics/Differential_Equations/Probability_Distribution_Equations/","excerpt":"","text":"Example ProblemsSolve the equation \\(-z_i \\cdot q_{\\phi}(\\mathbf{z}) = \\frac{\\partial}{\\partial z_i}\\,q_{\\phi}(\\mathbf{z})\\)Let \\(q_{\\phi}(\\mathbf{z}) = q_{\\phi}(z_1|z_2,\\dots,z_K) \\cdot q_{\\phi}(z_2,\\dots,z_K)\\), where \\(K\\) is the number of dimensions of \\(\\mathbf{z}\\). We shall first solve the differential equation on \\(z_1\\):\\[ \\begin{align} &amp; -z_1 \\cdot q_{\\phi}(\\mathbf{z}) = \\frac{\\partial}{\\partial z_1}\\,q_{\\phi}(\\mathbf{z}) \\\\ \\implies &amp; -z_1 \\cdot q_{\\phi}(z_1|z_2,\\dots,z_K)\\cdot q_{\\phi}(z_2,\\dots,z_K) = \\frac{\\partial}{\\partial z_1}\\,q_{\\phi}(z_1|z_2,\\dots,z_K)\\cdot q_{\\phi}(z_2,\\dots,z_K) \\\\ \\implies &amp; -z_1 \\cdot q_{\\phi}(z_1|z_2,\\dots,z_K)\\cdot q_{\\phi}(z_2,\\dots,z_K) = q_{\\phi}(z_2,\\dots,z_K)\\cdot \\frac{\\partial}{\\partial z_1}\\,q_{\\phi}(z_1|z_2,\\dots,z_K) \\\\ \\implies &amp; -z_1 \\cdot q_{\\phi}(z_1|z_2,\\dots,z_K) = \\frac{\\partial}{\\partial z_1}\\,q_{\\phi}(z_1|z_2,\\dots,z_K) \\\\ \\implies &amp; -z_1\\,\\partial z_1 = \\frac{1}{q_{\\phi}(z_1|z_2,\\dots,z_K)}\\,\\partial q_{\\phi}(z_1|z_2,\\dots,z_K) \\\\ \\implies &amp; \\int -z_1\\,\\partial z_1 = \\int \\frac{1}{q_{\\phi}(z_1|z_2,\\dots,z_K)}\\,\\partial q_{\\phi}(z_1|z_2,\\dots,z_K) \\\\ \\implies &amp; -\\frac{1}{2} z_1^2 + C(z_2,\\dots,z_K) = \\log q_{\\phi}(z_1|z_2,\\dots,z_K) \\\\ \\implies &amp; \\exp\\left(-\\frac{1}{2} z_1^2\\right)\\cdot\\exp\\left(C(z_2,\\dots,z_K))\\right) = q_{\\phi}(z_1|z_2,\\dots,z_K) \\end{align} \\]Since \\(q_{\\phi}(z_1|z_2,\\dots,z_K)\\) is a probability distribution, we have:\\[ \\begin{equation*} \\exp\\left(C(z_2,\\dots,z_K)\\right) = 1 / \\int \\exp\\left(-\\frac{1}{2} z_1^2\\right) \\,\\mathrm{d}z_1 = \\frac{1}{\\sqrt{2\\pi}} \\end{equation*} \\]thus we have:\\[ \\begin{equation*} q_{\\phi}(\\mathbf{z}) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{1}{2} z_1^2\\right)\\cdot q_{\\phi}(z_2,\\dots,z_K) \\end{equation*} \\]We then solve the differential equation on \\(z_2\\). We have:\\[ \\begin{align} &amp; -z_2 \\cdot q_{\\phi}(\\mathbf{z}) = \\frac{\\partial}{\\partial z_2}\\,q_{\\phi}(\\mathbf{z}) \\\\ \\implies &amp; -z_2 \\cdot \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{1}{2} z_1^2\\right)\\cdot q_{\\phi}(z_2,\\dots,z_K) = \\frac{\\partial}{\\partial z_2} \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{1}{2} z_1^2\\right)\\cdot q_{\\phi}(z_2,\\dots,z_K) \\\\ \\implies &amp; -z_2 \\cdot \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{1}{2} z_1^2\\right)\\cdot q_{\\phi}(z_2,\\dots,z_K) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{1}{2} z_1^2\\right)\\cdot\\frac{\\partial}{\\partial z_2} q_{\\phi}(z_2,\\dots,z_K) \\\\ \\implies &amp; -z_2 \\cdot q_{\\phi}(z_2,\\dots,z_K) = \\frac{\\partial}{\\partial z_2} q_{\\phi}(z_2,\\dots,z_K) \\end{align} \\]If we let \\(\\mathbf{z}’ = z_2,\\dots,z_K\\), the form of the differential equation on \\(z_2\\) is now exactly the same as the differential equation on \\(z_1\\). Use the same method, we can solve the equation on \\(z_2\\), and further on \\(z_3, \\dots, z_K\\). Finally ,we can get the solution:\\[ \\begin{equation*} q_{\\phi}(\\mathbf{z}) = \\frac{1}{\\left(\\sqrt{2\\pi}\\right)^K}\\prod_{i=1}^K \\exp\\left(-\\frac{1}{2} z_i^2\\right) \\end{equation*} \\]which is a \\(K\\)-dimensional unit Gaussian.","categories":[{"name":"Mathematics","slug":"Mathematics","permalink":"https://wiki.haowen-xu.com/categories/Mathematics/"},{"name":"Differential_Equations","slug":"Mathematics/Differential-Equations","permalink":"https://wiki.haowen-xu.com/categories/Mathematics/Differential-Equations/"}],"tags":[]},{"title":"Calculus of Variations","slug":"Mathematics/Calculus/Calculus_of_Variations","date":"2019-05-16T04:00:00.000Z","updated":"2020-06-01T12:15:59.028Z","comments":true,"path":"Mathematics/Calculus/Calculus_of_Variations/","link":"","permalink":"https://wiki.haowen-xu.com/Mathematics/Calculus/Calculus_of_Variations/","excerpt":"","text":"Notation\\(C_n[a,b]​\\): the set of continuous functions defined on \\([a,b]​\\), which are continuous and have derivatives up to order \\(n​\\). Specifically, \\(C[a,b]​\\) is the set of continuous functions.\\(D_n[a,b]​\\): the set of functions defined on \\([a,b]​\\) which are continuous and have continuous derivatives up to order \\(n​\\).Fundamental lemma in the calculus of variationsIf \\(\\alpha(x)\\) is continuous in \\([a,b]\\), and if \\[ \\int_a^b \\alpha(x) h(x) = 0 \\] for every function \\(h(x) \\in \\mathcal{l}(a,b)\\) such that \\(h(a) = h(b) = 0\\), then \\(\\alpha(x)=0\\) for all \\(x\\) in \\([a,b]\\).Euler's EquationAll the functionals in following cases should have boundary conditions specified.Single variate, single functional caseA necessary condition for \\(J[y]\\):\\[ J[y] = \\int_a^b F(x,y,y') dx \\]to have an extremum for \\(y(x)\\in D_1[a,b]​\\) is:\\[ F_y - \\frac{\\partial}{\\partial x} \\frac{\\partial F}{\\partial (\\partial_x y)} = 0 \\]where \\(\\partial_x y = \\partial y / \\partial x = y'\\).Singla variate, multi functional caseA necessary condition for \\(J[y_1,\\dots,y_n]​\\): \\[ J[y_1,\\dots,y_n] = \\int_a^b F(x,y_1,\\dots,y_n,y_1',\\dots,y_n') \\,dx \\] to have an extremum is : \\[ F_{y_i} - \\frac{\\partial}{\\partial x} \\frac{\\partial F}{\\partial(\\partial_x y_i)} = 0 \\qquad (i=1,\\dots,n) \\]Multi variate, single functional caseA necessary condition for \\(J[u]​\\):\\[ J[u] = \\int\\cdots\\int_R F(x_1, \\dots, x_n, u, \\partial_{x_1} u, \\dots, \\partial_{x_n} u) \\,dx_1 \\cdots dx_n \\]to have an extremum is:\\[ F_u - \\sum_{i=1}^n \\frac{\\partial}{\\partial x_i} \\frac{\\partial F}{\\partial(\\partial_{x_i} u)} = 0 \\]","categories":[{"name":"Mathematics","slug":"Mathematics","permalink":"https://wiki.haowen-xu.com/categories/Mathematics/"},{"name":"Calculus","slug":"Mathematics/Calculus","permalink":"https://wiki.haowen-xu.com/categories/Mathematics/Calculus/"}],"tags":[]},{"title":"Linear space vs functional space","slug":"Mathematics/Analysis/Linear_Space_vs_Functional_Space","date":"2019-04-12T04:00:00.000Z","updated":"2020-06-01T12:15:59.028Z","comments":true,"path":"Mathematics/Analysis/Linear_Space_vs_Functional_Space/","link":"","permalink":"https://wiki.haowen-xu.com/Mathematics/Analysis/Linear_Space_vs_Functional_Space/","excerpt":"","text":"Linear spaceFunctional space (on \\([a, b]\\))Element\\(\\mathbf{x} = [x_1, x_2, \\dots, x_n]\\)\\(f = [f(x_i), \\dots]\\), where \\(i \\in I\\) is an uncountable index on \\([a,b]\\)Inner product\\(\\langle\\mathbf{x},\\mathbf{y}\\rangle = [x_1 y_1, x_2 y_2, \\dots, x_n y_n]\\)\\(\\langle f,g\\rangle = \\int_a^b f(x) \\,g(x)\\, \\mathrm{d}x\\)Orthogonal basis\\(\\mathbf{x} = \\sum_{k=1}^n \\alpha_k \\mathbf{e}_k\\)\\(f=\\sum_k \\alpha_k f_k\\)\\(\\alpha_k = \\langle \\mathbf{x},\\mathbf{e}_k \\rangle\\)\\(\\alpha_k = \\langle f, f_k \\rangle\\)","categories":[{"name":"Mathematics","slug":"Mathematics","permalink":"https://wiki.haowen-xu.com/categories/Mathematics/"},{"name":"Analysis","slug":"Mathematics/Analysis","permalink":"https://wiki.haowen-xu.com/categories/Mathematics/Analysis/"}],"tags":[]},{"title":"Gradient Estimators for Variational Inference","slug":"Deep_Learning/Variational_Autoencoder/Gradient_Estimators_for_Variational_Inference","date":"2018-08-21T08:41:00.000Z","updated":"2020-06-01T12:15:59.012Z","comments":true,"path":"Deep_Learning/Variational_Autoencoder/Gradient_Estimators_for_Variational_Inference/","link":"","permalink":"https://wiki.haowen-xu.com/Deep_Learning/Variational_Autoencoder/Gradient_Estimators_for_Variational_Inference/","excerpt":"","text":"In complex Bayesian networks, especially for deep Bayesian networks, it is often computational intractable to compute some “posterior distributions” (typically along the opposite directions of links, for example, \\(p_{\\theta}(\\mathbf{z}|\\mathbf{x})\\) in the main network of the right figure). These posteriors are often required in both training and testing. In such cases, variational inference techniques are often adopted, to approximate the intractable posterior by another network (e.g., \\(q_{\\phi}(\\mathbf{z}|\\mathbf{x})\\) to approximate \\(p_{\\theta}(\\mathbf{z}|\\mathbf{x})\\) in the right figure). Such “other networks” are often called the “variational networks” or “inference networks”.Note that also we only present a very basic form of Bayesian networks in this page (i.e., having just one visible variable \\(\\mathbf{x}\\) and one latent variable \\(\\mathbf{z}\\)), the formula of this page can be extended to multiple visible and latent variables easily, by treating all visible variables as \\(\\mathbf{x}\\) and all latent variables as \\(\\mathbf{z}\\).Variational Lower BoundsWhen training a Bayesian network using variational inference, the state-of-the-arts technique is to construct a lower bound \\(\\mathcal{L}(\\mathbf{x};\\theta,\\phi)\\) for \\(\\log p_{\\theta}(\\mathbf{x})\\). When maximizing \\(\\mathbb{E}_{\\mathbf{x}\\sim p_{data}(\\mathbf{x})}\\left[\\mathcal{L}(\\mathbf{x};\\theta,\\phi)\\right]\\) , it simultaneously do the following two things: (1) maximize the joint log-likelihood \\(\\mathbb{E}_{\\mathbf{x} \\sim p_{data}(\\mathbf{x}),\\mathbf{z}\\sim p_{\\theta}(\\mathbf{z})}\\left[\\log p_{\\theta}(\\mathbf{x},\\mathbf{z})\\right]\\), and (2) let \\(q_{\\phi}(\\mathbf{z}|\\mathbf{x})\\) to approximate \\(p_{\\theta}(\\mathbf{z}|\\mathbf{x})\\).Evidence Lower Bound (ELBO)The “Evidence Lower Bound” \\(\\mathcal{L}(\\mathbf{x};\\theta,\\phi)\\) is deduced by:\\[ \\begin{aligned} \\log p_{\\theta}(\\mathbf{x}) &amp;\\geq \\log p_{\\theta}(\\mathbf{x}) - \\operatorname{D}_{KL}\\big[ q_{\\phi}(\\mathbf{z}|\\mathbf{x})\\|p_{\\theta}(\\mathbf{z}|\\mathbf{x}) \\big] \\\\ &amp;= \\mathbb{E}_{\\mathbf{z} \\sim q_{\\phi}(\\mathbf{z}|\\mathbf{x})}\\big[ \\log p_{\\theta}(\\mathbf{x}) + \\log p_{\\theta}(\\mathbf{z}|\\mathbf{x}) - \\log q_{\\phi}(\\mathbf{z}|\\mathbf{x}) \\big] \\\\ &amp;= \\mathbb{E}_{\\mathbf{z} \\sim q_{\\phi}(\\mathbf{z}|\\mathbf{x})}\\big[ \\log p_{\\theta}(\\mathbf{x},\\mathbf{z}) - \\log q_{\\phi}(\\mathbf{z}|\\mathbf{x}) \\big] \\\\ &amp;= \\mathcal{L}(\\mathbf{x};\\theta,\\phi) \\end{aligned} \\]Monte Carlo ObjectiveThe “Monto Carlo Objective” is an importance sampling based variational lower-bound, given by:\\[ \\mathcal{L}_{K}(\\mathbf{x};\\theta,\\phi) = \\mathbb{E}_{\\mathbf{z}^{(1:K)} \\sim q_{\\phi}(\\mathbf{z}|\\mathbf{x})}\\Bigg[ \\log \\frac{1}{K} \\sum_{k=1}^K { \\frac{p_{\\theta}(\\mathbf{x},\\mathbf{z}^{(k)})} {q_{\\phi}(\\mathbf{z}^{(k)}|\\mathbf{x})} } \\Bigg] \\]where \\(\\mathbf{z}^{(1:K)}\\) are \\(K\\) independent samples of \\(\\mathbf{z}\\) from \\(q_{\\phi}(\\mathbf{z}|\\mathbf{x})\\). It is proven by Burda, Grosse, and Salakhutdinov (2015) that:\\[ \\log p_{\\theta}(\\mathbf{x}) \\geq \\mathcal{L}_K(\\theta,\\phi) \\geq \\mathcal{L}_M(\\theta,\\phi)$ for $K \\geq M$, and $\\lim_{K \\to \\infty} \\mathcal{L}_K (\\theta,\\phi) = \\log p_{\\theta}(\\mathbf{x}) \\]Gradient EstimatorsTo optimize a variational lower bound, especially for deep Bayesian networks, stochastic gradient descent is often adopted. However, it is not straightforward to compute the gradient of an expectation. The gradient estimators thus serve to compute the gradients for the variational lower bounds.For simplicity, we shall omit the subscripts in this section, thus the gradient operator \\(\\nabla\\) should be applied on both \\(\\theta\\) and \\(\\phi\\), and \\(f(\\mathbf{x},\\mathbf{z})\\) should have both two sets of parameters.SGVBKingma and Welling (2014) proposes that, if \\(\\mathbf{z}\\) can be reparameterized by \\(\\mathbf{z} = \\mathbf{z}(\\epsilon)\\), where \\(\\mathbf{\\epsilon}\\) is another random variable independent of \\(\\theta\\) and \\(\\phi\\), and \\(\\mathbf{z}(\\mathbf{\\epsilon})\\) is a continuous, differentiable mapping, then the gradient estimator for \\(\\mathcal{L}(\\mathbf{x};\\theta,\\phi)=\\mathbb{E}_{q(\\mathbf{z}|\\mathbf{x})}\\big[f(\\mathbf{x},\\mathbf{z})\\big]\\) is given by:\\[ \\nabla \\, \\mathbb{E}_{q(\\mathbf{z}|\\mathbf{x})}\\big[f(\\mathbf{x},\\mathbf{z})\\big] = \\nabla \\, \\mathbb{E}_{q(\\mathbf{\\epsilon})}\\big[f(\\mathbf{x},\\mathbf{z}(\\mathbf{\\epsilon}))\\big] = \\mathbb{E}_{q(\\mathbf{\\epsilon})}\\big[\\nabla f(\\mathbf{x},\\mathbf{z}(\\mathbf{\\epsilon}))\\big] \\]IWAEBurda, Grosse, and Salakhutdinov (2015) extends SGVB to the Monte Carlo objective. Let \\(w_k = f\\big(\\mathbf{x},\\mathbf{z}(\\mathbf{\\epsilon}^{(k)})\\big)\\), and \\(\\widetilde{w}_k = w_k / \\sum_{i=1}^K w_i\\), the gradient estimator for \\(\\mathcal{L}_K(\\mathbf{x};\\theta,\\phi)=\\mathbb{E}_{q(\\mathbf{z}^{(1:K)}|\\mathbf{x})}\\Big[\\log \\frac{1}{K} \\sum_{k=1}^K f\\big(\\mathbf{x},\\mathbf{z}^{(k)}\\big)\\Big]\\) is deduced by:\\[ \\begin{aligned} &amp;\\nabla\\,\\mathbb{E}_{q(\\mathbf{z}^{(1:K)}|\\mathbf{x})}\\Big[\\log \\frac{1}{K} \\sum_{k=1}^K f\\big(\\mathbf{x},\\mathbf{z}^{(k)}\\big)\\Big] = \\nabla \\, \\mathbb{E}_{q(\\mathbf{\\epsilon}^{(1:K)})}\\Bigg[\\log \\frac{1}{K} \\sum_{k=1}^K w_k\\Bigg] = \\mathbb{E}_{q(\\mathbf{\\epsilon}^{(1:K)})}\\Bigg[\\nabla \\log \\frac{1}{K} \\sum_{k=1}^K w_k\\Bigg] = \\\\ &amp; \\quad \\mathbb{E}_{q(\\mathbf{\\epsilon}^{(1:K)})}\\Bigg[\\frac{\\nabla \\frac{1}{K} \\sum_{k=1}^K w_k}{\\frac{1}{K} \\sum_{i=1}^K w_i}\\Bigg] = \\mathbb{E}_{q(\\mathbf{\\epsilon}^{(1:K)})}\\Bigg[\\frac{\\sum_{k=1}^K w_k \\nabla \\log w_k}{\\sum_{i=1}^K w_i}\\Bigg] = \\mathbb{E}_{q(\\mathbf{\\epsilon}^{(1:K)})}\\Bigg[\\sum_{k=1}^K \\widetilde{w}_k \\nabla \\log w_k\\Bigg] \\end{aligned} \\]NVILMnih and Gregor (2014) proposes a variant of REINFORCE, choosing the gradient estimator for \\(\\mathcal{L}(\\mathbf{x};\\theta,\\phi)=\\mathbb{E}_{q(\\mathbf{z}|\\mathbf{x})}\\big[f(\\mathbf{x},\\mathbf{z})\\big]\\) as:\\[ \\begin{aligned} \\nabla \\, \\mathbb{E}_{q(\\mathbf{z}|\\mathbf{x})} \\big[f(\\mathbf{x},\\mathbf{z})\\big] &amp;= \\mathbb{E}_{q(\\mathbf{z}|\\mathbf{x})}\\Big[ \\nabla f(\\mathbf{x},\\mathbf{z}) + f(\\mathbf{x},\\mathbf{z})\\,\\nabla\\log q(\\mathbf{z}|\\mathbf{x})\\Big] \\\\ &amp;= \\mathbb{E}_{q(\\mathbf{z}|\\mathbf{x})}\\Big[ \\nabla f(\\mathbf{x},\\mathbf{z}) + \\big(f(\\mathbf{x},\\mathbf{z}) - C_{\\psi}(\\mathbf{x})-c\\big)\\,\\nabla\\log q(\\mathbf{z}|\\mathbf{x})\\Big] \\end{aligned} \\]\\(C_{\\psi}(\\mathbf{x})\\) is a learnable network with parameter \\(\\psi\\), and \\(c\\) is a learnable constant. They should be learnt by minimizing \\(\\mathbb{E}_{ q(\\mathbf{z}|\\mathbf{x}) }\\Big[\\big(f(\\mathbf{x},\\mathbf{z}) - C_{\\psi}(\\mathbf{x})-c\\big)^2 \\Big]\\), leading to the gradient estimator for \\(\\psi\\):\\[ \\nabla_{\\psi} \\, \\mathbb{E}_{q(\\mathbf{z}|\\mathbf{x})}\\Big[\\big(f(\\mathbf{x},\\mathbf{z})-C_{\\psi}(\\mathbf{x})-c\\big)^2\\Big] = \\mathbb{E}_{q(\\mathbf{z}|\\mathbf{x})}\\Big[-2\\, \\big(f(\\mathbf{x},\\mathbf{z})-C_{\\psi}(\\mathbf{x})-c\\big) \\, \\nabla_{\\psi} \\, C_{\\psi}(\\mathbf{x})\\Big] \\]The NVIL algorithm glues these two objectives together with a proper coefficient, and performs SGD on them together.VIMCOMnih and Rezende (2016) suggests to use the unrelated samples of the Monte Carlo objective, instead of the dedicated learnable \\(C_{\\psi}(\\mathbf{x})\\) and \\(c\\), to centralize the learning signals. The VIMCO gradient estimator for \\(\\mathcal{L}_K(\\mathbf{x};\\theta,\\phi)=\\mathbb{E}_{q(\\mathbf{z}^{(1:K)}|\\mathbf{x})}\\Big[\\log \\frac{1}{K} \\sum_{k=1}^K f\\big(\\mathbf{x},\\mathbf{z}^{(k)}\\big)\\Big]\\) is deduced by:\\[ \\begin{aligned} &amp;\\nabla\\,\\mathbb{E}_{q(\\mathbf{z}^{(1:K)}|\\mathbf{x})}\\Big[\\log \\frac{1}{K} \\sum_{k=1}^K f\\big(\\mathbf{x},\\mathbf{z}^{(k)}\\big)\\Big] \\\\ &amp;\\quad = \\mathbb{E}_{q(\\mathbf{z}^{(1:K)}|\\mathbf{x})}\\bigg[{\\sum_{k=1}^K \\hat{L}(\\mathbf{z}^{(k)}|\\mathbf{z}^{(-k)}) \\, \\nabla \\log q(\\mathbf{z}^{(k)}|\\mathbf{x})}\\bigg] + \\mathbb{E}_{q(\\mathbf{z}^{(1:K)}|\\mathbf{x})}\\bigg[{\\sum_{k=1}^K \\widetilde{w}_k\\,\\nabla\\log f(\\mathbf{x},\\mathbf{z}^{(k)})}\\bigg] \\end{aligned} \\]where \\(w_k = f\\big(\\mathbf{x},\\mathbf{z}^{(k)}\\big)\\), \\(\\widetilde{w}_k = w_k / \\sum_{i=1}^K w_i\\), and:\\[ \\begin{aligned} \\hat{L}(\\mathbf{z}^{(k)}|\\mathbf{z}^{(-k)}) &amp;= \\hat{L}(\\mathbf{z}^{(1:K)}) - \\log \\frac{1}{K} \\bigg(\\hat{f}(\\mathbf{x},\\mathbf{z}^{(-k)})+\\sum_{i \\neq k} f(\\mathbf{x},\\mathbf{z}^{(i)})\\bigg) \\\\ \\hat{L}(\\mathbf{z}^{(1:K)}) &amp;= \\log \\frac{1}{K} \\sum_{k=1}^K f(\\mathbf{x},\\mathbf{z}^{(k)}) \\\\ \\hat{f}(\\mathbf{x},\\mathbf{z}^{(-k)}) &amp;= \\exp\\big(\\frac{1}{K-1} \\sum_{i \\neq k} \\log f(\\mathbf{x},\\mathbf{z}^{(i)})\\big) \\end{aligned} \\]ReferencesBurda, Yuri, Roger Grosse, and Ruslan Salakhutdinov. 2015. “Importance Weighted Autoencoders.” arXiv Preprint arXiv:1509.00519.Kingma, Diederik P, and Max Welling. 2014. “Auto-Encoding Variational Bayes.” In Proceedings of the International Conference on Learning Representations.Mnih, Andriy, and Karol Gregor. 2014. “Neural Variational Inference and Learning in Belief Networks.” arXiv Preprint arXiv:1402.0030.Mnih, Andriy, and Danilo Rezende. 2016. “Variational Inference for Monte Carlo Objectives.” In PMLR, 2188–96.","categories":[{"name":"Deep_Learning","slug":"Deep-Learning","permalink":"https://wiki.haowen-xu.com/categories/Deep-Learning/"},{"name":"Variational_Autoencoder","slug":"Deep-Learning/Variational-Autoencoder","permalink":"https://wiki.haowen-xu.com/categories/Deep-Learning/Variational-Autoencoder/"}],"tags":[]}]}