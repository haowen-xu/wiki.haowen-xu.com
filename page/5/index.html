<!DOCTYPE html><html lang="en"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="utf-8"><title>My Research Wiki</title><meta name="keywords" content="My Research Wiki"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta property="og:type" content="website"><meta property="og:title" content="My Research Wiki"><meta property="og:url" content="https://wiki.haowen-xu.com/page/5/index.html"><meta property="og:site_name" content="My Research Wiki"><meta property="og:locale" content="en"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="My Research Wiki"><link rel="alternate" href="/atom.xml" title="My Research Wiki" type="application/atom+xml"><link rel="icon" href="/favicon.ico"><link rel="stylesheet" href="/libs/font-awesome/css/font-awesome.min.css"><link rel="stylesheet" href="/libs/open-sans/styles.css"><link rel="stylesheet" href="/libs/source-code-pro/styles.css"><link rel="stylesheet" href="/css/style.css"><script src="/libs/jquery/2.1.3/jquery.min.js"></script><script src="/libs/jquery/plugins/cookie/1.4.1/jquery.cookie.js"></script><link rel="stylesheet" href="/libs/lightgallery/css/lightgallery.min.css"><link rel="stylesheet" href="/libs/justified-gallery/justifiedGallery.min.css"></head></html><body><div id="container"><header id="header"><div id="header-main" class="header-inner"><div class="outer"><a href="/" id="logo"><i class="logo"></i> <span class="site-title">My Research Wiki</span></a><nav id="main-nav"><a class="main-nav-link" href="/">Home</a> <a class="main-nav-link" href="/archives">Archives</a> <a class="main-nav-link" href="/categories">Categories</a></nav><div id="search-form-wrap"><form class="search-form"><input type="text" class="ins-search-input search-form-input" placeholder="Search"> <button type="submit" class="search-form-submit"></button></form><div class="ins-search"><div class="ins-search-mask"></div><div class="ins-search-container"><div class="ins-input-wrapper"><input type="text" class="ins-search-input" placeholder="Type something..."> <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span></div><div class="ins-section-wrapper"><div class="ins-section-container"></div></div></div></div><script>window.INSIGHT_CONFIG={TRANSLATION:{POSTS:"Posts",PAGES:"Pages",CATEGORIES:"Categories",TAGS:"Tags",UNTITLED:"(Untitled)"},ROOT_URL:"/",CONTENT_URL:"/content.json"}</script><script src="/js/insight.js"></script></div></div></div><div id="main-nav-mobile" class="header-sub header-inner"><table class="menu outer"><tr><td><a class="main-nav-link" href="/">Home</a></td><td><a class="main-nav-link" href="/archives">Archives</a></td><td><a class="main-nav-link" href="/categories">Categories</a></td><td><div class="search-form"><input type="text" class="ins-search-input search-form-input" placeholder="Search"></div></td></tr></table></div></header><div class="outer"><aside id="sidebar"><div class="widget-wrap" id="categories"><h3 class="widget-title"><span>categories</span> &nbsp; <a id="allExpand" href="#"><i class="fa fa-angle-double-down fa-2x"></i></a></h3><ul class="unstyled" id="tree"><li class="directory"><a href="#" data-role="directory"><i class="fa fa-folder"></i> &nbsp; Deep Learning</a><ul class="unstyled" id="tree"><li class="directory"><a href="#" data-role="directory"><i class="fa fa-folder"></i> &nbsp; CV</a><ul class="unstyled" id="tree"><li class="file"><a href="/Deep_Learning/CV/Image_Segmentation/">Image Segmentation</a></li></ul></li><li class="directory"><a href="#" data-role="directory"><i class="fa fa-folder"></i> &nbsp; Confronting Partition Function</a><ul class="unstyled" id="tree"><li class="file"><a href="/Deep_Learning/Confronting_Partition_Function/Overview/">Overview</a></li><li class="file"><a href="/Deep_Learning/Confronting_Partition_Function/Contrastive_Divergence/">Contrastive Divergence</a></li><li class="file"><a href="/Deep_Learning/Confronting_Partition_Function/Score_Matching/">Score Matching</a></li><li class="file"><a href="/Deep_Learning/Confronting_Partition_Function/Softmax_Speedup/">Softmax Speedup</a></li></ul></li><li class="directory"><a href="#" data-role="directory"><i class="fa fa-folder"></i> &nbsp; Energy Based Models</a><ul class="unstyled" id="tree"><li class="file"><a href="/Deep_Learning/Energy_Based_Models/Overview/">Overview</a></li><li class="file"><a href="/Deep_Learning/Energy_Based_Models/Energy_Function_in_Probabilistic_Models/">Energy Function in Probabilistic Models</a></li><li class="file"><a href="/Deep_Learning/Energy_Based_Models/Restricted_Boltzmann_Machine/">Restricted Boltzmann Machine</a></li></ul></li><li class="directory"><a href="#" data-role="directory"><i class="fa fa-folder"></i> &nbsp; Evaluation</a><ul class="unstyled" id="tree"><li class="file"><a href="/Deep_Learning/Evaluation/Evaluation_Metrics/">Evaluation Metrics</a></li><li class="file"><a href="/Deep_Learning/Evaluation/Visualizing_High_Dimensional_Space/">Visualizing High Dimensional Space</a></li></ul></li><li class="directory"><a href="#" data-role="directory"><i class="fa fa-folder"></i> &nbsp; Generative Adversarial Nets</a><ul class="unstyled" id="tree"><li class="file"><a href="/Deep_Learning/Generative_Adversarial_Nets/Overview/">Overview</a></li><li class="file"><a href="/Deep_Learning/Generative_Adversarial_Nets/f-GAN/">f-GAN</a></li><li class="file"><a href="/Deep_Learning/Generative_Adversarial_Nets/Energy_GAN/">Energy GAN</a></li></ul></li><li class="directory"><a href="#" data-role="directory"><i class="fa fa-folder"></i> &nbsp; Graph Neural Networks</a><ul class="unstyled" id="tree"><li class="file"><a href="/Deep_Learning/Graph_Neural_Networks/Node_Embedding/">Node Embedding</a></li><li class="file"><a href="/Deep_Learning/Graph_Neural_Networks/Graph_Convolution_Network/">Graph Convolution Network</a></li><li class="file"><a href="/Deep_Learning/Graph_Neural_Networks/Graph_Auto_Encoder/">Graph Auto-Encoder</a></li></ul></li><li class="directory"><a href="#" data-role="directory"><i class="fa fa-folder"></i> &nbsp; Information Theoretical</a><ul class="unstyled" id="tree"><li class="file"><a href="/Deep_Learning/Information_Theoretical/KL-Divergence/">KL-Divergence</a></li><li class="file"><a href="/Deep_Learning/Information_Theoretical/Mutual_Information/">Mutual Information</a></li></ul></li><li class="directory"><a href="#" data-role="directory"><i class="fa fa-folder"></i> &nbsp; Monte Carlo Methods</a><ul class="unstyled" id="tree"><li class="file"><a href="/Deep_Learning/Monte_Carlo_Methods/Overview/">Overview</a></li><li class="file"><a href="/Deep_Learning/Monte_Carlo_Methods/Monte_Carlo_Integration/">Monte Carlo Integration</a></li><li class="file"><a href="/Deep_Learning/Monte_Carlo_Methods/Accept_Reject_Sampling/">Accept-Reject Sampling</a></li><li class="file"><a href="/Deep_Learning/Monte_Carlo_Methods/Markov_Chain/">Markov Chain</a></li><li class="file"><a href="/Deep_Learning/Monte_Carlo_Methods/Metropolis_Hastings_Algorithm/">Metropolis-Hastings Algorithm</a></li><li class="file"><a href="/Deep_Learning/Monte_Carlo_Methods/Gibbs_Sampler/">Gibbs Sampler</a></li><li class="file"><a href="/Deep_Learning/Monte_Carlo_Methods/Hamiltonian_Dynamics/">Hamiltonian Dynamics</a></li><li class="file"><a href="/Deep_Learning/Monte_Carlo_Methods/Langevin_Dynamics/">Langevin Dynamics</a></li></ul></li><li class="directory"><a href="#" data-role="directory"><i class="fa fa-folder"></i> &nbsp; Optimization</a><ul class="unstyled" id="tree"><li class="file"><a href="/Deep_Learning/Optimization/Gradient_Tricks/">Gradient Tricks</a></li><li class="file"><a href="/Deep_Learning/Optimization/Loss_Surface_and_Generalization/">Loss Surface and Generalization</a></li><li class="file"><a href="/Deep_Learning/Optimization/Stochastic_Gradient_Descent/">Stochastic Gradient descent</a></li></ul></li><li class="directory"><a href="#" data-role="directory"><i class="fa fa-folder"></i> &nbsp; Variational Autoencoder</a><ul class="unstyled" id="tree"><li class="file"><a href="/Deep_Learning/Variational_Autoencoder/Overview/">Overview</a></li><li class="file"><a href="/Deep_Learning/Variational_Autoencoder/Sequential_VAE/">Sequential VAE</a></li><li class="file"><a href="/Deep_Learning/Variational_Autoencoder/Gradient_Estimators_for_Variational_Inference/">Gradient Estimators for Variational Inference</a></li><li class="file"><a href="/Deep_Learning/Variational_Autoencoder/Theoretical_Facts/">Theoretical Facts about VAEs</a></li></ul></li></ul></li><li class="directory"><a href="#" data-role="directory"><i class="fa fa-folder"></i> &nbsp; Mathematics</a><ul class="unstyled" id="tree"><li class="directory"><a href="#" data-role="directory"><i class="fa fa-folder"></i> &nbsp; Analysis</a><ul class="unstyled" id="tree"><li class="file"><a href="/Mathematics/Analysis/Linear_Space_vs_Functional_Space/">Linear space vs functional space</a></li></ul></li><li class="directory"><a href="#" data-role="directory"><i class="fa fa-folder"></i> &nbsp; Calculus</a><ul class="unstyled" id="tree"><li class="file"><a href="/Mathematics/Calculus/Calculus_of_Variations/">Calculus of Variations</a></li></ul></li><li class="directory"><a href="#" data-role="directory"><i class="fa fa-folder"></i> &nbsp; Differential Equations</a><ul class="unstyled" id="tree"><li class="file"><a href="/Mathematics/Differential_Equations/Probability_Distribution_Equations/">Differential Equations on Probability Distributions</a></li></ul></li><li class="directory"><a href="#" data-role="directory"><i class="fa fa-folder"></i> &nbsp; Optimization</a><ul class="unstyled" id="tree"><li class="file"><a href="/Mathematics/Optimization/Convex_Optimization/">Convex Optimization</a></li></ul></li></ul></li><li class="directory"><a href="#" data-role="directory"><i class="fa fa-folder"></i> &nbsp; Research Work</a><ul class="unstyled" id="tree"><li class="file"><a href="/Research_Work/Reading_List/">Reading List</a></li><li class="file"><a href="/Research_Work/Tracking_the_Concepts/">Tracking the Concepts</a></li><li class="file"><a href="/Research_Work/Directions_to_Explore/">Directions to Explore</a></li></ul></li></ul></div><script>$(document).ready(function(){var r="fa-folder-open",i="fa-folder",l="fa-angle-double-down",d="fa-angle-double-up";$(document).on("click",'#categories a[data-role="directory"]',function(a){a.preventDefault();var e=$(this).children(".fa"),s=e.hasClass(r),l=$(this).siblings("ul");e.removeClass(r).removeClass(i),s?(void 0!==l&&l.slideUp({duration:100}),e.addClass(i)):(void 0!==l&&l.slideDown({duration:100}),e.addClass(r))}),$('#categories a[data-role="directory"]').bind("contextmenu",function(a){a.preventDefault();var e=$(this).children(".fa"),s=e.hasClass(r),l=$(this).siblings("ul"),d=$.merge(l.find("li ul"),l),o=$.merge(l.find(".fa"),e);o.removeClass(r).removeClass(i),s?(d.slideUp({duration:100}),o.addClass(i)):(d.slideDown({duration:100}),o.addClass(r))}),$(document).on("click","#allExpand",function(a){a.preventDefault();var e=$(this).children(".fa"),s=e.hasClass(l);e.removeClass(l).removeClass(d),s?($("#sidebar .fa.fa-folder").removeClass("fa-folder").addClass("fa-folder-open"),$("#categories li ul").slideDown({duration:100}),e.addClass(d)):($("#sidebar .fa.fa-folder-open").removeClass("fa-folder-open").addClass("fa-folder"),$("#categories li ul").slideUp({duration:100}),e.addClass(l))})})</script><div id="toTop" class="fa fa-angle-up"></div></aside><section id="main"><article id="post-Deep_Learning/Graph_Neural_Networks/Graph_Convolution_Network" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-inner"><header class="article-header"><div class="article-meta"><div class="article-category"><i class="fa fa-folder"></i> <a class="article-category-link" href="/categories/Deep-Learning/">Deep Learning</a><i class="fa fa-angle-right"></i><a class="article-category-link" href="/categories/Deep-Learning/Graph-Neural-Networks/">Graph Neural Networks</a></div><div class="article-date"><i class="fa fa-calendar"></i> <a href="/Deep_Learning/Graph_Neural_Networks/Graph_Convolution_Network/"><time datetime="2020-03-07T19:02:45.000Z" itemprop="datePublished">2020-03-08</time></a></div><div class="article-meta-button"><a href="https://github.com/haowen-xu/research-notes/raw/master/source/_posts/Deep_Learning/Graph_Neural_Networks/Graph_Convolution_Network.md">Source</a></div><div class="article-meta-button"><a href="https://github.com/haowen-xu/research-notes/edit/master/source/_posts/Deep_Learning/Graph_Neural_Networks/Graph_Convolution_Network.md">Edit</a></div><div class="article-meta-button"><a href="https://github.com/haowen-xu/research-notes/commits/master/source/_posts/Deep_Learning/Graph_Neural_Networks/Graph_Convolution_Network.md">History</a></div></div><h1 itemprop="name"><a class="article-title" href="/Deep_Learning/Graph_Neural_Networks/Graph_Convolution_Network/">Graph Convolution Network</a></h1></header><div class="article-entry" itemprop="articleBody"><h2 id="overview">Overview</h2><p>To spread the node features across the graph, according to the graph structure (typically local connectivity among the nodes).</p><p>The features after applying the <span class="math inline">\(l\)</span>-th graph convolution layer can be denoted as <span class="math inline">\(\mathbf{H}^{(l)}\)</span>, where it should be a <span class="math inline">\(n \times k\)</span> matrix, with <span class="math inline">\(n\)</span> being the number of nodes in the graph, and <span class="math inline">\(k\)</span> being the number of feature dimensions. <span class="math inline">\(\mathbf{h}^{(l)}_i\)</span> refers to the <span class="math inline">\(i\)</span>-th row of <span class="math inline">\(\mathbf{H}^{(l)}\)</span>, denoting the feature of the <span class="math inline">\(i\)</span>-th node after applying the <span class="math inline">\(l\)</span>-th layer.</p><p>The connectivity among the nodes can be represented by the adjacency matrix <span class="math inline">\(\mathbf{A}\)</span>, an <span class="math inline">\(n \times n\)</span> matrix, where <span class="math inline">\(\mathbf{A}_{ij}\)</span> representing the connectivity from node <span class="math inline">\(j\)</span> to <span class="math inline">\(i\)</span>. For undirected graph, <span class="math inline">\(\mathbf{A}\)</span> should be symmetric.</p><h2 id="feature-matrix">Feature Matrix</h2><p>Every node in the graph requires an initial feature matrix, for GCN layers to propagate along the connections between nodes. Besides using the task-specific features, one may also use:</p><ul><li>Node degree information <span class="citation" data-cites="cangeaSparseHierarchicalGraph2018">(Cangea et al. <a href="#ref-cangeaSparseHierarchicalGraph2018" role="doc-biblioref">2018</a>)</span>: one-hot encoding the node degree for all degrees up to a given upper bound.</li><li>One-hot index vector <span class="citation" data-cites="yaoGraphConvolutionalNetworks2019">(Yao, Mao, and Luo <a href="#ref-yaoGraphConvolutionalNetworks2019" role="doc-biblioref">2019</a>)</span>: one-hot encoding the index of nodes.</li></ul><h2 id="graph-convolution">Graph Convolution</h2><h3 id="basic-formulation">Basic Formulation</h3><p>The GCN layer can be formulated as: <span class="math display">\[ \mathbf{H}^{(l + 1)} = f(\hat{\mathbf{A}} \mathbf{H}^{(l)}\mathbf{W}^{(l)}) \]</span> where <span class="math inline">\(\hat{\mathbf{A}}\)</span> is the normalized adjacency matrix. A popular choice for undirected graph may be <span class="citation" data-cites="wuComprehensiveSurveyGraph2019">(Wu et al. <a href="#ref-wuComprehensiveSurveyGraph2019" role="doc-biblioref">2019</a>)</span>: <span class="math display">\[ \hat{\mathbf{A}} = \mathbf{D}^{-1/2} \mathbf{A} \,\mathbf{D}^{-1/2} \]</span> where <span class="math inline">\(\mathbf{D}_{ii} = \sum_j \mathbf{A}_{ij}\)</span> is the degree of node <span class="math inline">\(i\)</span>.</p><p>Another choice for <span class="math inline">\(\hat{\mathbf{A}}\)</span>, which not only can be used for undirected grpah, but also can be used for directed ones, can be formulated as: <span class="math display">\[ \hat{\mathbf{A}} = \mathbf{D}^{-1} \mathbf{A} \]</span> Some work may add auxiliary self-loop to the adjacency matrix, such that the normalized adjacency matrix becomes: <span class="math display">\[ \hat{\mathbf{A}} = \mathbf{D}^{-1/2} (\mathbf{A} + \lambda\,\mathbf{I}) \,\mathbf{D}^{-1/2} \]</span> or (used by <span class="citation" data-cites="zhangEndtoendDeepLearning2018">Zhang et al. (<a href="#ref-zhangEndtoendDeepLearning2018" role="doc-biblioref">2018</a>)</span>): <span class="math display">\[ \hat{\mathbf{A}} = \mathbf{D}^{-1} (\mathbf{A} + \lambda\,\mathbf{I}) \]</span></p><h3 id="hetergeneous-graph-with-various-edge-relationship">Hetergeneous Graph with Various Edge Relationship</h3><p>If the link between nodes represent different relationships (<span class="math inline">\(R = \{r\}\)</span>), each relationship may have their own kernel <span class="math inline">\(\mathbf{W}_r^{(l)}\)</span>, such that the GCN layer can be formulated as <span class="citation" data-cites="schlichtkrullModelingRelationalData2017">(Schlichtkrull et al. <a href="#ref-schlichtkrullModelingRelationalData2017" role="doc-biblioref">2017</a>)</span>: <span class="math display">\[ \mathbf{h}^{(l+1)}_i = f\left(\sum_{r \in R} \sum_{j \in \mathcal{N}^r_i} \frac{1}{c_{ij}} \mathbf{W}_r^{(l)} \mathbf{h}^{(l)}_j + \mathbf{W}_0^{(l)} \mathbf{h}^{(l)}_i \right) \]</span></p><p>where <span class="math inline">\(c_{ij} = \left| \mathcal{N}_i^r \right|\)</span> is the normalizing factor.</p><h3 id="hetergeneuos-graph-with-various-node-type">Hetergeneuos Graph with Various Node Type</h3><p>If the node are grouped into different types, i.e., node <span class="math inline">\(v_i\)</span> has type <span class="math inline">\(t_i\)</span>, then each type can have their own kernel <span class="math inline">\(\mathbf{W}_{t}^{(l)}\)</span>, and the GCN layer can be formulated as: <span class="math display">\[ \begin{align} \mathbf{h}^{(l+1)}_i &amp;= f\left( \sum_{j \in \mathcal{N}_{i}} c_{ij} \, g(\mathbf{h}_j^{(l)}) + g(\mathbf{h}_i^{(l)}) \right) \\ g(\mathbf{h}_i^{(l)}) &amp;= \mathbf{W}_{t_i}^{(l)} \mathbf{h}_i^{(l)} \end{align} \]</span> as is proposed by <span class="citation" data-cites="wangHeterogeneousAttributedNetwork2019">Wang et al. (<a href="#ref-wangHeterogeneousAttributedNetwork2019" role="doc-biblioref">2019</a>)</span>. For the normalizing factor <span class="math inline">\(c_{ij}\)</span>, <span class="citation" data-cites="wangHeterogeneousAttributedNetwork2019">Wang et al. (<a href="#ref-wangHeterogeneousAttributedNetwork2019" role="doc-biblioref">2019</a>)</span> also proposed to learn the attention score by self-attention: <span class="math display">\[ \begin{align} c_{ij}^{(l)} &amp;= \frac{\exp\left( s_{ij} \right)}{\sum_{\tilde{j} \in \mathcal{N}_i^{(l)}} \exp\left( s_{i\tilde{j}} \right)} \\ s_{ij} &amp;= g(\mathbf{h}_i^{(l)})^{\top} \cdot g(\mathbf{h}_j^{(l)}) \end{align} \]</span> where <span class="math inline">\(s_{ij}\)</span> is the attention score.</p><h3 id="lstm-aggregation-with-random-permuation-of-neighborhood-nodes">LSTM Aggregation with Random Permuation of Neighborhood Nodes</h3><p><span class="citation" data-cites="hamiltonInductiveRepresentationLearning2017">Hamilton, Ying, and Leskovec (<a href="#ref-hamiltonInductiveRepresentationLearning2017" role="doc-biblioref">2017</a>)</span> proposed a candidate neighborhoold information aggregation method, based on LSTM with random permutation of neighborhood nodes.</p><h3 id="neighborhood-sampling-instead-of-summation">Neighborhood Sampling Instead of Summation</h3><p><span class="citation" data-cites="hamiltonInductiveRepresentationLearning2017">Hamilton, Ying, and Leskovec (<a href="#ref-hamiltonInductiveRepresentationLearning2017" role="doc-biblioref">2017</a>)</span> proposed to use neighborhood sampling to reduce the training computation time.</p><h3 id="monte-carlo-approximation-instead-of-summation">Monte-Carlo Approximation Instead of Summation</h3><p><span class="citation" data-cites="chenFastGCNFastLearning2018">Chen, Ma, and Xiao (<a href="#ref-chenFastGCNFastLearning2018" role="doc-biblioref">2018</a>)</span> proposed to view the summation over nodes within one GCN layer as an expectation, formulated as: <span class="math display">\[ \mathbf{h}^{(l+1)}(v) = f\left( \int \hat{\mathbf{A}}(v,u) \,\mathbf{h}^{(l)}(u)\,\mathbf{W}^{(l)}\,dP(u) \right) \]</span> where <span class="math inline">\(u\)</span> and <span class="math inline">\(v\)</span> represent the nodes. <span class="math inline">\(P(u)\)</span> is regarded as a uniform distribution. According to this formulation, the authors proposed to use a proposal distribution: <span class="math display">\[ q(u) = \left\| \hat{\mathbf{A}}(:,u) \right\|^2 / \sum_{u'} \left\| \hat{\mathbf{A}}(:,u') \right\|^2 \]</span> which is the power of node <span class="math inline">\(u\)</span>'s out-degree, divided by the sum of the power of all nodes' out-degree. The proposal distribution is used to sample <span class="math inline">\(t_l\)</span> i.i.d. nodes for each GCN layer, namely <span class="math inline">\(u_1^{(l)}, \dots, u_{t_l}^{(l)}\)</span>, and estimate <span class="math inline">\(\mathbf{h}^{(l+1)}\)</span> by: <span class="math display">\[ \mathbf{h}^{(l+1)}(v) \approx f\left( \frac{1}{t_l}\sum_{j=1}^{t_l} \frac{\hat{\mathbf{A}(v,u_j^{(l)})}\,\mathbf{h}^{(l)}(u_j^{(l)})\,\mathbf{W}^{(l)}}{q(u_j^{(l)})} \right) \]</span></p><h2 id="learning-the-weight-of-message-passing">Learning the Weight of Message Passing</h2><h3 id="self-attention">Self-Attention</h3><p>Instead of relying on the fixed adjacency matrix to weight the neighborhood information in graph convolution networks, <span class="citation" data-cites="velickovicGraphAttentionNetworks2017">Veličković et al. (<a href="#ref-velickovicGraphAttentionNetworks2017" role="doc-biblioref">2017</a>)</span> proposed to use a self-attention mechanism to weight the neighborhood information.</p><p>The <em>attention coefficients</em> is calculated by: <span class="math display">\[ e_{ij} = a\left(\mathbf{W} \mathbf{h}_i, \mathbf{W} \mathbf{h}_j\right) \]</span> where <span class="math inline">\(a(\cdot)\)</span> is the attention network. <span class="math inline">\(e_{ij}\)</span> is formulated by <span class="citation" data-cites="velickovicGraphAttentionNetworks2017">Veličković et al. (<a href="#ref-velickovicGraphAttentionNetworks2017" role="doc-biblioref">2017</a>)</span> as follows: <span class="math display">\[ e_{ij} = a\left(\mathbf{W} \mathbf{h}_i, \mathbf{W} \mathbf{h}_j\right) = \mathrm{LeakyReLU}\left({\mathbf{a}}^{\top}\left[ \mathbf{W} \mathbf{h}_i \big\| \mathbf{W} \mathbf{h}_j \right]\right) \]</span> This attention score is further normalized as: <span class="math display">\[ \alpha_{ij} = \frac{\exp\left(e_{ij}\right)}{\sum_{k \in \mathcal{N}_i} \exp\left(e_{ik}\right)} = \frac{\exp\left(\mathrm{LeakyReLU}\left({\mathbf{a}}^{\top}\left[ \mathbf{W} \mathbf{h}_i \big\| \mathbf{W} \mathbf{h}_j \right]\right)\right)}{\sum_{k \in \mathcal{N}_i} \exp\left(\mathrm{LeakyReLU}\left({\mathbf{a}}^{\top}\left[ \mathbf{W} \mathbf{h}_i \big\| \mathbf{W} \mathbf{h}_k \right]\right)\right)} \]</span> The final output features of this layer is: <span class="math display">\[ \mathbf{h}' = \sigma\left( \sum_{j \in \mathcal{N}_i} \alpha_{ij} \mathbf{W} \mathbf{h}_j \right) \]</span> If using <em>mult-head attention</em>, the output features can be formulated as either one of the following (i.e., <em>concat</em> and <em>average</em> multi-head attention): <span class="math display">\[ \begin{align} \mathbf{h}' &amp;= \mathop{\bigg\|}_{k=1}^K \sigma\left( \sum_{j \in \mathcal{N}_i} \alpha^k_{ij} \mathbf{W}^k \mathbf{h}_j \right) \\ \mathbf{h}' &amp;= \sigma\left( \frac{1}{K} \sum_{k=1}^K \sum_{j \in \mathcal{N}_i} \alpha^k_{ij} \mathbf{W}^k \mathbf{h}_j \right) \end{align} \]</span></p><p>where the second formulation is adopted as the output layer.</p><h2 id="node-embedding">Node Embedding</h2><p>The feature vector <span class="math inline">\(\mathbf{h}^{(L)}\)</span> after <span class="math inline">\(L\)</span>-th GCN layer can be used as the node embedding <span class="citation" data-cites="hamiltonInductiveRepresentationLearning2017 schlichtkrullModelingRelationalData2017">(Hamilton, Ying, and Leskovec <a href="#ref-hamiltonInductiveRepresentationLearning2017" role="doc-biblioref">2017</a>; Schlichtkrull et al. <a href="#ref-schlichtkrullModelingRelationalData2017" role="doc-biblioref">2017</a>)</span>. The unsupervised loss, using negative sampling, should be: <span class="math display">\[ \mathcal{L} = \sum_{v_i \in V} \sum_{v_j \in N(v_i)} \left[ \log \frac{\exp(\mathbf{h}^{(L)}_i \cdot \mathbf{h}^{(L)}_j)}{\exp(\mathbf{h}^{(L)}_i \cdot \mathbf{h}^{(L)}_j) + 1} + \sum_{\tilde{v}_n \sim q(v) \\ n = 1 \dots k} \log\frac{1}{\exp(\mathbf{h}^{(L)}_i \cdot \tilde{\mathbf{h}}^{(L)}_n) + 1} \right] \]</span> same as the <a href="/Deep_Learning/Graph_Neural_Networks/Node_Embedding/#undirected-graph">negative sampling</a> method for node embedding. This embedding loss can be used along in a fully unsupervised task; or as a augumented/regularization term for a supervised task, along with the main objective for the specific task <span class="citation" data-cites="hamiltonInductiveRepresentationLearning2017">(Hamilton, Ying, and Leskovec <a href="#ref-hamiltonInductiveRepresentationLearning2017" role="doc-biblioref">2017</a>)</span>.</p><h2 id="pooling">Pooling</h2><h3 id="gpool">gPool</h3><p><span class="citation" data-cites="cangeaSparseHierarchicalGraph2018">Cangea et al. (<a href="#ref-cangeaSparseHierarchicalGraph2018" role="doc-biblioref">2018</a>)</span> proposed the <em>projection score</em> to sort the nodes, and choose the top-<span class="math inline">\(\lceil kN \rceil\)</span> nodes after pooling. <span class="math inline">\(k \in (0, 1]\)</span> is the <em>pooling ratio</em>, while <span class="math inline">\(N\)</span> is the numberr of input nodes. The projection score <span class="math inline">\(\mathbf{y}\)</span> and the pooling method is formulated as: <span class="math display">\[ \mathbf{Z} = \frac{\mathbf{X} \mathbf{p}}{\left\| \mathbf{p} \right\|_2} \qquad \overrightarrow{i} = \mathop{\text{top-rank}}\left( \mathbf{Z}, \lceil kN \rceil \right) \qquad \mathbf{X}' = \left( \mathbf{X} \odot \tanh(\mathbf{Z}) \right)_{\overrightarrow{i}} \qquad \mathbf{A}' = \mathbf{A}_{\overrightarrow{i},\overrightarrow{i}} \]</span> where <span class="math inline">\(\mathbf{X}\)</span> is feature matrix of input nodes, <span class="math inline">\(\mathbf{p}\)</span> is the learnable parameter for the projection. <span class="math inline">\(\cdot_{\overrightarrow{i}}\)</span> is the indexing operation.</p><p><span class="citation" data-cites="cangeaSparseHierarchicalGraph2018">Cangea et al. (<a href="#ref-cangeaSparseHierarchicalGraph2018" role="doc-biblioref">2018</a>)</span> proposed a similar architecture, named <em>gPool</em>, with <span class="math inline">\(\tanh(\mathbf{Z})\)</span> in <span class="math inline">\(\mathbf{X}' = \left( \mathbf{X} \odot \tanh(\mathbf{Z}) \right)_{\overrightarrow{i}}\)</span> replaced by <span class="math inline">\(\text{sigmoid}(\mathbf{\mathbf{Z}})\)</span>, mimic the often adopted gate mechanism.</p><p><strong>Note</strong>: <span class="math inline">\(\tanh(\mathbf{Z})\)</span> or <span class="math inline">\(\text{sigmoid}(\mathbf{Z})\)</span> enables the gradient to be passed along <span class="math inline">\(\mathbf{p}\)</span>. Without this term, <span class="math inline">\(\mathbf{p}\)</span> produces a solely discrete element selection, which cannot be trained by stochastic gradient descent.</p><h3 id="self-attention-graph-pooling">Self-Attention Graph Pooling</h3><p><span class="citation" data-cites="leeSelfattentionGraphPooling2019">Lee, Lee, and Kang (<a href="#ref-leeSelfattentionGraphPooling2019" role="doc-biblioref">2019</a>)</span> proposed to improve the projection score based pooling by adding a self-attention mechanism to calculate the projection score. The new projection score is derived as: <span class="math display">\[ \mathbf{Z} = f(\mathop{\mathrm{GNN}}(\mathbf{X},\mathbf{A})) \]</span> where <span class="math inline">\(f\)</span> is the activation function, <span class="math inline">\(\mathbf{X}\)</span> is the node feature matrix to be pooled, <span class="math inline">\(\mathbf{A}\)</span> is the adjacency matrix, which may be normalized. <span class="math inline">\(\mathop{\mathrm{GNN}}\)</span> indicates a general graph network, and authors proposed the following variants of this GNN network:</p><ul><li><span class="math inline">\(\mathbf{Z} = f\left(\tilde{\mathbf{D}}^{-1/2}\,\tilde{\mathbf{A}}\,\tilde{\mathbf{D}}^{1/2}\,\mathbf{X}\,\mathbf{p}\right)\)</span>, where <span class="math inline">\(\tilde{\mathbf{A}} = \mathbf{A}+\mathbf{I}\)</span> is the normalized adjacency matrix with self-loop, and <span class="math inline">\(\tilde{\mathbf{D}}\)</span> is the degree matrix of <span class="math inline">\(\tilde{\mathbf{A}}\)</span>.</li><li><span class="math inline">\(\mathbf{Z} = f\left(\mathop{\mathrm{GNN}}(\mathbf{X},\mathbf{A}+\mathbf{A}^2)\right)\)</span>, which considers the second-order neighborhood by augument 2-hop nodes in the adjacency matrix <span class="math inline">\(\mathbf{A}\)</span>.</li><li><span class="math inline">\(\mathbf{Z} = f\left(\mathop{\mathrm{GNN}_2}(\mathop{\mathrm{GNN}_1}(\mathbf{X},\mathbf{A}),\mathbf{A})\right)\)</span>, which considers the second-order neighborhood by stacking GNN layers.</li><li><span class="math inline">\(\mathbf{Z}=\frac{1}{M}\sum_{m} f\left( \mathop{\mathrm{GNN}_m}(\mathbf{X},\mathbf{A}) \right))\)</span>, which using an average of multiple attention scores.</li></ul><h3 id="hierarchical-pooling-vs-global-pooling">Hierarchical Pooling vs Global Pooling</h3><figure><img src="/Deep_Learning/Graph_Neural_Networks/Graph_Convolution_Network/Global_Pooling_vs_Hierarchical_Pooling.png" id="fig:Global_Pooling_vs_Hierarchical_Pooling" data-max-height="640px" alt="" style="max-height:640px"><figcaption>Figure 1: Global Pooling vs Hierarchical Pooling (view <a href="/Deep_Learning/Graph_Neural_Networks/Graph_Convolution_Network/Global_Pooling_vs_Hierarchical_Pooling.pdf">pdf</a>)</figcaption></figure><p><span class="citation" data-cites="leeSelfattentionGraphPooling2019">Lee, Lee, and Kang (<a href="#ref-leeSelfattentionGraphPooling2019" role="doc-biblioref">2019</a>)</span> found that hierarchical pooling seems to work better on large graphs, where global pooling works better on small graphs.</p><h2 id="readout">Readout</h2><h3 id="combine-the-global-average-and-max-pooling">Combine the Global Average and Max Pooling</h3><figure><img src="/Deep_Learning/Graph_Neural_Networks/Graph_Convolution_Network/Combine_the_Global_Average_and_Max_Pooling.png" id="fig:Combine_the_Global_Average_and_Max_Pooling" data-max-height="240px" alt="" style="max-height:240px"><figcaption>Figure 2: Combine the Global Average and Max Pooling (view <a href="/Deep_Learning/Graph_Neural_Networks/Graph_Convolution_Network/Combine_the_Global_Average_and_Max_Pooling.pdf">pdf</a>)</figcaption></figure><p><span class="citation" data-cites="cangeaSparseHierarchicalGraph2018">Cangea et al. (<a href="#ref-cangeaSparseHierarchicalGraph2018" role="doc-biblioref">2018</a>)</span> proposed a graph readout layer that combines both the <em>global average</em> and <em>global max</em> pooling techniques. Specifically, for the output graph of every <span class="math inline">\(l\)</span>-th pooling layer <span class="math inline">\((\mathbf{X}^{(l)}, \mathbf{A}^{(l)})\)</span>, the readout layer produces the summarized output by: <span class="math display">\[ \mathbf{s}^{(l)} = \mathop{\mathrm{Concat}}\left(\frac{1}{N^{(l)}} \sum_{i=1}^{N^{(l)}} \mathbf{x}^{(l)}_i \Bigg\|\max_{i=1}^{N^{(l)}} \mathbf{x}^{(l)}_i\right) \]</span> And the summarized output of all pooling layers <span class="math inline">\(\mathbf{s}^{(1)}, \dots, \mathbf{s}^{(L)}\)</span> are then summed together: <span class="math display">\[ \mathbf{s} = \sum_{l=1}^L \mathbf{s}^{(l)} \]</span> as the final aggregated feature of the graph. The whole structure is demonstrated as fig.&nbsp;<a href="#fig:Combine_the_Global_Average_and_Max_Pooling">2</a>.</p><h2 id="practical-gcn-architectures">Practical GCN Architectures</h2><h3 id="modeling-relational-data-with-graph-convolutional-networks-2017">Modeling Relational Data with Graph Convolutional Networks [2017]</h3><p><span class="citation" data-cites="schlichtkrullModelingRelationalData2017">Schlichtkrull et al. (<a href="#ref-schlichtkrullModelingRelationalData2017" role="doc-biblioref">2017</a>)</span> proposed the following formulation for their GCN layer with regarding to the node relationship: <span class="math display">\[ \mathbf{h}^{(l+1)}_i = f\left(\sum_r \sum_{j \in \mathcal{N}^r_i} \frac{1}{c_{ij}} \mathbf{W}_r^{(l)} \mathbf{h}^{(l)}_j + \mathbf{W}_0^{(l)} \mathbf{h}^{(l)}_i \right) \]</span> where the term <span class="math inline">\(\mathbf{W}_0^{(l)} \mathbf{h}^{(l)}_i\)</span> represents the information passed along the self-loop from <span class="math inline">\(l\)</span>-th layer to the <span class="math inline">\((l+1)\)</span>-layer. <span class="math inline">\(c_{ij}\)</span> is the normalizing factor, and is chosen to be <span class="math inline">\(\left| \mathcal{N}_i^r \right|\)</span> (i.e., the in-coming degree of node <span class="math inline">\(i\)</span>) in their paper.</p><p>To avoid having too many parameters, which may be potentially prone to over-fitting, <span class="citation" data-cites="schlichtkrullModelingRelationalData2017">Schlichtkrull et al. (<a href="#ref-schlichtkrullModelingRelationalData2017" role="doc-biblioref">2017</a>)</span> also proposed two methods to regularize the weights <span class="math inline">\(\mathbf{W}^{(l)}_r\)</span> of different relationships.</p><p>For link prediction, <span class="citation" data-cites="schlichtkrullModelingRelationalData2017">Schlichtkrull et al. (<a href="#ref-schlichtkrullModelingRelationalData2017" role="doc-biblioref">2017</a>)</span> proposed to use the GCN output at <span class="math inline">\(L\)</span>-th layer <span class="math inline">\(\mathbf{h}_i^{(L)}\)</span> as the embedding <span class="math inline">\(\mathbf{e}_i\)</span> of nodes, deriving the score function as: <span class="math display">\[ f(i,r,j) = \mathbf{e}_i^{\top}\mathbf{R}_r\mathbf{e}_j \]</span> where <span class="math inline">\(\mathbf{R}_r\)</span> is a learned diagonal matrix, to represent the "dot-product" between <span class="math inline">\(\mathbf{e}_i\)</span> and <span class="math inline">\(\mathbf{e}_j\)</span> under the relationship <span class="math inline">\(r\)</span>. The whole graph is then trained by <a href="/Deep_Learning/Confronting_Partition_Function/Softmax_Speedup/#nagative-sampling">negative sampling</a>. See the <a href="/Deep_Learning/Graph_Neural_Networks/Node_Embedding/#multi-relational-link">loss function</a> for this situation.</p><h1 id="bibliography" class="unnumbered">References</h1><div id="refs" class="references" role="doc-bibliography"><div id="ref-cangeaSparseHierarchicalGraph2018"><p>Cangea, Cătălina, Petar Veličković, Nikola Jovanović, Thomas Kipf, and Pietro Liò. 2018. “Towards Sparse Hierarchical Graph Classifiers.” <em>arXiv Preprint arXiv:1811.01287</em>.</p></div><div id="ref-chenFastGCNFastLearning2018"><p>Chen, Jie, Tengfei Ma, and Cao Xiao. 2018. “FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling.” <em>arXiv:1801.10247 [Cs]</em>, January. <a href="http://arxiv.org/abs/1801.10247" target="_blank" rel="noopener">http://arxiv.org/abs/1801.10247</a>.</p></div><div id="ref-hamiltonInductiveRepresentationLearning2017"><p>Hamilton, Will, Zhitao Ying, and Jure Leskovec. 2017. “Inductive Representation Learning on Large Graphs.” In <em>Advances in Neural Information Processing Systems</em>, 1024–34.</p></div><div id="ref-leeSelfattentionGraphPooling2019"><p>Lee, Junhyun, Inyeop Lee, and Jaewoo Kang. 2019. “Self-Attention Graph Pooling.” <em>arXiv Preprint arXiv:1904.08082</em>.</p></div><div id="ref-schlichtkrullModelingRelationalData2017"><p>Schlichtkrull, Michael, Thomas N. Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and Max Welling. 2017. “Modeling Relational Data with Graph Convolutional Networks.” <em>arXiv:1703.06103 [Cs, Stat]</em>, October. <a href="http://arxiv.org/abs/1703.06103" target="_blank" rel="noopener">http://arxiv.org/abs/1703.06103</a>.</p></div><div id="ref-velickovicGraphAttentionNetworks2017"><p>Veličković, Petar, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. 2017. “Graph Attention Networks.” <em>arXiv Preprint arXiv:1710.10903</em>.</p></div><div id="ref-wangHeterogeneousAttributedNetwork2019"><p>Wang, Yueyang, Ziheng Duan, Binbing Liao, Fei Wu, and Yueting Zhuang. 2019. “Heterogeneous Attributed Network Embedding with Graph Convolutional Networks.” In <em>Proceedings of the AAAI Conference on Artificial Intelligence</em>, 33:10061–2.</p></div><div id="ref-wuComprehensiveSurveyGraph2019"><p>Wu, Zonghan, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and Philip S. Yu. 2019. “A Comprehensive Survey on Graph Neural Networks.” <em>arXiv:1901.00596 [Cs, Stat]</em>, August. <a href="http://arxiv.org/abs/1901.00596" target="_blank" rel="noopener">http://arxiv.org/abs/1901.00596</a>.</p></div><div id="ref-yaoGraphConvolutionalNetworks2019"><p>Yao, Liang, Chengsheng Mao, and Yuan Luo. 2019. “Graph Convolutional Networks for Text Classification.” In <em>Proceedings of the AAAI Conference on Artificial Intelligence</em>, 33:7370–7.</p></div><div id="ref-zhangEndtoendDeepLearning2018"><p>Zhang, Muhan, Zhicheng Cui, Marion Neumann, and Yixin Chen. 2018. “An End-to-End Deep Learning Architecture for Graph Classification.” In <em>Thirty-Second AAAI Conference on Artificial Intelligence</em>.</p></div></div></div><div style="height:10px"></div></div></article><article id="post-Deep_Learning/Graph_Neural_Networks/Node_Embedding" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-inner"><header class="article-header"><div class="article-meta"><div class="article-category"><i class="fa fa-folder"></i> <a class="article-category-link" href="/categories/Deep-Learning/">Deep Learning</a><i class="fa fa-angle-right"></i><a class="article-category-link" href="/categories/Deep-Learning/Graph-Neural-Networks/">Graph Neural Networks</a></div><div class="article-date"><i class="fa fa-calendar"></i> <a href="/Deep_Learning/Graph_Neural_Networks/Node_Embedding/"><time datetime="2020-03-06T01:07:30.000Z" itemprop="datePublished">2020-03-06</time></a></div><div class="article-meta-button"><a href="https://github.com/haowen-xu/research-notes/raw/master/source/_posts/Deep_Learning/Graph_Neural_Networks/Node_Embedding.md">Source</a></div><div class="article-meta-button"><a href="https://github.com/haowen-xu/research-notes/edit/master/source/_posts/Deep_Learning/Graph_Neural_Networks/Node_Embedding.md">Edit</a></div><div class="article-meta-button"><a href="https://github.com/haowen-xu/research-notes/commits/master/source/_posts/Deep_Learning/Graph_Neural_Networks/Node_Embedding.md">History</a></div></div><h1 itemprop="name"><a class="article-title" href="/Deep_Learning/Graph_Neural_Networks/Node_Embedding/">Node Embedding</a></h1></header><div class="article-entry" itemprop="articleBody"><h2 id="overview">Overview</h2><p>A graph is defined by <span class="math inline">\(G = V \times E\)</span>, where <span class="math inline">\(V = \{v_i\}\)</span> is the set of all vertex (nodes), and <span class="math inline">\(E=\{e_i\}\)</span> is the set of all edges.</p><p>For directed graph, the task is often to learn one embedding for a particular node <span class="math inline">\(v_i\)</span>, namely, <span class="math inline">\(\mathbf{u}_i\)</span>.</p><p>For undirected graph, the task is often to learn two embeddings for a particular node <span class="math inline">\(v_i\)</span>, the out-going (source) embedding <span class="math inline">\(\mathbf{s}_i\)</span>, and the in-going (destination) embedding <span class="math inline">\(\mathbf{t}_i\)</span>.</p><h2 id="objective-for-learning-the-embedding">Objective for Learning the Embedding</h2><h3 id="learning-embedding-that-represents-the-local-connectivity-homophily-of-nodes">Learning Embedding that Represents the Local Connectivity (homophily) of Nodes</h3><h4 id="undirected-graph">Undirected Graph</h4><p>For undirected graph, the connectivity from <span class="math inline">\(v_i\)</span> to <span class="math inline">\(v_j\)</span> can be defined as the probability of predicting <span class="math inline">\(v_j\)</span> given <span class="math inline">\(v_i\)</span>: <span class="math display">\[ p(v_j|v_i) = \frac{\exp(\mathbf{u}_i \cdot \mathbf{u}_j)}{\sum_{v_n \in G} \exp(\mathbf{u}_i \cdot \mathbf{u}_n)} \]</span> The dot product can be seen as a score to indicate the connectivity from <span class="math inline">\(v_i\)</span> to <span class="math inline">\(v_j\)</span>. The training objective is to maximize <span class="math inline">\(p(v_j|v_i)\)</span> for every <span class="math inline">\(v_j\)</span> among the neighbors of <span class="math inline">\(v_i\)</span> (i.e., <span class="math inline">\(N(v_i)\)</span>, and all the nodes <span class="math inline">\(v_i\)</span> in <span class="math inline">\(V\)</span>. <span class="math display">\[ \mathcal{L} = \mathbb{E}_{v_i \in V} \mathbb{E}_{v_j \in N(v_i)} \left[ \log p(v_j|v_i) \right] = \mathbb{E}_{v_i \in V} \mathbb{E}_{v_j \in N(v_i)} \left[ \log\frac{\exp(\mathbf{u}_i \cdot \mathbf{u}_j)}{\sum_{v_n \in G} \exp(\mathbf{u}_i \cdot \mathbf{u}_n)} \right] \]</span> The partition function of <span class="math inline">\(p(v_j|v_i)\)</span> is hard to evaluate. For learning an embedding, <a href="/Deep_Learning/Confronting_Partition_Function/Softmax_Speedup/#negative_sampling">negative sampling</a> could be adopted to train the above objective. If <span class="math inline">\(p(v_i)\)</span> and <span class="math inline">\(p(v_j|v_i)\)</span> are uniform, we have: <span class="math display">\[ \tilde{\mathcal{L}} = \sum_{v_i \in V} \sum_{v_j \in N(v_i)} \left[ \log \frac{\exp(\mathbf{u}_i \cdot \mathbf{u}_j)}{\exp(\mathbf{u}_i \cdot \mathbf{u}_j) + 1} + \sum_{\tilde{v}_n \sim q(v) \\ n = 1 \dots k} \log\frac{1}{\exp(\mathbf{u}_i \cdot \tilde{\mathbf{u}}_n) + 1} \right] \]</span> where <span class="math inline">\(q(v)\)</span> is a noise distribution that samples negative node samples for a given pair <span class="math inline">\((v_i,v_j)\)</span>. <span class="math inline">\(k\)</span> controls the number of negative samples for each pair <span class="math inline">\((v_i,v_j)\)</span>. The noise distribution is usually also uniform.</p><h4 id="directed-graph">Directed Graph</h4><p>In directed graph, the connectivity from <span class="math inline">\(v_i\)</span> to <span class="math inline">\(v_j\)</span> can be defined as the probability of predicting <span class="math inline">\(v_j\)</span> given <span class="math inline">\(v_i\)</span>: <span class="math display">\[ p(v_j|v_i) = \frac{\exp(\mathbf{s}_i \cdot \mathbf{t}_j)}{\sum_{v_n \in G} \exp(\mathbf{s}_i \cdot \mathbf{t}_n)} \]</span></p><h4 id="multi-relational-link">Multi-Relational Link</h4><p>A relational matrix <span class="math inline">\(\mathbf{R}_r, \, r \in R\)</span> can be used to replace the dot-product in homogeneous graphs, which brings the probability of predicting <span class="math inline">\(v_j\)</span> given <span class="math inline">\(v_i\)</span>:</p><p><span class="math display">\[ p(v_j|v_i,r) = \frac{\exp(\mathbf{s}_i^{\top} \mathbf{R}_r \mathbf{t}_j)}{\sum_{\tilde{r} \in R, v_n \in G} \exp(\mathbf{s}_i^{\top} \mathbf{R}_{\tilde{r}} \mathbf{t}_n)} \]</span></p><p>The negative-sampling based training objective then becomes:</p><p><span class="math display">\[ \tilde{\mathcal{L}} = \sum_{v_i \in V} \sum_{r \in R} \sum_{v_j \in N_r(v_i)} \left[ \log \frac{\exp(\mathbf{u}_i^{\top} \mathbf{R}_r \mathbf{u}_j)}{\exp(\mathbf{u}_i^{\top} \mathbf{R}_r \mathbf{u}_j) + 1} + \sum_{\tilde{r}_n \in R \\ \tilde{v}_n \sim q_{\tilde{r}_n}(v) \\ n = 1 \dots k} \log\frac{1}{\exp(\mathbf{u}_i^{\top} \mathbf{R}_{\tilde{r}_n} \tilde{\mathbf{u}}_n) + 1} \right] \]</span></p><p>where <span class="citation" data-cites="yangEmbeddingEntitiesRelations2014">Yang et al. (<a href="#ref-yangEmbeddingEntitiesRelations2014" role="doc-biblioref">2014</a>)</span> and <span class="citation" data-cites="schlichtkrullModelingRelationalData2017">Schlichtkrull et al. (<a href="#ref-schlichtkrullModelingRelationalData2017" role="doc-biblioref">2017</a>)</span> used diagonal <span class="math inline">\(\mathbf{R}_r\)</span> to reduce the parameters.</p><h2 id="random-walk">Random Walk</h2><ul><li>Monte-Carlo End-Point sampling method</li><li>Balancing DFS and BFS<ul><li><a href="http://go.ipwx.me/zotero/MPUCA3Q8" target="_blank" rel="noopener">node2vec: Scalable Feature Learning for Networks</a></li></ul></li><li>Path sharing techniques: for a sampled random-walk path <span class="math inline">\(v_1, \dots, v_L\)</span>, consider that<ul><li>Suffixes: <span class="math inline">\(v_i, \dots, v_L\)</span> are valid paths.</li><li>Prefixes: <span class="math inline">\(v_1, \dots, v_i\)</span> are valid paths. [Powerwalk: Scalable personalized pagerank via random walks with vertexcentric decomposition.]</li><li>Sliding windows: <span class="math inline">\(v_i, \dots, v_{i+W}\)</span> are valid paths. <span class="citation" data-cites="groverNode2vecScalableFeature2016">(Grover and Leskovec <a href="#ref-groverNode2vecScalableFeature2016" role="doc-biblioref">2016</a>)</span></li></ul></li></ul><h1 id="bibliography" class="unnumbered">References</h1><div id="refs" class="references" role="doc-bibliography"><div id="ref-groverNode2vecScalableFeature2016"><p>Grover, Aditya, and Jure Leskovec. 2016. “Node2vec: Scalable Feature Learning for Networks.” <em>arXiv:1607.00653 [Cs, Stat]</em>, July. <a href="http://arxiv.org/abs/1607.00653" target="_blank" rel="noopener">http://arxiv.org/abs/1607.00653</a>.</p></div><div id="ref-schlichtkrullModelingRelationalData2017"><p>Schlichtkrull, Michael, Thomas N. Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and Max Welling. 2017. “Modeling Relational Data with Graph Convolutional Networks.” <em>arXiv:1703.06103 [Cs, Stat]</em>, October. <a href="http://arxiv.org/abs/1703.06103" target="_blank" rel="noopener">http://arxiv.org/abs/1703.06103</a>.</p></div><div id="ref-yangEmbeddingEntitiesRelations2014"><p>Yang, Bishan, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. 2014. “Embedding Entities and Relations for Learning and Inference in Knowledge Bases.” <em>arXiv Preprint arXiv:1412.6575</em>.</p></div></div></div><div style="height:10px"></div></div></article><article id="post-Research_Work/Directions_to_Explore" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-inner"><header class="article-header"><div class="article-meta"><div class="article-category"><i class="fa fa-folder"></i> <a class="article-category-link" href="/categories/Research-Work/">Research Work</a></div><div class="article-date"><i class="fa fa-calendar"></i> <a href="/Research_Work/Directions_to_Explore/"><time datetime="2020-03-05T00:59:49.000Z" itemprop="datePublished">2020-03-05</time></a></div><div class="article-meta-button"><a href="https://github.com/haowen-xu/research-notes/raw/master/source/_posts/Research_Work/Directions_to_Explore.md">Source</a></div><div class="article-meta-button"><a href="https://github.com/haowen-xu/research-notes/edit/master/source/_posts/Research_Work/Directions_to_Explore.md">Edit</a></div><div class="article-meta-button"><a href="https://github.com/haowen-xu/research-notes/commits/master/source/_posts/Research_Work/Directions_to_Explore.md">History</a></div></div><h1 itemprop="name"><a class="article-title" href="/Research_Work/Directions_to_Explore/">Directions to Explore</a></h1></header><div class="article-entry" itemprop="articleBody"><p>Here's something encrypted, password is required to continue reading.</p></div><div class="article-more-link"><a href="/Research_Work/Directions_to_Explore/#more">Read More</a></div><div style="height:10px"></div></div></article><nav id="page-nav"><a class="extend prev" rel="prev" href="/page/4/">&laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><span class="page-number current">5</span><a class="page-number" href="/page/6/">6</a><a class="page-number" href="/page/7/">7</a><span class="space">&hellip;</span><a class="page-number" href="/page/14/">14</a><a class="extend next" rel="next" href="/page/6/">Next &raquo;</a></nav></section></div><footer id="footer"><div class="outer"><div id="footer-info" class="inner">Haowen Xu &copy; 2020 <a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/4.0/"><img alt="Creative Commons License" style="border-width:0" src="/images/by-nc-nd-4.0-80x15.png"></a><br>Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> &amp; theme <a href="https://github.com/zthxxx/hexo-theme-Wikitten">Wikitten</a></div></div></footer><script src="/libs/lightgallery/js/lightgallery.min.js"></script><script src="/libs/lightgallery/js/lg-thumbnail.min.js"></script><script src="/libs/lightgallery/js/lg-pager.min.js"></script><script src="/libs/lightgallery/js/lg-autoplay.min.js"></script><script src="/libs/lightgallery/js/lg-fullscreen.min.js"></script><script src="/libs/lightgallery/js/lg-zoom.min.js"></script><script src="/libs/lightgallery/js/lg-hash.min.js"></script><script src="/libs/lightgallery/js/lg-share.min.js"></script><script src="/libs/lightgallery/js/lg-video.min.js"></script><script src="/libs/justified-gallery/jquery.justifiedGallery.min.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true,
            TeX: {
                equationNumbers: {
                    autoNumber: 'AMS'
                },
                style: {
                    'font-family': 'serif'
                }
            }
        },
        'HTML-CSS': {
            //preferredFont: 'TeX',
            fonts: ['TeX']
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });</script><script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="/js/main.js"></script></div></body>