<!DOCTYPE html><html lang="en"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="utf-8"><title>Sequential VAE | My Research Wiki</title><meta name="keywords" content="Sequential VAE"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="description" content="Sequential VAEs model observed sequence \(\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_T\) using latent sequence \(\mathbf{z}_1, \mathbf{z}_2, \dots, \mathbf{z}_T\).In this article, we use \(\mathbf{"><meta property="og:type" content="article"><meta property="og:title" content="Sequential VAE"><meta property="og:url" content="https://wiki.haowen-xu.com/Deep_Learning/Variational_Autoencoder/Sequential_VAE/index.html"><meta property="og:site_name" content="My Research Wiki"><meta property="og:description" content="Sequential VAEs model observed sequence \(\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_T\) using latent sequence \(\mathbf{z}_1, \mathbf{z}_2, \dots, \mathbf{z}_T\).In this article, we use \(\mathbf{"><meta property="og:locale" content="en"><meta property="og:image" content="https://wiki.haowen-xu.com/Deep_Learning/Variational_Autoencoder/Sequential_VAE/hmm_model.png"><meta property="og:image" content="https://wiki.haowen-xu.com/Deep_Learning/Variational_Autoencoder/Sequential_VAE/hmm_d_separation.png"><meta property="og:image" content="https://wiki.haowen-xu.com/Deep_Learning/Variational_Autoencoder/Sequential_VAE/vrnn_original_arch.png"><meta property="og:image" content="https://wiki.haowen-xu.com/Deep_Learning/Variational_Autoencoder/Sequential_VAE/vrnn.png"><meta property="og:image" content="https://wiki.haowen-xu.com/Deep_Learning/Variational_Autoencoder/Sequential_VAE/srnn_original_arch.png"><meta property="og:updated_time" content="2020-06-01T12:15:59.012Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Sequential VAE"><meta name="twitter:description" content="Sequential VAEs model observed sequence \(\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_T\) using latent sequence \(\mathbf{z}_1, \mathbf{z}_2, \dots, \mathbf{z}_T\).In this article, we use \(\mathbf{"><meta name="twitter:image" content="https://wiki.haowen-xu.com/Deep_Learning/Variational_Autoencoder/Sequential_VAE/hmm_model.png"><link rel="alternate" href="/atom.xml" title="My Research Wiki" type="application/atom+xml"><link rel="icon" href="/favicon.ico"><link rel="stylesheet" href="/libs/font-awesome/css/font-awesome.min.css"><link rel="stylesheet" href="/libs/open-sans/styles.css"><link rel="stylesheet" href="/libs/source-code-pro/styles.css"><link rel="stylesheet" href="/css/style.css"><script src="/libs/jquery/2.1.3/jquery.min.js"></script><script src="/libs/jquery/plugins/cookie/1.4.1/jquery.cookie.js"></script><link rel="stylesheet" href="/libs/lightgallery/css/lightgallery.min.css"><link rel="stylesheet" href="/libs/justified-gallery/justifiedGallery.min.css"></head></html><body><div id="container"><header id="header"><div id="header-main" class="header-inner"><div class="outer"><a href="/" id="logo"><i class="logo"></i> <span class="site-title">My Research Wiki</span></a><nav id="main-nav"><a class="main-nav-link" href="/">Home</a> <a class="main-nav-link" href="/archives">Archives</a> <a class="main-nav-link" href="/categories">Categories</a></nav><div id="search-form-wrap"><form class="search-form"><input type="text" class="ins-search-input search-form-input" placeholder="Search"> <button type="submit" class="search-form-submit"></button></form><div class="ins-search"><div class="ins-search-mask"></div><div class="ins-search-container"><div class="ins-input-wrapper"><input type="text" class="ins-search-input" placeholder="Type something..."> <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span></div><div class="ins-section-wrapper"><div class="ins-section-container"></div></div></div></div><script>window.INSIGHT_CONFIG={TRANSLATION:{POSTS:"Posts",PAGES:"Pages",CATEGORIES:"Categories",TAGS:"Tags",UNTITLED:"(Untitled)"},ROOT_URL:"/",CONTENT_URL:"/content.json"}</script><script src="/js/insight.js"></script></div></div></div><div id="main-nav-mobile" class="header-sub header-inner"><table class="menu outer"><tr><td><a class="main-nav-link" href="/">Home</a></td><td><a class="main-nav-link" href="/archives">Archives</a></td><td><a class="main-nav-link" href="/categories">Categories</a></td><td><div class="search-form"><input type="text" class="ins-search-input search-form-input" placeholder="Search"></div></td></tr></table></div></header><div class="outer"><aside id="sidebar"><div class="widget-wrap" id="categories"><h3 class="widget-title"><span>categories</span> &nbsp; <a id="allExpand" href="#"><i class="fa fa-angle-double-down fa-2x"></i></a></h3><ul class="unstyled" id="tree"><li class="directory open"><a href="#" data-role="directory"><i class="fa fa-folder-open"></i> &nbsp; Deep Learning</a><ul class="unstyled" id="tree"><li class="directory"><a href="#" data-role="directory"><i class="fa fa-folder"></i> &nbsp; CV</a><ul class="unstyled" id="tree"><li class="file"><a href="/Deep_Learning/CV/Image_Segmentation/">Image Segmentation</a></li></ul></li><li class="directory"><a href="#" data-role="directory"><i class="fa fa-folder"></i> &nbsp; Confronting Partition Function</a><ul class="unstyled" id="tree"><li class="file"><a href="/Deep_Learning/Confronting_Partition_Function/Overview/">Overview</a></li><li class="file"><a href="/Deep_Learning/Confronting_Partition_Function/Contrastive_Divergence/">Contrastive Divergence</a></li><li class="file"><a href="/Deep_Learning/Confronting_Partition_Function/Score_Matching/">Score Matching</a></li><li class="file"><a href="/Deep_Learning/Confronting_Partition_Function/Softmax_Speedup/">Softmax Speedup</a></li></ul></li><li class="directory"><a href="#" data-role="directory"><i class="fa fa-folder"></i> &nbsp; Energy Based Models</a><ul class="unstyled" id="tree"><li class="file"><a href="/Deep_Learning/Energy_Based_Models/Overview/">Overview</a></li><li class="file"><a href="/Deep_Learning/Energy_Based_Models/Energy_Function_in_Probabilistic_Models/">Energy Function in Probabilistic Models</a></li><li class="file"><a href="/Deep_Learning/Energy_Based_Models/Restricted_Boltzmann_Machine/">Restricted Boltzmann Machine</a></li></ul></li><li class="directory"><a href="#" data-role="directory"><i class="fa fa-folder"></i> &nbsp; Evaluation</a><ul class="unstyled" id="tree"><li class="file"><a href="/Deep_Learning/Evaluation/Evaluation_Metrics/">Evaluation Metrics</a></li><li class="file"><a href="/Deep_Learning/Evaluation/Visualizing_High_Dimensional_Space/">Visualizing High Dimensional Space</a></li></ul></li><li class="directory"><a href="#" data-role="directory"><i class="fa fa-folder"></i> &nbsp; Generative Adversarial Nets</a><ul class="unstyled" id="tree"><li class="file"><a href="/Deep_Learning/Generative_Adversarial_Nets/Overview/">Overview</a></li><li class="file"><a href="/Deep_Learning/Generative_Adversarial_Nets/f-GAN/">f-GAN</a></li><li class="file"><a href="/Deep_Learning/Generative_Adversarial_Nets/Energy_GAN/">Energy GAN</a></li></ul></li><li class="directory"><a href="#" data-role="directory"><i class="fa fa-folder"></i> &nbsp; Graph Neural Networks</a><ul class="unstyled" id="tree"><li class="file"><a href="/Deep_Learning/Graph_Neural_Networks/Node_Embedding/">Node Embedding</a></li><li class="file"><a href="/Deep_Learning/Graph_Neural_Networks/Graph_Convolution_Network/">Graph Convolution Network</a></li><li class="file"><a href="/Deep_Learning/Graph_Neural_Networks/Graph_Auto_Encoder/">Graph Auto-Encoder</a></li></ul></li><li class="directory"><a href="#" data-role="directory"><i class="fa fa-folder"></i> &nbsp; Information Theoretical</a><ul class="unstyled" id="tree"><li class="file"><a href="/Deep_Learning/Information_Theoretical/KL-Divergence/">KL-Divergence</a></li><li class="file"><a href="/Deep_Learning/Information_Theoretical/Mutual_Information/">Mutual Information</a></li></ul></li><li class="directory"><a href="#" data-role="directory"><i class="fa fa-folder"></i> &nbsp; Monte Carlo Methods</a><ul class="unstyled" id="tree"><li class="file"><a href="/Deep_Learning/Monte_Carlo_Methods/Overview/">Overview</a></li><li class="file"><a href="/Deep_Learning/Monte_Carlo_Methods/Monte_Carlo_Integration/">Monte Carlo Integration</a></li><li class="file"><a href="/Deep_Learning/Monte_Carlo_Methods/Accept_Reject_Sampling/">Accept-Reject Sampling</a></li><li class="file"><a href="/Deep_Learning/Monte_Carlo_Methods/Markov_Chain/">Markov Chain</a></li><li class="file"><a href="/Deep_Learning/Monte_Carlo_Methods/Metropolis_Hastings_Algorithm/">Metropolis-Hastings Algorithm</a></li><li class="file"><a href="/Deep_Learning/Monte_Carlo_Methods/Gibbs_Sampler/">Gibbs Sampler</a></li><li class="file"><a href="/Deep_Learning/Monte_Carlo_Methods/Hamiltonian_Dynamics/">Hamiltonian Dynamics</a></li><li class="file"><a href="/Deep_Learning/Monte_Carlo_Methods/Langevin_Dynamics/">Langevin Dynamics</a></li></ul></li><li class="directory"><a href="#" data-role="directory"><i class="fa fa-folder"></i> &nbsp; Optimization</a><ul class="unstyled" id="tree"><li class="file"><a href="/Deep_Learning/Optimization/Gradient_Tricks/">Gradient Tricks</a></li><li class="file"><a href="/Deep_Learning/Optimization/Loss_Surface_and_Generalization/">Loss Surface and Generalization</a></li><li class="file"><a href="/Deep_Learning/Optimization/Stochastic_Gradient_Descent/">Stochastic Gradient descent</a></li></ul></li><li class="directory open"><a href="#" data-role="directory"><i class="fa fa-folder-open"></i> &nbsp; Variational Autoencoder</a><ul class="unstyled" id="tree"><li class="file"><a href="/Deep_Learning/Variational_Autoencoder/Overview/">Overview</a></li><li class="file active"><a href="/Deep_Learning/Variational_Autoencoder/Sequential_VAE/">Sequential VAE</a></li><li class="file"><a href="/Deep_Learning/Variational_Autoencoder/Gradient_Estimators_for_Variational_Inference/">Gradient Estimators for Variational Inference</a></li><li class="file"><a href="/Deep_Learning/Variational_Autoencoder/Theoretical_Facts/">Theoretical Facts about VAEs</a></li></ul></li></ul></li><li class="directory"><a href="#" data-role="directory"><i class="fa fa-folder"></i> &nbsp; Mathematics</a><ul class="unstyled" id="tree"><li class="directory"><a href="#" data-role="directory"><i class="fa fa-folder"></i> &nbsp; Analysis</a><ul class="unstyled" id="tree"><li class="file"><a href="/Mathematics/Analysis/Linear_Space_vs_Functional_Space/">Linear space vs functional space</a></li></ul></li><li class="directory"><a href="#" data-role="directory"><i class="fa fa-folder"></i> &nbsp; Calculus</a><ul class="unstyled" id="tree"><li class="file"><a href="/Mathematics/Calculus/Calculus_of_Variations/">Calculus of Variations</a></li></ul></li><li class="directory"><a href="#" data-role="directory"><i class="fa fa-folder"></i> &nbsp; Differential Equations</a><ul class="unstyled" id="tree"><li class="file"><a href="/Mathematics/Differential_Equations/Probability_Distribution_Equations/">Differential Equations on Probability Distributions</a></li></ul></li><li class="directory"><a href="#" data-role="directory"><i class="fa fa-folder"></i> &nbsp; Optimization</a><ul class="unstyled" id="tree"><li class="file"><a href="/Mathematics/Optimization/Convex_Optimization/">Convex Optimization</a></li></ul></li></ul></li><li class="directory"><a href="#" data-role="directory"><i class="fa fa-folder"></i> &nbsp; Research Work</a><ul class="unstyled" id="tree"><li class="file"><a href="/Research_Work/Reading_List/">Reading List</a></li><li class="file"><a href="/Research_Work/Tracking_the_Concepts/">Tracking the Concepts</a></li><li class="file"><a href="/Research_Work/Directions_to_Explore/">Directions to Explore</a></li></ul></li></ul></div><script>$(document).ready(function(){var r="fa-folder-open",i="fa-folder",l="fa-angle-double-down",d="fa-angle-double-up";$(document).on("click",'#categories a[data-role="directory"]',function(a){a.preventDefault();var e=$(this).children(".fa"),s=e.hasClass(r),l=$(this).siblings("ul");e.removeClass(r).removeClass(i),s?(void 0!==l&&l.slideUp({duration:100}),e.addClass(i)):(void 0!==l&&l.slideDown({duration:100}),e.addClass(r))}),$('#categories a[data-role="directory"]').bind("contextmenu",function(a){a.preventDefault();var e=$(this).children(".fa"),s=e.hasClass(r),l=$(this).siblings("ul"),d=$.merge(l.find("li ul"),l),o=$.merge(l.find(".fa"),e);o.removeClass(r).removeClass(i),s?(d.slideUp({duration:100}),o.addClass(i)):(d.slideDown({duration:100}),o.addClass(r))}),$(document).on("click","#allExpand",function(a){a.preventDefault();var e=$(this).children(".fa"),s=e.hasClass(l);e.removeClass(l).removeClass(d),s?($("#sidebar .fa.fa-folder").removeClass("fa-folder").addClass("fa-folder-open"),$("#categories li ul").slideDown({duration:100}),e.addClass(d)):($("#sidebar .fa.fa-folder-open").removeClass("fa-folder-open").addClass("fa-folder"),$("#categories li ul").slideUp({duration:100}),e.addClass(l))})})</script><div id="toTop" class="fa fa-angle-up"></div></aside><section id="main"><article id="post-Deep_Learning/Variational_Autoencoder/Sequential_VAE" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-inner"><header class="article-header"><div class="article-meta"><div class="article-category"><i class="fa fa-folder"></i> <a class="article-category-link" href="/categories/Deep-Learning/">Deep Learning</a><i class="fa fa-angle-right"></i><a class="article-category-link" href="/categories/Deep-Learning/Variational-Autoencoder/">Variational Autoencoder</a></div><div class="article-date"><i class="fa fa-calendar"></i> <a href="/Deep_Learning/Variational_Autoencoder/Sequential_VAE/"><time datetime="2019-11-04T06:52:00.000Z" itemprop="datePublished">2019-11-04</time></a></div><div class="article-meta-button"><a href="https://github.com/haowen-xu/research-notes/raw/master/source/_posts/Deep_Learning/Variational_Autoencoder/Sequential_VAE.md">Source</a></div><div class="article-meta-button"><a href="https://github.com/haowen-xu/research-notes/edit/master/source/_posts/Deep_Learning/Variational_Autoencoder/Sequential_VAE.md">Edit</a></div><div class="article-meta-button"><a href="https://github.com/haowen-xu/research-notes/commits/master/source/_posts/Deep_Learning/Variational_Autoencoder/Sequential_VAE.md">History</a></div></div><h1 class="article-title" itemprop="name">Sequential VAE</h1></header><div class="article-entry" itemprop="articleBody"><p>Sequential VAEs model observed sequence <span class="math inline">\(\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_T\)</span> using latent sequence <span class="math inline">\(\mathbf{z}_1, \mathbf{z}_2, \dots, \mathbf{z}_T\)</span>.</p><p>In this article, we use <span class="math inline">\(\mathbf{x}_{1:T}\)</span> and <span class="math inline">\(\mathbf{z}_{1:T}\)</span> to denote <span class="math inline">\(\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_T\)</span> and <span class="math inline">\(\mathbf{z}_1, \mathbf{z}_2, \dots, \mathbf{z}_T\)</span>, respectively. Also, we use <span class="math inline">\(\mathbf{x}_{\neg t}\)</span> to denote <span class="math inline">\(\mathbf{x}_1,\dots,\mathbf{x}_{t-1},\mathbf{x}_{t+1},\dots,\mathbf{x}_T\)</span>, and <span class="math inline">\(\mathbf{z}_{\neg t}\)</span> likewise.</p><h2 id="the-future-dependency-problem">The Future Dependency Problem</h2><p>In this section, we shall discuss the future dependency problem of <span class="math inline">\(\mathbf{z}_t\)</span> on <span class="math inline">\(\mathbf{x}_{t:T}\)</span> in the variational distribution <span class="math inline">\(q_{\phi}(\mathbf{z}_{1:T}|\mathbf{x}_{1:T})\)</span>, via a simple hidden state markov chain model.</p><p>Suppose <span class="math inline">\(\mathbf{z}_{1:T}\)</span> is a Markov chain, and serves as a sequence of hidden states that determines the <span class="math inline">\(\mathbf{x}_{1:T}\)</span> sequence. Formally, the probabilistic formulation of such a model can be written as: <span class="math display">\[ \begin{align} p_{\lambda}(\mathbf{z}_{1:T}) &amp;= \prod_{t=1}^T p_{\lambda}(\mathbf{z}_t|\mathbf{z}_{t-1}) \\ p_{\theta}(\mathbf{x}_{1:T}|\mathbf{z}_{1:T}) &amp;= \prod_{t=1}^T p_{\theta}(\mathbf{x}_t|\mathbf{z}_t) \end{align} \]</span> This formulation can also be visualized as the following Bayesian network diagram:</p><figure><img src="/Deep_Learning/Variational_Autoencoder/Sequential_VAE/hmm_model.png" id="fig:hidden-state-markov-chain-model" data-max-height="120px" alt="" style="max-height:120px"><figcaption>Figure 1: Hidden State Markov Chain Model</figcaption></figure><p>Surprisingly, the posterior distribution, <em>i.e.</em>, <span class="math inline">\(p_{\theta}(\mathbf{z}_{1:T}|\mathbf{x}_{1:T})\)</span>, exhibits <em>future dependency</em>, as is noticed by <span class="citation" data-cites="fraccaroSequentialNeuralModels2016">Fraccaro et al. (<a href="#ref-fraccaroSequentialNeuralModels2016" role="doc-biblioref">2016</a>)</span>. Using <span class="dangling-link">d-separation</span>, one can easily figure out that: <span class="math display">\[ p_{\theta}(\mathbf{z}_{1:T}|\mathbf{x}_{1:T}) = \prod_{t=1}^T p_{\theta}(\mathbf{z}_t|\mathbf{z}_{t-1}, \mathbf{x}_{t:T}) \]</span> Such independence relationship is also illustrated in the following diagram. Clearly, the observation of <span class="math inline">\(\mathbf{z}_{t-1}\)</span> blocks the dependency of <span class="math inline">\(\mathbf{z}_t\)</span> on <span class="math inline">\(\mathbf{x}_{1:(t-1)}\)</span> and <span class="math inline">\(\mathbf{z}_{1:(t-1)}\)</span>, however, does not block its dependence on the future <span class="math inline">\(\mathbf{x}_{(t+1):T}\)</span>.</p><figure><img src="/Deep_Learning/Variational_Autoencoder/Sequential_VAE/hmm_d_separation.png" id="fig:hmm-d-separation" data-max-height="120px" alt="" style="max-height:120px"><figcaption>Figure 2: <em>d-separation</em> illustration for <span class="math inline">\(\mathbf{z}_t\)</span></figcaption></figure><p>The future dependency seems counter-intuitive at the first glance. However, this dependency can be naturally explained in the information theoretic perspective. Since the sequence <span class="math inline">\(\mathbf{x}_{t:T}\)</span> is generated with the influence of <span class="math inline">\(\mathbf{z}_t\)</span>, <em>i.e.</em>, the information of <span class="math inline">\(\mathbf{z}_t\)</span> flows into <span class="math inline">\(\mathbf{x}_{t:T}\)</span>, then it should be not suprising that knowing the information of <span class="math inline">\(\mathbf{x}_{t:T}\)</span> is helpful to infer <span class="math inline">\(\mathbf{z}_t\)</span>.</p><h2 id="vrnn">VRNN</h2><p><span class="citation" data-cites="chungRecurrentLatentVariable2015">Chung et al. (<a href="#ref-chungRecurrentLatentVariable2015" role="doc-biblioref">2015</a>)</span> proposed to embed a variational autoencoder into each step of an LSTM or GRU recurrent network, formalized as: <span class="math display">\[ \begin{align} p_{\lambda}(\mathbf{z}_t|\mathbf{h}_{t-1}) &amp;= \mathcal{N}(\mathbf{z}_t|\boldsymbol{\mu}_{\lambda,t}, \boldsymbol{\sigma}_{\lambda,t}^2) \\ p_{\theta}(\mathbf{x}_t|\mathbf{z}_t,\mathbf{h}_{t-1}) &amp;= \mathcal{N}(\mathbf{x}_t| \boldsymbol{\mu}_{\theta,t}, \boldsymbol{\sigma}_{\theta,t}^2) \\ q_{\phi}(\mathbf{z}_t|\mathbf{x}_t,\mathbf{h}_{t-1}) &amp;= \mathcal{N}(\mathbf{z}_t| \boldsymbol{\mu}_{\phi,t}, \boldsymbol{\sigma}_{\phi,t}^2) \\ \mathbf{h}_t &amp;= f_{\text{rnn}}(f_{\mathbf{x}}(\mathbf{x}_t),f_{\mathbf{z}}(\mathbf{z}_t),\mathbf{h}_{t-1}) \\ [\boldsymbol{\mu}_{\lambda,t}, \boldsymbol{\sigma}_{\lambda,t}] &amp;= f_{\text{prior}}(\mathbf{h}_{t-1}) \\ [\boldsymbol{\mu}_{\theta,t}, \boldsymbol{\sigma}_{\theta,t}] &amp;= f_{\text{dec}}(f_{\mathbf{z}}(\mathbf{z}_t), \mathbf{h}_{t-1}) \\ [\boldsymbol{\mu}_{\phi,t}, \boldsymbol{\sigma}_{\phi,t}] &amp;= f_{\text{enc}}(f_{\mathbf{x}}(\mathbf{x}_t), \mathbf{h}_{t-1}) \end{align} \]</span> where <span class="math inline">\(f_{\mathbf{z}}\)</span> and <span class="math inline">\(f_{\mathbf{x}}\)</span> are feature networks shared among the prior, the encoder and the decoder, which are “crucial for learning complex sequences” according to <span class="citation" data-cites="chungRecurrentLatentVariable2015">Chung et al. (<a href="#ref-chungRecurrentLatentVariable2015" role="doc-biblioref">2015</a>)</span>. <span class="math inline">\(f_{\text{rnn}}\)</span> is the transition kernel of the recurrent network, and <span class="math inline">\(\mathbf{h}_{1:T}\)</span> are the deterministic hidden states of the recurrent network. <span class="math inline">\(f_{\text{prior}}\)</span>, <span class="math inline">\(f_{\text{enc}}\)</span> and <span class="math inline">\(f_{\text{dec}}\)</span> are the neural networks in the prior, the encoder and the decoder, respectively.</p><p>The overall architecture of VRNN can be illustrated as the following figure, given by <span class="citation" data-cites="chungRecurrentLatentVariable2015">Chung et al. (<a href="#ref-chungRecurrentLatentVariable2015" role="doc-biblioref">2015</a>)</span>:</p><figure><img src="/Deep_Learning/Variational_Autoencoder/Sequential_VAE/vrnn_original_arch.png" id="fig:vrnn-original-arch" data-max-height="240px" alt="" style="max-height:240px"><figcaption>Figure 3: Overall architecture of VRNN</figcaption></figure><p>The following figure may provide a more clear illustration of the dependency among <span class="math inline">\(\mathbf{h}_{1:T}\)</span>, <span class="math inline">\(\mathbf{x}_{1:T}\)</span> and <span class="math inline">\(\mathbf{z}_{1:T}\)</span> in the generative part:</p><figure><img src="/Deep_Learning/Variational_Autoencoder/Sequential_VAE/vrnn.png" id="fig:vrnn" data-max-height="192px" alt="" style="max-height:192px"><figcaption>Figure 4: Dependency graph of VRNN in the generative part</figcaption></figure><p>The authors did not provide a theoretical analysis of the dependency relationship in their variational posterior <span class="math inline">\(q_{\phi}(\mathbf{z}_{1:T}|\mathbf{x}_{1:T})\)</span>, but according to <span class="dangling-link">d-separation</span>, we can easily figure out the correct dependency for the true posterior should be: <span class="math display">\[ p_{\theta}(\mathbf{z}_t|\mathbf{z}_{1:(t-1)},\mathbf{x}_{1:T},\mathbf{h}_{1:T}) = p_{\theta}(\mathbf{z}_t|\mathbf{h}_{t-1},\mathbf{x}_t,\mathbf{h}_t) \]</span> The dependency of <span class="math inline">\(\mathbf{z}_t\)</span> on future state <span class="math inline">\(\mathbf{h}_t\)</span> brings trouble for posterior inference. <span class="citation" data-cites="chungRecurrentLatentVariable2015">Chung et al. (<a href="#ref-chungRecurrentLatentVariable2015" role="doc-biblioref">2015</a>)</span> simply neglected this dependency. On the other hand, <span class="citation" data-cites="fraccaroSequentialNeuralModels2016">Fraccaro et al. (<a href="#ref-fraccaroSequentialNeuralModels2016" role="doc-biblioref">2016</a>)</span> considered such dependency and proposed SRNN, which brought us to a theoretically more reasonable factorization.</p><h3 id="srnn">SRNN</h3><p><span class="citation" data-cites="fraccaroSequentialNeuralModels2016">Fraccaro et al. (<a href="#ref-fraccaroSequentialNeuralModels2016" role="doc-biblioref">2016</a>)</span> proposed to factorize <span class="math inline">\(\mathbf{z}_{1:T}\)</span> as a state-space machine, depending on deterministic hidden state <span class="math inline">\(\mathbf{h}_{1:T}\)</span> of a recurrent network, and potentially the input <span class="math inline">\(\mathbf{u}_{1:T}\)</span> of each time step. The observation <span class="math inline">\(\mathbf{x}_t\)</span> of each time step is then assumed to depend only on <span class="math inline">\(\mathbf{d}_t\)</span> and <span class="math inline">\(\mathbf{z}_t\)</span>. The overall architecture of SRNN is illustrated in the following figure <span class="citation" data-cites="fraccaroSequentialNeuralModels2016">(Fraccaro et al. <a href="#ref-fraccaroSequentialNeuralModels2016" role="doc-biblioref">2016</a>)</span>:</p><figure><img src="/Deep_Learning/Variational_Autoencoder/Sequential_VAE/srnn_original_arch.png" id="fig:srnn-original-arch" data-max-height="320px" alt="" style="max-height:320px"><figcaption>Figure 5: Overall architecture of SRNN</figcaption></figure><h4 id="generative-part">Generative Part</h4><p>The initial state is chosen to be <span class="math inline">\(\mathbf{z}_0 = \mathbf{0}\)</span> and <span class="math inline">\(\mathbf{d}_0 = \mathbf{0}\)</span>. According to <span class="dangling-link">d-separation</span>, the generative part is formulated as: <span class="math display">\[ \begin{align} p_{\lambda}(\mathbf{z}_{1:T},\mathbf{d}_{1:T}|\mathbf{u}_{1:T},\mathbf{z}_0,\mathbf{d}_0) &amp;= \prod_{t=1}^T p_{\lambda}(\mathbf{z}_t|\mathbf{z}_{t-1},\mathbf{d}_t) \, p_{\lambda}(\mathbf{d}_t|\mathbf{d}_{t-1},\mathbf{u}_t) \\ p_{\theta}(\mathbf{x}_{1:T}|\mathbf{z}_{1:T},\mathbf{d}_{1:T},\mathbf{u}_{1:T},\mathbf{z}_0,\mathbf{d}_0) &amp;= \prod_{t=1}^T p_{\theta}(\mathbf{x}_t|\mathbf{z}_t,\mathbf{d}_t) \end{align} \]</span></p><p><span class="math inline">\(p_{\lambda}(\mathbf{d}_t|\mathbf{d}_{t-1},\mathbf{u}_t)\)</span> is a dirac distribution, derived by <span class="math inline">\(\text{RNN}^{(p)}\)</span>, a recurrent network: <span class="math display">\[ \begin{align} p_{\lambda}(\mathbf{d}_t|\mathbf{d}_{d-1},\mathbf{u}_t) &amp;= \delta(\mathbf{d}_t-\widetilde{\mathbf{d}}_t) \\ \widetilde{\mathbf{d}}_t &amp;= \text{RNN}^{(p)}(\mathbf{d}_{t-1}, \mathbf{u}_t) \end{align} \]</span> <span class="math inline">\(p_{\lambda}(\mathbf{z}_t|\mathbf{z}_{t-1},\mathbf{d}_t)\)</span> is a <span class="dangling-link">state-space machine</span>, given by: <span class="math display">\[ \begin{align} p_{\lambda}(\mathbf{z}_t|\mathbf{z}_{t-1},\mathbf{d}_t) &amp;= \mathcal{N}(\mathbf{z}_t| \boldsymbol{\mu}_{\lambda}(\mathbf{z}_{t-1},\mathbf{d}_t), \boldsymbol{\sigma}^2_{\lambda}(\mathbf{z}_{t-1},\mathbf{d}_t)) \\ \boldsymbol{\mu}_{\lambda}(\mathbf{z}_{t-1},\mathbf{d}_t) &amp;= \text{NN}^{(p)}_1(\mathbf{z}_{t-1},\mathbf{d}_t) \\ \log \boldsymbol{\sigma}_{\lambda}(\mathbf{z}_{t-1},\mathbf{d}_t) &amp;= \text{NN}^{(p)}_2(\mathbf{z}_{t-1},\mathbf{d}_t) \end{align} \]</span></p><p><span class="math inline">\(p_{\theta}(\mathbf{x}_t|\mathbf{z}_t, \mathbf{d}_t)\)</span> is derived by: <span class="math display">\[ p_{\theta}(\mathbf{x}_t|\mathbf{z}_t,\mathbf{d}_t) = \mathcal{N}(\mathbf{x}_t| \boldsymbol{\mu}_{\theta}(\mathbf{z}_t,\mathbf{d}_t), \boldsymbol{\sigma}^2_{\theta}(\mathbf{z}_t,\mathbf{d}_t)) \]</span></p><p>where <span class="math inline">\(\boldsymbol{\mu}_{\theta}(\mathbf{z}_t,\mathbf{d}_t)\)</span> and <span class="math inline">\(\boldsymbol{\sigma}_{\theta}(\mathbf{z}_t,\mathbf{d}_t)\)</span> use similar parameterization as in <span class="math inline">\(p_{\lambda}(\mathbf{z}_t|\mathbf{z}_{t-1},\mathbf{d}_t)\)</span>.</p><h4 id="variational-part">Variational Part</h4><p>The variational approximated posterior can be factorized as: <span class="math display">\[ \begin{align} q_{\phi}(\mathbf{z}_{1:T},\mathbf{d}_{1:T}|\mathbf{x}_{1:T},\mathbf{u}_{1:T},\mathbf{d}_0,\mathbf{u}_0) &amp;= q_{\phi}(\mathbf{z}_{1:T}|\mathbf{d}_{1:T},\mathbf{x}_{1:T},\mathbf{u}_{1:T},\mathbf{d}_0,\mathbf{u}_0)\, q_{\phi}(\mathbf{d}_{1:T}|\mathbf{x}_{1:T},\mathbf{u}_{1:T},\mathbf{d}_0,\mathbf{u}_0) \end{align} \]</span></p><p>Since in the generative part, <span class="math inline">\(p_{\lambda}(\mathbf{d}_t|\mathbf{d}_{t-1},\mathbf{u}_t)\)</span> is a dirac distribution, <span class="citation" data-cites="fraccaroSequentialNeuralModels2016">Fraccaro et al. (<a href="#ref-fraccaroSequentialNeuralModels2016" role="doc-biblioref">2016</a>)</span> decided to assume the second term in the above equation to be: <span class="math display">\[ q_{\phi}(\mathbf{d}_{1:T}|\mathbf{x}_{1:T},\mathbf{u}_{1:T},\mathbf{d}_0,\mathbf{u}_0) \equiv p_{\lambda}(\mathbf{d}_{1:T}|\mathbf{u}_{1:T},\mathbf{d}_0,\mathbf{u}_0) = \prod_{t=1}^T p_{\lambda}(\mathbf{d}_t|\mathbf{d}_{t-1},\mathbf{u}_t) \]</span> That is, the same recurrent network <span class="math inline">\(\text{RNN}^{(p)}\)</span> is used to produce the deterministic states <span class="math inline">\(\mathbf{d}_{1:T}\)</span> in both the generative part and variational part.</p><p>The first term is factorized according to <span class="dangling-link">d-separation</span>, as: <span class="math display">\[ q_{\phi}(\mathbf{z}_{1:T}|\mathbf{d}_{1:T},\mathbf{x}_{1:T},\mathbf{u}_{1:T},\mathbf{d}_0,\mathbf{u}_0) = \prod_{t=1}^T q_{\phi}(\mathbf{z}_t|\mathbf{z}_{t-1},\mathbf{d}_{t:T},\mathbf{x}_{t:T}) \]</span> where <span class="math inline">\(q_{\phi}(\mathbf{z}_t|\mathbf{z}_{t-1},\mathbf{d}_{t:T},\mathbf{x}_{t:T})\)</span> is derived by a reverse recurrent network <span class="math inline">\(\text{RNN}^{(q)}\)</span>, whose hidden state was denoted as <span class="math inline">\(\mathbf{a}_t\)</span>, as illustrated in Fig.&nbsp;<a href="#fig:srnn-original-arch">5</a>. The formalization of <span class="math inline">\(q_{\phi}(\mathbf{z}_t|\mathbf{z}_{t-1},\mathbf{d}_{t:T},\mathbf{x}_{t:T})\)</span> is: <span class="math display">\[ \begin{align} q_{\phi}(\mathbf{z}_t|\mathbf{z}_{t-1},\mathbf{d}_{t:T},\mathbf{x}_{t:T}) &amp;= q_{\phi}(\mathbf{z}_t|\mathbf{z}_{t-1},\mathbf{a}_t) \\ q_{\phi}(\mathbf{z}_t|\mathbf{z}_{t-1},\mathbf{a}_t) &amp;= \mathcal{N}(\mathbf{z}_t| \boldsymbol{\mu}_{\phi}(\mathbf{z}_{t-1},\mathbf{a}_t), \boldsymbol{\sigma}_{\phi}^2(\mathbf{z}_{t-1},\mathbf{a}_t)) \\ \boldsymbol{\mu}_{\phi}(\mathbf{z}_{t-1},\mathbf{a}_t) &amp;= \boldsymbol{\mu}_{\lambda}(\mathbf{z}_{t-1},\mathbf{d}_t) + \text{NN}^{(q)}_1(\mathbf{z}_{t-1},\mathbf{a}_t) \\ \log \boldsymbol{\sigma}_{\phi}(\mathbf{z}_{t-1},\mathbf{a}_t) &amp;= \text{NN}^{(q)}_2(\mathbf{z}_{t-1},\mathbf{a}_t) \\ \mathbf{a}_t &amp;= \text{RNN}^{(q)}(\mathbf{a}_{t+1},[\mathbf{d}_t,\mathbf{x}_t]) \end{align} \]</span> Notice <span class="math inline">\(\text{NN}^{(q)}_1(\mathbf{z}_{t-1},\mathbf{a}_t)\)</span> is adopted to learn the residual <span class="math inline">\(\boldsymbol{\mu}_{\phi}(\mathbf{z}_{t-1},\mathbf{a}_t) - \boldsymbol{\mu}_{\lambda}(\mathbf{z}_{t-1},\mathbf{d}_t)\)</span>, instead of <span class="math inline">\(\boldsymbol{\mu}_{\phi}(\mathbf{z}_{t-1},\mathbf{a}_t)\)</span> directly. <span class="citation" data-cites="fraccaroSequentialNeuralModels2016">Fraccaro et al. (<a href="#ref-fraccaroSequentialNeuralModels2016" role="doc-biblioref">2016</a>)</span> found that this residual parameterization can lead to better performance.</p><h1 id="bibliography" class="unnumbered">References</h1><div id="refs" class="references" role="doc-bibliography"><div id="ref-chungRecurrentLatentVariable2015"><p>Chung, Junyoung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron C. Courville, and Yoshua Bengio. 2015. “A Recurrent Latent Variable Model for Sequential Data.” In <em>Advances in Neural Information Processing Systems</em>, 2980–8.</p></div><div id="ref-fraccaroSequentialNeuralModels2016"><p>Fraccaro, Marco, Søren Kaae Sønderby, Ulrich Paquet, and Ole Winther. 2016. “Sequential Neural Models with Stochastic Layers.” In <em>Advances in Neural Information Processing Systems</em>, 2199–2207.</p></div></div></div><div style="height:10px"></div></div></article><nav id="article-nav"><a href="/Research_Work/Reading_List/" id="article-nav-newer" class="article-nav-link-wrap"><strong class="article-nav-caption">Newer</strong><div class="article-nav-title">Reading List</div></a><a href="/Deep_Learning/Variational_Autoencoder/Overview/" id="article-nav-older" class="article-nav-link-wrap"><strong class="article-nav-caption">Older</strong><div class="article-nav-title">Overview</div></a></nav></section></div><footer id="footer"><div class="outer"><div id="footer-info" class="inner">Haowen Xu &copy; 2020 <a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/4.0/"><img alt="Creative Commons License" style="border-width:0" src="/images/by-nc-nd-4.0-80x15.png"></a><br>Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> &amp; theme <a href="https://github.com/zthxxx/hexo-theme-Wikitten">Wikitten</a></div></div></footer><script src="/libs/lightgallery/js/lightgallery.min.js"></script><script src="/libs/lightgallery/js/lg-thumbnail.min.js"></script><script src="/libs/lightgallery/js/lg-pager.min.js"></script><script src="/libs/lightgallery/js/lg-autoplay.min.js"></script><script src="/libs/lightgallery/js/lg-fullscreen.min.js"></script><script src="/libs/lightgallery/js/lg-zoom.min.js"></script><script src="/libs/lightgallery/js/lg-hash.min.js"></script><script src="/libs/lightgallery/js/lg-share.min.js"></script><script src="/libs/lightgallery/js/lg-video.min.js"></script><script src="/libs/justified-gallery/jquery.justifiedGallery.min.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true,
            TeX: {
                equationNumbers: {
                    autoNumber: 'AMS'
                },
                style: {
                    'font-family': 'serif'
                }
            }
        },
        'HTML-CSS': {
            //preferredFont: 'TeX',
            fonts: ['TeX']
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });</script><script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="/js/main.js"></script></div></body>