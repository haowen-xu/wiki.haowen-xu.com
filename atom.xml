<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>My Research Wiki</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://wiki.haowen-xu.com/"/>
  <updated>2020-06-01T12:15:59.012Z</updated>
  <id>https://wiki.haowen-xu.com/</id>
  
  <author>
    <name>Haowen Xu</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Stochastic Gradient descent</title>
    <link href="https://wiki.haowen-xu.com/Deep_Learning/Optimization/Stochastic_Gradient_Descent/"/>
    <id>https://wiki.haowen-xu.com/Deep_Learning/Optimization/Stochastic_Gradient_Descent/</id>
    <published>2020-05-29T17:53:26.000Z</published>
    <updated>2020-06-01T12:15:59.012Z</updated>
    
    <content type="html"><![CDATA[<h2 id="first-order-methods">First-Order Methods</h2><p>For simplicity, we first introduce the following common notations:</p><ul><li><span class="math inline">\(J(\theta)\)</span>: The objective function, which should be minimized <em>w.r.t.</em> <span class="math inline">\(\theta\)</span>.</li><li><span class="math inline">\(g(\theta)\)</span>: The gradient of <span class="math inline">\(J(\theta)\)</span>, i.e., <span class="math inline">\(g(\theta) = \nabla J(\theta)\)</span>.</li><li><span class="math inline">\(g_t\)</span>: The abbrevation for <span class="math inline">\(g(\theta_t)\)</span>.</li></ul><p>One key difference between this article and that of <span class="citation" data-cites="OverviewGradientDescent2016">(“An Overview of Gradient Descent Optimization Algorithms” <a href="#ref-OverviewGradientDescent2016" role="doc-biblioref">2016</a>)</span> is that, <span class="math inline">\(\eta\)</span> is applied on the whole delta when updating the parameters <span class="math inline">\(\theta_t\)</span>, including the momentum term. This is because, practically, one may expect <span class="math inline">\(\eta\)</span> to be served as a direct constraint of how much the parameters <span class="math inline">\(\theta_t\)</span> are updated within each training step.</p><h3 id="naive-stochastic-gradient-descent">(Naive) Stochastic Gradient Descent</h3><p><span class="math display">\[ \theta_{t+1} = \theta_t - \eta \, g_t \]</span></p><h3 id="momentum-sgd">Momentum SGD</h3><p><span class="math display">\[ \begin{align} v_t &amp;= \gamma \, v_{t-1} + g_t \\ \theta_{t+1} &amp;= \theta_t - \eta \,v_t \end{align} \]</span></p><p>It converges generally faster than the naive stochastic gradient descent, due to the accumulated <span class="math inline">\(v_t\)</span> can help eliminate the unrelated directions of the gradient, and amplify the most related directions of the gradient. The ratio of amplification is roughly <span class="math inline">\(\frac{1}{1 - \gamma}\)</span>.</p><h3 id="nesterov-momentum-sgd">Nesterov Momentum SGD</h3><p><span class="math display">\[ \begin{align} v_t &amp;= \gamma \,v_{t-1} + g(\theta_t - \eta\, \gamma \,v_{t-1}) \\ \theta_{t+1} &amp;= \theta_t - \eta\, v_t \end{align} \]</span></p><h4 id="alternative-form">Alternative Form</h4><p><span class="citation" data-cites="dozatIncorporatingNesterovMomentum2016">Dozat (<a href="#ref-dozatIncorporatingNesterovMomentum2016" role="doc-biblioref">2016</a>)</span> proposed to combine the two momentum steps into one, resulting in: <span class="math display">\[ \begin{align} v_t &amp;= \gamma \, v_{t-1} + g_t \\ \theta_{t+1} &amp;= \theta_t - \eta \, (\gamma \,v_t + g_t) \end{align} \]</span> which may be more preferable in practice.</p><p><em>Proof</em></p><p>Let <span class="math inline">\(\hat{\theta}_t = \theta_t - \eta\,\gamma\,v_{t-1}\)</span> and <span class="math inline">\(\hat{g}_t = g(\hat{\theta}_t) = g(\theta_t - \eta\, \gamma \,v_{t-1})\)</span>, and substitute into the original form, we can obtain: <span class="math display">\[ \begin{align} v_t &amp;= \gamma\,v_{t-1} + \hat{g}_t \\ \hat{\theta}_{t+1} &amp;= \theta_{t+1} - \eta\,\gamma\,v_t \\ &amp;= \theta_t - \eta\,v_t - \eta\,\gamma\,v_t \\ &amp;= \theta_t - \eta(\gamma\,v_{t-1} + \hat{g}_t) - \eta\,\gamma\,v_t \\ &amp;= (\theta_t - \eta\,\gamma\,v_{t-1}) - \eta\,(\hat{g}_t + \gamma\,v_t) \\ &amp;= \hat{\theta}_t - \eta\,(\gamma\,v_t + \hat{g}_t) \end{align} \]</span> Discarding all <span class="math inline">\(\hat{ }\)</span> marks, we then get to the conclusion.</p><h4 id="convergence-analysis">Convergence Analysis</h4><p>This is a modified version of the above momentum SGD, which converges generally faster. <span class="citation" data-cites="BiMomentumGengKuaiJieKaiNesterovAccelerated">(“比Momentum更快：揭开Nesterov Accelerated Gradient的真面目,” <a href="#ref-BiMomentumGengKuaiJieKaiNesterovAccelerated" role="doc-biblioref">n.d.</a>)</span> suggests that this difference may be caused by the (approximately) second-order property of nesterov momentum SGD, since if we let: <span class="math display">\[ \begin{align} \hat{\theta}_t &amp;= \theta_t - \eta\,\gamma\,v_{t-1} \\ \hat{v}_t &amp;= \gamma^2 \,v_{t-1} + (\gamma + 1) \, g(\hat{\theta}_t) \end{align} \]</span> we can obtain the following iterative equations: <span class="math display">\[ \begin{align} \hat{v}_t &amp;= \gamma\,\hat{v}_{t-1} + g(\hat{\theta}_t) + \gamma\left[ g(\hat{\theta}_t) - g(\hat{\theta}_{t-1}) \right] \\ \hat{\theta}_{t+1} &amp;= \hat{\theta}_t - \eta \, \hat{v}_t \end{align} \]</span> which suggests the nesterov momentum SGD uses the second order gradient (approximated by <span class="math inline">\(g(\hat{\theta}_{t-1}) - g(\hat{\theta}_{t-2})\)</span>) to revise the trajectory produced by the first order gradient <span class="math inline">\(g(\hat{\theta}_{t-1})\)</span>.</p><p><em>Proof</em></p><p>From the original equation of nesterov momentum SGD, we have: <span class="math display">\[ \begin{align} \theta_{t+1} - \eta\,\gamma\,v_t &amp;= \theta_t - \eta\,(\gamma+1)\,v_t \\ &amp;= \theta_t - \eta\,(\gamma+1)\,\big( \gamma\,v_{t-1} + g(\theta_t - \eta\,\gamma\,v_{t-1}) \big) \\ &amp;= \theta_t - \eta\, \gamma\,v_{t-1} - \eta\,\gamma^2\,v_{t-1} - \eta\,(\gamma+1)\,g(\theta_t - \eta\,\gamma\,v_{t-1}) \end{align} \]</span> Substitute <span class="math inline">\(\hat{\theta}_t\)</span> and <span class="math inline">\(\hat{v}_t\)</span> into the above equation, we have: <span class="math display">\[ \hat{\theta}_{t+1} = \hat{\theta}_t - \eta \, \hat{v}_t \]</span> We then next deal with the term <span class="math inline">\(\hat{v}_t\)</span>. Substitute <span class="math inline">\(v_t\)</span>, we have: <span class="math display">\[ \begin{align} \hat{v}_t &amp;= (\gamma+1) \, g(\hat{\theta}_t) + \gamma^2\,v_{t-1} \\ &amp;= (\gamma+1) \, g(\hat{\theta}_t) + \gamma^2 \left( \gamma \, v_{t-2} + g(\hat{\theta}_{t-1})\right) \\ &amp;= (\gamma+1) \, g(\hat{\theta}_t) + \gamma^2 \, g(\hat{\theta}_{t-1}) + \gamma^3\left( \gamma\,v_{t-3} + g(\hat{\theta}_{t-2}) \right) \\ &amp;= (\gamma+1) \, g(\hat{\theta}_t) + \gamma^2 \, g(\hat{\theta}_{t-1}) + \gamma^3 \, g(\hat{\theta}_{t-2}) + \dots \\ \hat{v}_{t-1} &amp;= \qquad \qquad \quad \;\, (\gamma+1) \, g(\hat{\theta}_{t-1}) + \gamma^2 \, g(\hat{\theta}_{t-2}) + \dots \end{align} \]</span></p><p>Subtract <span class="math inline">\(\gamma \, \hat{v}_{t-1}\)</span> from <span class="math inline">\(\hat{v}_t\)</span>, we have: <span class="math display">\[ \hat{v}_t - \gamma\,\hat{v}_{t-1} = (\gamma+1) \, g(\hat{\theta}_t) - \gamma \, g(\hat{\theta}_{t-1}) \]</span> We thus obtain: <span class="math display">\[ \hat{v}_t = \gamma \, \hat{v}_{t-1} + g(\hat{\theta}_t) + \gamma\left[ g(\hat{\theta}_t) - g(\hat{\theta}_{t-1}) \right] \]</span></p><h3 id="adagrad">Adagrad</h3><p><span class="citation" data-cites="duchiAdaptiveSubgradientMethods2011">Duchi, Hazan, and Singer (<a href="#ref-duchiAdaptiveSubgradientMethods2011" role="doc-biblioref">2011</a>)</span> proposed a method, which adapts the learning rate according to the update rate of each parameter: for fast updating parameters, it uses smaller learning rate, and conversely, it uses larger learning rate. This mechanism is well suitable for sparse data <span class="citation" data-cites="OverviewGradientDescent2016">(“An Overview of Gradient Descent Optimization Algorithms” <a href="#ref-OverviewGradientDescent2016" role="doc-biblioref">2016</a>)</span>. <span class="math display">\[ \begin{align} G_t &amp;= \sum_{\tau=1}^t g_{\tau}^2 \\ \theta_{t+1} &amp;= \theta_t - \frac{\eta}{\sqrt{G_t + \epsilon}} \odot g_t \end{align} \]</span> where <span class="math inline">\(\odot\)</span> denotes the element-wise multiplication between two vectors. Here <span class="math inline">\(g_t^2 = g_t \odot g_t\)</span>, is the element-wise square of <span class="math inline">\(g_t\)</span>. <span class="math inline">\(G_t\)</span> is the sum of squares of all gradients of <span class="math inline">\(\theta\)</span> since the beginning of training. The small infinitesimal <span class="math inline">\(\epsilon\)</span> is adopted to avoid dividing by zero.</p><h3 id="adadelta">Adadelta</h3><p>To avoid having an infinitesimal learning rate as that of Adagrad, <span class="citation" data-cites="zeilerADADELTAAdaptiveLearning2012">Zeiler (<a href="#ref-zeilerADADELTAAdaptiveLearning2012" role="doc-biblioref">2012</a>)</span> proposed to use a exponential moving average to estimate the expectation of <span class="math inline">\(g_t^2\)</span> (denoted as <span class="math inline">\(E[g^2]\)</span>), instead of summing up all squares of gradients.</p><p>Also, it maintains the moving average for square of the term <span class="math inline">\(\Delta \theta\)</span> (i.e., the update of parameters at <span class="math inline">\(t\)</span>, denoted as <span class="math inline">\(E[\Delta\theta^2]\)</span>) to match the units of <span class="math inline">\(E[g^2]_t\)</span>. <span class="math display">\[ \begin{align} E[g^2]_t &amp;= \gamma \,E[g^2]_{t-1} + (1-\gamma) \,g_t^2 \\ E[\Delta\theta^2]_t &amp;= \gamma\,E[\Delta\theta^2]_{t-1} + (1-\gamma)\,(\Delta\theta_t)^2 \\ \mathop{RMS}[g]_t &amp;= \sqrt{E[g^2]_t + \epsilon} \\ RMS[\Delta\theta]_t &amp;= \sqrt{E[\Delta\theta^2]_t + \epsilon} \\ \Delta \theta_t &amp;= -\frac{RMS[\Delta\theta]_{t-1}}{RMS[g]_{t-1}} \odot g_t \\ \theta_{t+1} &amp;= \theta_t + \eta \,\Delta \theta_t \end{align} \]</span> where the learning rate <span class="math inline">\(\eta\)</span> is chosen to be 1 in the original paper.</p><h3 id="rmsprop">RMSprop</h3><p>Hinton proposed an unpublished SGD method based on Adagrad, also to avoid having an infinitesimal learning rate. (See <a href="http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf" target="_blank" rel="noopener">Lecture 6e of his Coursera Class</a>). <span class="math display">\[ \begin{align} E[g^2]_t &amp;= \gamma \,E[g^2]_{t-1} + (1-\gamma)\,g_t^2 \\ \theta_{t+1} &amp;= \theta_t - \frac{\eta}{\sqrt{E[g^2]_t + \epsilon}} \odot g_t \end{align} \]</span> with the denominator exactly the same as Adadelta. The moving average decay factor <span class="math inline">\(\gamma\)</span> is suggested to be 0.9, while the initial learning rate <span class="math inline">\(\eta\)</span> is suggested to be 0.001.</p><h3 id="adam">Adam</h3><p>In addition to tracking the moving average of squares of gradients, <span class="citation" data-cites="kingmaAdamMethodStochastic2017">Kingma and Ba (<a href="#ref-kingmaAdamMethodStochastic2017" role="doc-biblioref">2017</a>)</span> proposed to also track the moving average of the gradients. <span class="math display">\[ \begin{align} m_t &amp;= \beta_1 m_{t-1} + (1-\beta_1)\,g_t \\ v_t &amp;= \beta_2 v_{t-1} + (1-\beta_2)\,g_t^2 \\ \end{align} \]</span> The authors also proposed to apply a zero-debias term to the moving average estimates: <span class="math display">\[ \begin{align} \hat{m}_t &amp;= \frac{m_t}{1-\beta_1^t} \\ \hat{v}_t &amp;= \frac{v_t}{1-\beta_2^t} \end{align} \]</span> Then update the parameters accordingly: <span class="math display">\[ \theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \odot \hat{m}_t \]</span> By default, the hyper-parameters are suggested to be <span class="math inline">\(\beta_1 = 0.9\)</span>, <span class="math inline">\(\beta_2 = 0.999\)</span>, <span class="math inline">\(\epsilon = 10^{-8}\)</span>.</p><h3 id="adamax">Adamax</h3><p>As a variant of Adam, Adamax uses <span class="math inline">\(l_{\infty}\)</span> norm instead of <span class="math inline">\(l_2\)</span> norm for the denominator: <span class="math display">\[ \begin{align} m_t &amp;= \beta_1 m_{t-1} + (1-\beta_1)\,g_t \\ \hat{m}_t &amp;= \frac{m_t}{1-\beta_1^t} \\ u_t &amp;= \left( \beta_2^{\infty} \, u_{t-1}^{\infty} + (1-\beta_2^{\infty}) \,\left| g_t \right|^{\infty} \right)^{1/\infty} \\ &amp;= \max\left( \beta_2\,u_{t-1}, \left| g_t \right| \right) \\ \theta_{t+1} &amp;= \theta_t - \frac{\eta}{u_t} \, \hat{m}_t \end{align} \]</span> By default, the hyper-parameters are suggested to be <span class="math inline">\(\beta_1 = 0.9\)</span>, <span class="math inline">\(\beta_2 = 0.999\)</span>, <span class="math inline">\(\epsilon = 10^{-8}\)</span>.</p><h3 id="nadam">Nadam</h3><p><span class="citation" data-cites="dozatIncorporatingNesterovMomentum2016">Dozat (<a href="#ref-dozatIncorporatingNesterovMomentum2016" role="doc-biblioref">2016</a>)</span> incorporates Nesterov momentum into Adam.</p><p>Comparing the momentum method: <span class="math display">\[ \begin{align} v_t &amp;= \gamma \, v_{t-1} + g_t \\ \theta_{t+1} &amp;= \theta_t - \eta \,v_t \\ &amp;= \theta_t - \eta\,(\gamma\,v_{t-1} + g_t) \end{align} \]</span></p><p>with the “alternative form” of Nesterov Momentum method: <span class="math display">\[ \begin{align} v_t &amp;= \gamma \, v_{t-1} + g_t \\ \theta_{t+1} &amp;= \theta_t - \eta \, (\gamma \,v_t + g_t) \end{align} \]</span></p><p>It is clear that, we can obtain the Nesterov Momentum method by replacing <span class="math inline">\(v_{t-1}\)</span> with <span class="math inline">\(v_t\)</span> in the term <span class="math inline">\(\gamma\,v_{t-1} + g_t\)</span> in the Momentum method.</p><p>We then first seek to write Adam in such a form. Given that: <span class="math display">\[ \begin{align} m_t &amp;= \beta_1 m_{t-1} + (1-\beta_1)\,g_t \\ \hat{m}_t &amp;= \frac{m_t}{1-\beta_1^t} \end{align} \]</span> We have: <span class="math display">\[ \begin{align} \theta_{t+1} &amp;= \theta_t - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \odot \hat{m}_t \\ &amp;= \theta_t - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \odot \frac{m_t}{1-\beta_1^t} \\ &amp;= \theta_t - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \odot \left( \frac{\beta_1}{1-\beta_1^t} \cdot m_{t-1} + \frac{1-\beta_1}{1-\beta_1^t} \cdot g_t \right) \end{align} \]</span></p><p>If we consider <span class="math inline">\(m_{t-1}/(1-\beta_1^t) \approx m_{t-1}/(1-\beta_1^{t-1})\)</span>, which is true when <span class="math inline">\(t \to \infty\)</span>, we can obtain: <span class="math display">\[ \theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v}_t}+\epsilon}\odot \left( \beta_1\,\hat{m}_{t-1} + \frac{1-\beta_1}{1-\beta_1^t} \cdot g_t \right) \]</span> By replacing <span class="math inline">\(\hat{m}_{t-1}\)</span> with <span class="math inline">\(\hat{m}_t\)</span> as analogue to the Nesterov Momentum method, we finally obtain: <span class="math display">\[ \theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v}_t}+\epsilon}\odot \left( \beta_1\,\hat{m}_t + \frac{1-\beta_1}{1-\beta_1^t} \cdot g_t \right) \]</span></p><p>Note the <span class="math inline">\(\hat{v}_t\)</span> is not changed in Nadam. In summary, we get the following update equations for Nadam: <span class="math display">\[ \begin{align} v_t &amp;= \beta_2 v_{t-1} + (1-\beta_2)\,g_t^2 \\ \hat{v}_t &amp;= \frac{v_t}{1-\beta_2^t} \\ m_t &amp;= \beta_1 m_{t-1} + (1-\beta_1)\,g_t \\ \hat{m}_t &amp;= \frac{m_t}{1-\beta_1^t} \\ \theta_{t+1} &amp;= \theta_t - \frac{\eta}{\sqrt{\hat{v}_t}+\epsilon}\odot \left( \beta_1\,\hat{m}_t + \frac{1-\beta_1}{1-\beta_1^t} \cdot g_t \right) \end{align} \]</span></p><h3 id="amsgrad">AMSGrad</h3><p>The aggressive moving average strategy used by Adam may cause it hard to converge in some problems <span class="citation" data-cites="reddiConvergenceAdam2019">(Reddi, Kale, and Kumar <a href="#ref-reddiConvergenceAdam2019" role="doc-biblioref">2019</a>)</span>. AMSGrad is thus proposed as a modified version of Adam: <span class="math display">\[ \begin{align} m_t &amp;= \beta_1 m_{t-1} + (1-\beta_1)\,g_t \\ v_t &amp;= \beta_2 v_{t-1} + (1-\beta_2)\,g_t^2 \\ \hat{v}_t &amp;= \max(\hat{v}_{t-1}, v_t) \\ \theta_{t+1} &amp;= \theta_t - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \odot m_t \end{align} \]</span> The authors discarded the bias correction term <span class="math inline">\(1 / (1 - \beta_1^t)\)</span> and <span class="math inline">\(1 / (1 - \beta_2^t)\)</span> for simplicity. But in practice, some implementations may still consider this correction term, resulting in: <span class="math display">\[ \begin{align} m_t &amp;= \beta_1 m_{t-1} + (1-\beta_1)\,g_t \\ v_t &amp;= \beta_2 v_{t-1} + (1-\beta_2)\,g_t^2 \\ u_t &amp;= \max(u_{t-1}, v_t) \\ \hat{m}_t &amp;= \frac{m_t}{1 - \beta_1^t} \\ \hat{u}_t &amp;= \frac{u_t}{1 - \beta_2^t} \\ \theta_{t+1} &amp;= \theta_t - \frac{\eta}{\sqrt{\hat{u}_t} + \epsilon} \odot \hat{m}_t \end{align} \]</span></p><h1 id="bibliography" class="unnumbered">References</h1><div id="refs" class="references" role="doc-bibliography"><div id="ref-OverviewGradientDescent2016"><p>“An Overview of Gradient Descent Optimization Algorithms.” 2016. <em>Sebastian Ruder</em>. https://ruder.io/optimizing-gradient-descent/.</p></div><div id="ref-dozatIncorporatingNesterovMomentum2016"><p>Dozat, Timothy. 2016. “Incorporating Nesterov Momentum into Adam,” 4.</p></div><div id="ref-duchiAdaptiveSubgradientMethods2011"><p>Duchi, John, Elad Hazan, and Yoram Singer. 2011. “Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.” <em>Journal of Machine Learning Research</em> 12 (61): 2121–59.</p></div><div id="ref-kingmaAdamMethodStochastic2017"><p>Kingma, Diederik P., and Jimmy Ba. 2017. “Adam: A Method for Stochastic Optimization.” <em>arXiv:1412.6980 [Cs]</em>, January. <a href="http://arxiv.org/abs/1412.6980" target="_blank" rel="noopener">http://arxiv.org/abs/1412.6980</a>.</p></div><div id="ref-reddiConvergenceAdam2019"><p>Reddi, Sashank J., Satyen Kale, and Sanjiv Kumar. 2019. “On the Convergence of Adam and Beyond.” <em>arXiv:1904.09237 [Cs, Math, Stat]</em>, April. <a href="http://arxiv.org/abs/1904.09237" target="_blank" rel="noopener">http://arxiv.org/abs/1904.09237</a>.</p></div><div id="ref-zeilerADADELTAAdaptiveLearning2012"><p>Zeiler, Matthew D. 2012. “ADADELTA: An Adaptive Learning Rate Method.” <em>arXiv:1212.5701 [Cs]</em>, December. <a href="http://arxiv.org/abs/1212.5701" target="_blank" rel="noopener">http://arxiv.org/abs/1212.5701</a>.</p></div><div id="ref-BiMomentumGengKuaiJieKaiNesterovAccelerated"><p>“比Momentum更快：揭开Nesterov Accelerated Gradient的真面目.” n.d. <em>知乎专栏</em>. https://zhuanlan.zhihu.com/p/22810533.</p></div></div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;first-order-methods&quot;&gt;First-Order Methods&lt;/h2&gt;&lt;p&gt;For simplicity, we first introduce the following common notations:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;span 
      
    
    </summary>
    
      <category term="Deep_Learning" scheme="https://wiki.haowen-xu.com/categories/Deep-Learning/"/>
    
      <category term="Optimization" scheme="https://wiki.haowen-xu.com/categories/Deep-Learning/Optimization/"/>
    
    
  </entry>
  
  <entry>
    <title>Visualizing High Dimensional Space</title>
    <link href="https://wiki.haowen-xu.com/Deep_Learning/Evaluation/Visualizing_High_Dimensional_Space/"/>
    <id>https://wiki.haowen-xu.com/Deep_Learning/Evaluation/Visualizing_High_Dimensional_Space/</id>
    <published>2020-05-22T02:46:41.000Z</published>
    <updated>2020-06-01T12:15:59.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="orthogonal-projection">Orthogonal Projection</h2><p><span class="citation" data-cites="izmailovAveragingWeightsLeads2018">Izmailov et al. (<a href="#ref-izmailovAveragingWeightsLeads2018" role="doc-biblioref">2018</a>)</span> proposed to project weight tensors <span class="math inline">\(w\)</span> into 2d plane (i.e., obtain the projector <span class="math inline">\(w(x,y)\)</span>), by constructing the orthogonal basis <span class="math inline">\((\hat{u},\hat{v})\)</span> from three given weight tensors <span class="math inline">\((w_1, w_2, w_3)\)</span>: <span class="math display">\[ \begin{align} u &amp;= w_2 - w_1 \\ v &amp;= (w_3 - w_1) - \frac{\left&lt;w_3-w_1, w_2-w_1\right&gt;}{\left\| w_2-w_1 \right\|^2} \cdot (w_2 - w_1) \\ \hat{u} &amp;= \frac{u}{\left\| u \right\|} \\ \hat{v} &amp;= \frac{v}{\left\| v \right\|} \\ w(x,y) &amp;= w_1 + x\cdot \hat{u} + y \cdot \hat{v} \end{align} \]</span></p><h1 id="bibliography" class="unnumbered">References</h1><div id="refs" class="references" role="doc-bibliography"><div id="ref-izmailovAveragingWeightsLeads2018"><p>Izmailov, Pavel, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wilson. 2018. “Averaging Weights Leads to Wider Optima and Better Generalization.” <em>arXiv Preprint arXiv:1803.05407</em>.</p></div></div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;orthogonal-projection&quot;&gt;Orthogonal Projection&lt;/h2&gt;&lt;p&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;izmailovAveragingWeightsLeads2018&quot;&gt;Izmailov e
      
    
    </summary>
    
      <category term="Deep_Learning" scheme="https://wiki.haowen-xu.com/categories/Deep-Learning/"/>
    
      <category term="Evaluation" scheme="https://wiki.haowen-xu.com/categories/Deep-Learning/Evaluation/"/>
    
    
  </entry>
  
  <entry>
    <title>Loss Surface and Generalization</title>
    <link href="https://wiki.haowen-xu.com/Deep_Learning/Optimization/Loss_Surface_and_Generalization/"/>
    <id>https://wiki.haowen-xu.com/Deep_Learning/Optimization/Loss_Surface_and_Generalization/</id>
    <published>2020-05-21T00:13:22.000Z</published>
    <updated>2020-06-01T12:15:59.008Z</updated>
    
    <content type="html"><![CDATA[<h2 id="stochastic-weight-averaging">Stochastic Weight Averaging</h2><p><span class="citation" data-cites="izmailovAveragingWeightsLeads2018">Izmailov et al. (<a href="#ref-izmailovAveragingWeightsLeads2018" role="doc-biblioref">2018</a>)</span> suggested</p><figure><img src="/Deep_Learning/Optimization/Loss_Surface_and_Generalization/Stochastic_Weighted_Averaging.png" id="fig:Stochastic_Weighted_Averaging" data-max-height="240px" alt="" style="max-height:240px"><figcaption>Figure 1: Stochastic Weighted Averaging Illustration (view <a href="/Deep_Learning/Optimization/Loss_Surface_and_Generalization/Stochastic_Weighted_Averaging.pdf">pdf</a>)</figcaption></figure><p>The visualization method of the above figure can be found at <a href="/Deep_Learning/Evaluation/Visualizing_High_Dimensional_Space/#orthogonal-projection">Orthogonal Projection</a>.</p><h1 id="bibliography" class="unnumbered">References</h1><div id="refs" class="references" role="doc-bibliography"><div id="ref-izmailovAveragingWeightsLeads2018"><p>Izmailov, Pavel, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wilson. 2018. “Averaging Weights Leads to Wider Optima and Better Generalization.” <em>arXiv Preprint arXiv:1803.05407</em>.</p></div></div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;stochastic-weight-averaging&quot;&gt;Stochastic Weight Averaging&lt;/h2&gt;&lt;p&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;izmailovAveragingWeightsLeads2018
      
    
    </summary>
    
      <category term="Deep_Learning" scheme="https://wiki.haowen-xu.com/categories/Deep-Learning/"/>
    
      <category term="Optimization" scheme="https://wiki.haowen-xu.com/categories/Deep-Learning/Optimization/"/>
    
    
  </entry>
  
  <entry>
    <title>Overview</title>
    <link href="https://wiki.haowen-xu.com/Deep_Learning/Generative_Adversarial_Nets/Overview/"/>
    <id>https://wiki.haowen-xu.com/Deep_Learning/Generative_Adversarial_Nets/Overview/</id>
    <published>2020-03-21T04:07:29.000Z</published>
    <updated>2020-06-01T12:15:59.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="original-gan">Original GAN</h2><p><span class="citation" data-cites="goodfellowGenerativeAdversarialNets2014">Goodfellow et al. (<a href="#ref-goodfellowGenerativeAdversarialNets2014" role="doc-biblioref">2014</a>)</span> proposed the following first GAN architecture:</p><ul><li>Prior: <span class="math inline">\(p_g(\mathbf{z}) = \mathcal{N}(\mathbf{0},\mathbf{I})\)</span></li><li>Generator: <span class="math inline">\(G(\mathbf{z})\)</span></li><li>Discriminator: <span class="math inline">\(D(\mathbf{x}) \in [0, 1]\)</span></li></ul><p>The overall loss from their theory: <span class="math display">\[ \mathcal{L} = \min_G \max_D \left\{ \mathbb{E}_{p_d(\mathbf{x})}\left[ \log D(\mathbf{x}) \right] + \mathbb{E}_{p_g(\mathbf{z})}\left[ \log(1 - D(G(\mathbf{z}))) \right] \right\} \]</span> The discriminator loss (to <strong>maximize</strong>): <span class="math display">\[ \mathcal{L}_D = \mathbb{E}_{p_d(\mathbf{x})} \left[ \log D(\mathbf{x}) \right] + \mathbb{E}_{p_g(\mathbf{z})}\left[ \log(1 - D(G(\mathbf{z}))) \right] \]</span></p><p>The theoretical generator loss (to <strong>minimize</strong>): <span class="math display">\[ \mathcal{L}_G = \mathbb{E}_{p_g(\mathbf{z})}\left[ \log(1 - D(G(\mathbf{z}))) \right] \]</span> The actual generator loss (to <strong>maximize</strong>) in experiments, to avoid saturation in the early stage of training: <span class="math display">\[ \mathcal{L}_G = \mathbb{E}_{p_g(\mathbf{z})}\left[ \log(D(G(\mathbf{z}))) \right] \]</span></p><h3 id="gan-training-algorithm">GAN Training Algorithm</h3><p>The most widely adopted GAN training algorithm, which is proposed in this work, alternates between training the discriminator and the generator. That is, in each training iteration:</p><ul><li><p>Repeat for <em>n_critics</em> iterations</p><ul><li><p>Sample <span class="math inline">\(\mathbf{x}^{(1)}, \dots, \mathbf{x}^{(b)}\)</span> from <span class="math inline">\(p_d(\mathbf{x})\)</span>, <span class="math inline">\(\mathbf{z}^{(1)}, \dots, \mathbf{z}^{(b)}\)</span> from <span class="math inline">\(p_g(\mathbf{z})\)</span>.</p></li><li><p>Update the discriminator by: <span class="math display">\[ \theta_D = \theta_D + \eta \, \nabla_{\theta_D}\left( \frac{1}{b}\sum_{i=1}^b \left[ \log D(\mathbf{x}^{(i)}) \right] + \frac{1}{b}\sum_{i=1}^b \left[ \log(1 - D(G(\mathbf{z}^{(i)}))) \right] \right) \]</span></p></li></ul></li><li><p>Sample <span class="math inline">\(\mathbf{z}^{(1)}, \dots, \mathbf{z}^{(b)}\)</span> from <span class="math inline">\(p_g(\mathbf{z})\)</span>.</p></li><li><p>Update the generator by: <span class="math display">\[ \theta_G = \theta_G - \eta\,\nabla_{\theta_G}\left( \frac{1}{b} \sum_{i=1}^b \log(1 - D(G(\mathbf{z}^{(i)}))) \right) \]</span> or alternatively, <span class="math display">\[ \theta_G = \theta_G + \eta\,\nabla_{\theta_G}\left( \frac{1}{b} \sum_{i=1}^b \log(D(G(\mathbf{z}^{(i)}))) \right) \]</span></p></li></ul><h1 id="bibliography" class="unnumbered">References</h1><div id="refs" class="references" role="doc-bibliography"><div id="ref-goodfellowGenerativeAdversarialNets2014"><p>Goodfellow, Ian, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. “Generative Adversarial Nets.” In <em>Advances in Neural Information Processing Systems</em>, 2672–80.</p></div></div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;original-gan&quot;&gt;Original GAN&lt;/h2&gt;&lt;p&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;goodfellowGenerativeAdversarialNets2014&quot;&gt;Goodfellow et al. (&lt;a 
      
    
    </summary>
    
      <category term="Deep_Learning" scheme="https://wiki.haowen-xu.com/categories/Deep-Learning/"/>
    
      <category term="Generative_Adversarial_Nets" scheme="https://wiki.haowen-xu.com/categories/Deep-Learning/Generative-Adversarial-Nets/"/>
    
    
  </entry>
  
  <entry>
    <title>Evaluation Metrics</title>
    <link href="https://wiki.haowen-xu.com/Deep_Learning/Evaluation/Evaluation_Metrics/"/>
    <id>https://wiki.haowen-xu.com/Deep_Learning/Evaluation/Evaluation_Metrics/</id>
    <published>2020-03-20T18:07:18.000Z</published>
    <updated>2020-06-01T12:15:59.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="likelihood">Likelihood</h2><h3 id="negative-log-likelihood">Negative Log-Likelihood</h3><p>The negative log-likelihood (NLL) for <span class="math inline">\(p_{\theta}(\mathbf{x})\)</span> is defined as:</p><p><span class="math display">\[ \begin{align} \text{NLL} &amp;= \mathbb{E}_{p_d(\mathbf{x})} \left[ -\log p_{\theta}(\mathbf{x}) \right] \end{align} \]</span></p><h4 id="find-the-original-nll-using-scaled-data">Find the Original NLL using Scaled Data</h4><p>If the original data <span class="math inline">\(\mathbf{x}\)</span> is <span class="math inline">\(k\)</span>-dimensional ,and is scaled by <span class="math inline">\(\frac{1}{\sigma}\)</span> at each of its dimensions, such that the data fed into the model is <span class="math inline">\(\tilde{\mathbf{x}} = \frac{1}{\sigma} \mathbf{x}\)</span>, then: <span class="math display">\[ \begin{align} p_d(\mathbf{x}) &amp;= \tilde{p}_d(\tilde{\mathbf{x}}) \left| \det\left( \frac{\mathrm{d}\tilde{\mathbf{x}}}{\mathrm{d}\mathbf{x}} \right) \right| = \tilde{p}_d(\tilde{\mathbf{x}})\left| \det\left( \frac{\mathrm{d}(\mathbf{x} / \sigma)}{\mathrm{d}\mathbf{x}} \right) \right| = \frac{1}{\sigma^k}\,\tilde{p}_d(\tilde{\mathbf{x}}) \\ p_{\theta}(\mathbf{x}) &amp;= \frac{1}{\sigma^k}\, \tilde{p}_{\theta}(\tilde{\mathbf{x}}) \end{align} \]</span> Thus the computed NLL for <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\tilde{\mathbf{x}}\)</span> has the following relationship: <span class="math display">\[ \begin{align} \text{NLL} &amp;= \mathbb{E}_{p_d(\mathbf{x})} \left[ -\log p_{\theta}(\mathbf{x}) \right] \\ &amp;= -\int p_d(\mathbf{x}) \log p_{\theta}(\mathbf{x})\,\mathrm{d}\mathbf{x} \\ &amp;= -\int \frac{1}{\sigma^k}\,\tilde{p}_{d}(\tilde{\mathbf{x}}) \log \left( \frac{1}{\sigma^k}\, \tilde{p}_{\theta}(\tilde{\mathbf{x}}) \right)\left| \det\left( \frac{\mathrm{d}\mathbf{x}}{\mathrm{d}\tilde{\mathbf{x}}} \right) \right|\mathrm{d}\tilde{\mathbf{x}} \\ &amp;= -\int \tilde{p}_{d}(\tilde{\mathbf{x}}) \left[ \log \tilde{p}_{\theta}(\tilde{\mathbf{x}}) - k\log \sigma \right]\mathrm{d}\tilde{\mathbf{x}} \\ &amp;= \mathbb{E}_{\tilde{p}_d(\mathbf{x})} \left[ -\log \tilde{p}_{\theta}(\mathbf{x}) \right] + k\log \sigma \\ &amp;= \widetilde{NLL} + k\log\sigma \end{align} \]</span></p><h4 id="continuous-nll-as-an-upper-bound-of-discrete-nll">Continuous NLL as an Upper-Bound of Discrete NLL</h4><p>To train a continuous model upon discrete data (e.g., images), one may add a uniform noise to the data, and obtain an upper-bound of the discrete data NLL with the augmented data.</p><p>For pixel integer-valued <span class="math inline">\(\mathbf{x}\)</span> ranging from 0 to 255, adding a uniform noise <span class="math inline">\(\mathbf{u} \sim \mathcal{U}[0, 1)\)</span>, such that <span class="math inline">\(\tilde{\mathbf{x}} = \mathbf{x} + \mathbf{u}\)</span>, we have <span class="citation" data-cites="theisNoteEvaluationGenerative2015">(Theis, Oord, and Bethge <a href="#ref-theisNoteEvaluationGenerative2015" role="doc-biblioref">2015</a>)</span>: <span class="math display">\[ \begin{align} -\int \tilde{p}_d(\tilde{\mathbf{x}}) \log \tilde{p}_{\theta}(\tilde{\mathbf{x}}) \,\mathrm{d}\tilde{\mathbf{x}} &amp;= -\sum_{\mathbf{x}} P_d(\mathbf{x}) \int \log \tilde{p}_{\theta}(\mathbf{x} + \mathbf{u}) \,\mathrm{d}\mathbf{u} \\ &amp;\geq -\sum_{\mathbf{x}} P_d(\mathbf{x}) \log \int \tilde{p}_{\theta}(\mathbf{x} + \mathbf{u}) \,\mathrm{d}\mathbf{u} \\ \\ &amp;= -\sum_{\mathbf{x}} P_d(\mathbf{x}) \log P_{\theta}(\mathbf{x}) \\ \end{align} \]</span> where we define the probability of the true discrete data to be: <span class="math display">\[ P_{\theta}(\mathbf{x}) = \int \tilde{p}_{\theta}(\mathbf{x} + \mathbf{u}) \,\mathrm{d}\mathbf{u} \]</span> That is to say, the NLL of the augmented continuous random variable <span class="math inline">\(\tilde{\mathbf{x}}\)</span> can serve as an upper-bound as the true discrete data NLL.</p><h2 id="image-quality">Image Quality</h2><h2 id="roc-and-auc">ROC and AUC</h2><p><span class="citation" data-cites="davisRelationshipPrecisionRecallROC2006">(Davis and Goadrich <a href="#ref-davisRelationshipPrecisionRecallROC2006" role="doc-biblioref">2006</a>)</span></p><ol type="1"><li><p>There is a one-to-one correspondence between the points on ROC and AUC curves.</p></li><li><p>A curve dominates the ROC (fpr-tpr curve) <span class="math inline">\(\Leftrightarrow\)</span> dominates the AUC (recall-precision curve).</p></li><li><p>Interpolation between two points <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>:</p><ol type="1"><li><p>On ROC: linear interpolation.</p></li><li><p>On AUC: <span class="math display">\[ \left( \frac{TP_A + x}{\text{Total Pos}}, \frac{TP_A + x}{TP_A + x + FP_A + \frac{FP_B - FP_A}{TP_B - TP_A} x} \right) \]</span></p></li></ol></li><li><p>Compute the area: include the interpolation and use composite trapezoidal method.</p><ol type="1"><li>Incorrect interpoluation for computing AUC-PR will cause over-estimate.</li></ol></li><li><p>Optimize Area under ROC and AUC curves: not exactly the same. (especially when not one algorithm dominates the curve?)</p></li></ol><h1 id="bibliography" class="unnumbered">References</h1><div id="refs" class="references" role="doc-bibliography"><div id="ref-davisRelationshipPrecisionRecallROC2006"><p>Davis, Jesse, and Mark Goadrich. 2006. “The Relationship Between Precision-Recall and ROC Curves.” In <em>Proceedings of the 23rd International Conference on Machine Learning - ICML ’06</em>, 233–40. Pittsburgh, Pennsylvania: ACM Press. <a href="https://doi.org/10.1145/1143844.1143874" target="_blank" rel="noopener">https://doi.org/10.1145/1143844.1143874</a>.</p></div><div id="ref-theisNoteEvaluationGenerative2015"><p>Theis, Lucas, Aäron van den Oord, and Matthias Bethge. 2015. “A Note on the Evaluation of Generative Models.” <em>arXiv Preprint arXiv:1511.01844</em>.</p></div></div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;likelihood&quot;&gt;Likelihood&lt;/h2&gt;&lt;h3 id=&quot;negative-log-likelihood&quot;&gt;Negative Log-Likelihood&lt;/h3&gt;&lt;p&gt;The negative log-likelihood (NLL) for &lt;sp
      
    
    </summary>
    
      <category term="Deep_Learning" scheme="https://wiki.haowen-xu.com/categories/Deep-Learning/"/>
    
      <category term="Evaluation" scheme="https://wiki.haowen-xu.com/categories/Deep-Learning/Evaluation/"/>
    
    
  </entry>
  
  <entry>
    <title>Energy GAN</title>
    <link href="https://wiki.haowen-xu.com/Deep_Learning/Generative_Adversarial_Nets/Energy_GAN/"/>
    <id>https://wiki.haowen-xu.com/Deep_Learning/Generative_Adversarial_Nets/Energy_GAN/</id>
    <published>2020-03-19T20:46:25.000Z</published>
    <updated>2020-06-01T12:15:59.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="overview">Overview</h2><p>Given the discriminator <span class="math inline">\(E_{\theta}(\mathbf{x})\)</span>, a density function can be derived as: <span class="math display">\[ \begin{align} p_{\theta}(\mathbf{x}) &amp;= \frac{1}{Z_{\theta}} e^{-E_{\theta}(\mathbf{x})} \\ Z_{\theta} &amp;= \int e^{-E_{\theta}(\mathbf{x})}\,\mathrm{d}\mathbf{x} \end{align} \]</span></p><h2 id="maximum-entropy-generators-for-energy-based-models">Maximum Entropy Generators for Energy-Based Models</h2><p><span class="citation" data-cites="kumarMaximumEntropyGenerators2019">Kumar et al. (<a href="#ref-kumarMaximumEntropyGenerators2019" role="doc-biblioref">2019</a>)</span> proposed the following architecture:</p><ul><li>Prior: <span class="math inline">\(p_z(\mathbf{z})\)</span></li><li>Generator: <span class="math inline">\(G_{\omega}(\mathbf{z})\)</span></li><li>Discriminator: <span class="math inline">\(E_{\theta}(\mathbf{x}) \in (-\infty, \infty)\)</span>, the energy function<ul><li>The density function: <span class="math inline">\(p_{\theta}(\mathbf{x}) = \frac{1}{Z_{\theta}} e^{-E_{\theta}(\mathbf{x})}\)</span></li></ul></li><li>Discriminator for the mutual information estimator: <span class="math inline">\(T_{\phi}(\mathbf{x},\mathbf{z}) \in (-\infty,\infty)\)</span></li></ul><p>The discriminator loss (to <strong>minimize</strong>): <span class="math display">\[ \begin{align} \mathcal{L}_E &amp;= \mathbb{E}_{p_d(\mathbf{x})}\left[ E_{\theta}(\mathbf{x}) \right] - \mathbb{E}_{p_G(\mathbf{x})}\left[ E_{\theta}(\mathbf{x}) \right] + \Omega \\ \Omega &amp;= \lambda\,\mathbb{E}_{p_d(\mathbf{x})} \left[ \left\| \nabla_{\mathbf{x}} E_{\theta}(\mathbf{x}) \right\|^2 \right] \end{align} \]</span> The generator loss (to <strong>minimize</strong>) <span class="math display">\[ \begin{align} \mathcal{L}_G &amp;= \mathbb{E}_{p_z(\mathbf{z})}\left[ E_{\theta}(G_{\omega}(\mathbf{z})) \right] - H_{p_G}[X] \\ &amp;= \mathbb{E}_{p_z(\mathbf{z})}\left[ E_{\theta}(G_{\omega}(\mathbf{z})) \right] - I_{p_G}(X;Z) \end{align} \]</span> where <span class="math inline">\(H_{p_G}[X] = I_{p_G}(X;Z) + H(G_{\omega}(Z)|Z)\)</span>, and <span class="math inline">\(H(G_{\omega}(Z)|Z) \equiv 0\)</span> since <span class="math inline">\(G_{\omega}(\mathbf{z})\)</span> is a deterministic mapper.</p><p>The mutual information <span class="math inline">\(I_{p_G}(X;Z)\)</span> is estimated via the <a href="/Deep_Learning/Information_Theoretical/Mutual_Information/#deep-infomax">Deep InfoMax</a> estimator by <span class="citation" data-cites="kumarMaximumEntropyGenerators2019">Kumar et al. (<a href="#ref-kumarMaximumEntropyGenerators2019" role="doc-biblioref">2019</a>)</span>, formulated as: <span class="math display">\[ \begin{align} I_{p_G}(X;Z) &amp;= \mathbb{E}_{p_z(\mathbf{z})}\left[ -\text{sp}(-T_{\phi}(G_{\omega}(\mathbf{z}),\mathbf{z})) \right] - \mathbb{E}_{p_z(\mathbf{z})\times\tilde{p}_z(\tilde{\mathbf{z}})}\left[ \text{sp}(T_{\phi}(G_{\omega}(\mathbf{z}),\tilde{\mathbf{z}})) \right] \\ &amp;= \mathbb{E}_{p_z(\mathbf{z})}\left[ \log \sigma(T_{\phi}(G_{\omega}(\mathbf{z}),\mathbf{z})) \right] - \mathbb{E}_{p_z(\mathbf{z})\times\tilde{p}_z(\tilde{\mathbf{z}})}\left[ \log\left(1 - \sigma(T_{\phi}(G_{\omega}(\mathbf{z}),\tilde{\mathbf{z}}))\right) \right] \end{align} \]</span></p><p>where <span class="math inline">\(\text{sp}(a) = \log(1 + e^a)\)</span> is the <em>SoftPlus</em> function, and <span class="math inline">\(\sigma(a) = \frac{1}{1 + e^{-a}}\)</span> is the sigmoid function.</p><h3 id="training-algorithm">Training Algorithm</h3><p><span class="citation" data-cites="kumarMaximumEntropyGenerators2019">Kumar et al. (<a href="#ref-kumarMaximumEntropyGenerators2019" role="doc-biblioref">2019</a>)</span> proposed the following training algorithm:</p><ul><li><p>Repeat for <em>n_critics</em> Iterations</p><ul><li><p>Sample <span class="math inline">\(\mathbf{x}^{(1)}, \dots, \mathbf{x}^{(b)}\)</span> from <span class="math inline">\(p_d(\mathbf{x})\)</span>, <span class="math inline">\(\mathbf{z}^{(1)}, \dots, \mathbf{z}^{(b)}\)</span> from <span class="math inline">\(p_z(\mathbf{z})\)</span>.</p></li><li><p>Obtain <span class="math inline">\(\tilde{\mathbf{x}}^{(i)} = G_{\omega}(\mathbf{z}^{(i)})\)</span>.</p></li><li><p>Calculate: <span class="math display">\[ \mathcal{L}_E = \frac{1}{b} \left[ \sum_{i=1}^b E_{\theta}(\mathbf{x}^{(i)}) - \sum_{i=1}^b E_{\theta}(\tilde{\mathbf{x}}^{(i)}) + \lambda\sum_{i=1}^b \left\| \nabla_{\mathbf{x}} E_{\theta}(\mathbf{x}^{(i)}) \right\|^2 \right] \]</span></p></li><li><p>Gradient descent: <span class="math display">\[ \theta^{t+1} = \theta^{t} - \eta \, \nabla_{\theta} \mathcal{L}_E \]</span></p></li></ul></li><li><p>Sample <span class="math inline">\(\mathbf{z}^{(1)}, \dots, \mathbf{z}^{(b)}\)</span> from <span class="math inline">\(p_z(\mathbf{z})\)</span>.</p></li><li><p>Per-dimensional shuffle of <span class="math inline">\(\mathbf{z}\)</span>, yielding <span class="math inline">\(\tilde{\mathbf{z}}^{(1)}, \dots, \tilde{\mathbf{z}}^{(b)}\)</span>.</p><p><strong>(Is this really better than re-sampling from the prior <span class="math inline">\(p_z(\mathbf{z})\)</span>?)</strong></p></li><li><p>Obtain <span class="math inline">\(\tilde{\mathbf{x}}^{(i)} = G_{\omega}(\mathbf{z}^{(i)})\)</span>.</p></li><li><p>Calculate: <span class="math display">\[ \begin{align} \mathcal{L}_H &amp;= \frac{1}{b} \sum_{i=1}^b \left[ \log \sigma(T_{\phi}(\tilde{\mathbf{x}}^{(i)},\mathbf{z}^{(i)})) - \log\left(1 - \sigma(T_{\phi}(\tilde{\mathbf{x}}^{(i)},\tilde{\mathbf{z}}^{(i)}))\right) \right] \\ \mathcal{L}_G &amp;= \frac{1}{b} \sum_{i=1}^b \left[ E_{\theta}(\tilde{\mathbf{x}}^{(i)}) \right] - \mathcal{L}_H \end{align} \]</span></p></li><li><p>Gradient descent: <span class="math display">\[ \begin{align} \omega^{t+1} &amp;= \omega^{t} - \eta \, \nabla_{\omega} \mathcal{L}_G \\ \phi^{t+1} &amp;= \phi^{t} - \eta \, \nabla_{\phi} \mathcal{L}_H \\ \end{align} \]</span></p></li></ul><p>In summary, this algorithm:</p><ul><li>Minimize <span class="math inline">\(\theta\)</span> <em>w.r.t.</em> <span class="math inline">\(\mathcal{L}_E\)</span> =&gt;<ul><li>Minimizes <span class="math inline">\(E_{\theta}\)</span> in the discriminator loss <span class="math inline">\(\mathcal{L}_E\)</span>.</li></ul></li><li>Minimize <span class="math inline">\(\phi\)</span> <em>w.r.t.</em> <span class="math inline">\(L_H\)</span> =&gt;<ul><li>Minimizes <span class="math inline">\(T_{\phi}\)</span> in the mutual information regularizer <span class="math inline">\(I_{p_G}(X;Z)\)</span>.</li></ul></li><li>Minimize <span class="math inline">\(\omega\)</span> <em>w.r.t.</em> <span class="math inline">\(L_G\)</span> =&gt;<ul><li>Maximizes <span class="math inline">\(G_{\omega}\)</span> in the mutual information regularizer <span class="math inline">\(I_{p_G}(X;Z)\)</span>;</li><li>Minimizes <span class="math inline">\(G_{\omega}\)</span> in the generator loss <span class="math inline">\(\mathbb{E}_{p_z(\mathbf{z})}\left[ E_{\theta}(G_{\omega}(\mathbf{z})) \right]\)</span>.</li></ul></li></ul><h3 id="latent-space-mcmc">Latent Space MCMC</h3><p><span class="citation" data-cites="kumarMaximumEntropyGenerators2019">Kumar et al. (<a href="#ref-kumarMaximumEntropyGenerators2019" role="doc-biblioref">2019</a>)</span> also proposed an MCMC method to refine the <span class="math inline">\(\mathbf{z}\)</span> samples obtained from the prior <span class="math inline">\(p_z(\mathbf{z})\)</span>, according to the energy function on <span class="math inline">\(\mathbf{z}\)</span>, derived as: <span class="math display">\[ E(\mathbf{z}) = E_{\theta}(G_{\omega}(\mathbf{z})) \]</span> Then <a href="/Deep_Learning/Monte_Carlo_Methods/Langevin_Dynamics/">Metropolis-adjusted Langevin algorithm</a> is adopted to sample <span class="math inline">\(\mathbf{z}\)</span>:</p><ul><li><p>Langevin dynamics: <span class="math display">\[ \mathbf{z}^{(t+1)} = \mathbf{z}^{(t)} - \alpha \frac{\partial E_{\theta}(G_{\omega}(\mathbf{z}^{(t)}))}{\partial \mathbf{z}^{(t)}} + \epsilon \sqrt{2 * \alpha} \]</span> where <span class="math inline">\(\epsilon \sim \mathcal{N}(\mathbf{0},\mathbf{I})\)</span>.</p></li><li><p>Metropolis-Hastings Algorithm: <span class="math display">\[ \begin{align} r &amp;= \min\left\{ 1, \frac{p(\mathbf{z}^{(t+1)})}{p(\mathbf{z}^{(t)})} \cdot \frac{q(\mathbf{z}^{(t)}|\mathbf{z}^{(t+1)})}{q(\mathbf{z}^{(t+1)}|\mathbf{z}^{(t)})} \right\} \\ p(\mathbf{z}^{(t)}) &amp;\propto \exp\left\{ -E_{\theta}(G_{\omega}(\mathbf{z}^{(t)})) \right\} \\ q(\mathbf{z}^{(t+1)}|\mathbf{z}^{(t)}) &amp;\propto \exp\left( -\frac{1}{4 \alpha}\left\| \mathbf{z}^{(t+1)} - \mathbf{z}^{(t)} + \alpha \frac{\partial E_{\theta}(G_{\omega}(\mathbf{z}^{(t)}))}{\partial \mathbf{z}^{(t)}} \right\|^2_2 \right) \end{align} \]</span></p></li></ul><h1 id="bibliography" class="unnumbered">References</h1><div id="refs" class="references" role="doc-bibliography"><div id="ref-kumarMaximumEntropyGenerators2019"><p>Kumar, Rithesh, Anirudh Goyal, Aaron Courville, and Yoshua Bengio. 2019. “Maximum Entropy Generators for Energy-Based Models.” <em>arXiv:1901.08508 [Cs, Stat]</em>, January. <a href="http://arxiv.org/abs/1901.08508" target="_blank" rel="noopener">http://arxiv.org/abs/1901.08508</a>.</p></div></div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;&lt;p&gt;Given the discriminator &lt;span class=&quot;math inline&quot;&gt;\(E_{\theta}(\mathbf{x})\)&lt;/span&gt;, a density function ca
      
    
    </summary>
    
      <category term="Deep_Learning" scheme="https://wiki.haowen-xu.com/categories/Deep-Learning/"/>
    
      <category term="Generative_Adversarial_Nets" scheme="https://wiki.haowen-xu.com/categories/Deep-Learning/Generative-Adversarial-Nets/"/>
    
    
  </entry>
  
  <entry>
    <title>Gradient Tricks</title>
    <link href="https://wiki.haowen-xu.com/Deep_Learning/Optimization/Gradient_Tricks/"/>
    <id>https://wiki.haowen-xu.com/Deep_Learning/Optimization/Gradient_Tricks/</id>
    <published>2020-03-18T17:28:16.000Z</published>
    <updated>2020-06-01T12:15:59.008Z</updated>
    
    <content type="html"><![CDATA[<h2 id="clipping">Clipping</h2><h3 id="adaptive-clipping">Adaptive Clipping</h3><p>To avoid the optimizer putting too much attention on just one of the loss components (or adversarial losses), <em>adaptive clipping</em> can be adopted <span class="citation" data-cites="belghaziMineMutualInformation2018">(Belghazi et al. <a href="#ref-belghaziMineMutualInformation2018" role="doc-biblioref">2018</a>)</span>, to match the gradient scales of different losses.</p><p>For example, <span class="citation" data-cites="belghaziMineMutualInformation2018">Belghazi et al. (<a href="#ref-belghaziMineMutualInformation2018" role="doc-biblioref">2018</a>)</span> adapts <span class="math inline">\(g_m\)</span> to match the scale of <span class="math inline">\(g_u\)</span>, where <span class="math inline">\(g_u\)</span> is the main loss gradient, and <span class="math inline">\(g_m\)</span> is the gradient of the mutual information regularizer: <span class="math display">\[ \tilde{g}_m = \min\left( \|g_u\|, \|g_m\| \right) \frac{g_m}{\|g_m\|} \]</span></p><h1 id="bibliography" class="unnumbered">References</h1><div id="refs" class="references" role="doc-bibliography"><div id="ref-belghaziMineMutualInformation2018"><p>Belghazi, Mohamed Ishmael, Aristide Baratin, Sai Rajeswar, Sherjil Ozair, Yoshua Bengio, Aaron Courville, and R. Devon Hjelm. 2018. “Mine: Mutual Information Neural Estimation.” <em>arXiv Preprint arXiv:1801.04062</em>.</p></div></div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;clipping&quot;&gt;Clipping&lt;/h2&gt;&lt;h3 id=&quot;adaptive-clipping&quot;&gt;Adaptive Clipping&lt;/h3&gt;&lt;p&gt;To avoid the optimizer putting too much attention on just
      
    
    </summary>
    
      <category term="Deep_Learning" scheme="https://wiki.haowen-xu.com/categories/Deep-Learning/"/>
    
      <category term="Optimization" scheme="https://wiki.haowen-xu.com/categories/Deep-Learning/Optimization/"/>
    
    
  </entry>
  
  <entry>
    <title>f-GAN</title>
    <link href="https://wiki.haowen-xu.com/Deep_Learning/Generative_Adversarial_Nets/f-GAN/"/>
    <id>https://wiki.haowen-xu.com/Deep_Learning/Generative_Adversarial_Nets/f-GAN/</id>
    <published>2020-03-16T22:58:04.000Z</published>
    <updated>2020-06-01T12:15:59.000Z</updated>
    
    <content type="html"><![CDATA[<p>Proposed by <span class="citation" data-cites="nowozinFGANTrainingGenerative2016">Nowozin, Cseke, and Tomioka (<a href="#ref-nowozinFGANTrainingGenerative2016" role="doc-biblioref">2016</a>)</span>, f-GAN generalizes the KL-divergence of the original GAN framework <span class="citation" data-cites="goodfellowGenerativeAdversarialNets2014">(Goodfellow et al. <a href="#ref-goodfellowGenerativeAdversarialNets2014" role="doc-biblioref">2014</a>)</span> to the f-divergence.</p><h2 id="theoretical-framework">Theoretical Framework</h2><h3 id="f-divergence">f-Divergence</h3><p><span class="math display">\[ D_{f}(P \| Q) = \int q(\mathbf{x}) \,f\left( \frac{p(\mathbf{x})}{q(\mathbf{x})} \right)\,\mathrm{d}\mathbf{x} \]</span></p><p>where <span class="math inline">\(f: \mathbb{R}^+ \to \mathbb{R}\)</span> is a convex, lower-semicontinuous function satisfying <span class="math inline">\(f(\mathbf{1})=0\)</span>.</p><h3 id="fenchel-conjugate">Fenchel conjugate</h3><p>The Fenchel conjugate of <span class="math inline">\(f(u)\)</span> is: <span class="math display">\[ f^*(t) = \sup_{u \in \text{dom}_f}\left\{ ut - f(u) \right\} \]</span> Since <span class="math inline">\(f\)</span> is a convex and lower-semicontinuous function, <span class="math inline">\(f^{**} = f\)</span>, therefore: <span class="math display">\[ f(u) = \sup_{t \in \text{dom}_{f^*}} \left\{ tu - f^*(t) \right\} \]</span> And further we have: <span class="math display">\[ D_{f}(P \| Q) = \int q(\mathbf{x}) \, \sup_{t \in \text{dom}_{f^*}} \left\{ t\,\frac{p(\mathbf{x})}{q(\mathbf{x})} - f^*(t) \right\} \,\mathrm{d}\mathbf{x} \]</span></p><h3 id="variational-estimation-of-f-divergence">Variational Estimation of f-Divergence</h3><p>Using a function <span class="math inline">\(T: \mathcal{X} \to \mathbb{R}\)</span> of family <span class="math inline">\(\mathcal{T}\)</span>, we can can obtain the following lower-bound <span class="citation" data-cites="nowozinFGANTrainingGenerative2016">(Nowozin, Cseke, and Tomioka <a href="#ref-nowozinFGANTrainingGenerative2016" role="doc-biblioref">2016</a>)</span>: <span class="math display">\[ \begin{align} D_{f}(P \| Q) &amp;= \int q(\mathbf{x}) \, \sup_{t \in \text{dom}_{f^*}} \left\{ t\,\frac{p(\mathbf{x})}{q(\mathbf{x})} - f^*(t) \right\} \,\mathrm{d}\mathbf{x} \\ &amp;\geq \sup_{T \in \mathcal{T}} \left\{ \int q(\mathbf{x}) \left( T(\mathbf{x})\,\frac{p(\mathbf{x})}{q(\mathbf{x})} - f^*(T(\mathbf{x})) \right) \,\mathrm{d}\mathbf{x} \right\} \\ &amp;= \sup_{T \in \mathcal{T}} \left\{ \int p(\mathbf{x})\,T(\mathbf{x}) \,\mathrm{d}\mathbf{x} - \int q(\mathbf{x}) \,f^*(T(\mathbf{x})) \,\mathrm{d}\mathbf{x} \right\} \\ &amp;= \sup_{T \in \mathcal{T}} \left\{ \mathbb{E}_{p(\mathbf{x})}\left[ T(\mathbf{x}) \right] - \mathbb{E}_{q(\mathbf{x})} \left[ f^*(T(\mathbf{x})) \right] \right\} \end{align} \]</span> It is a lower-bound, because the family <span class="math inline">\(\mathcal{T}\)</span> might not cover all of the possible functions. In fact, the bound is tight for <span class="math display">\[ T^*(\mathbf{x}) = f'\left(\frac{p(\mathbf{x})}{q(\mathbf{x})}\right) \]</span> where <span class="math inline">\(f'\)</span> is the first derivative of <span class="math inline">\(f\)</span>.</p><h3 id="variational-divergence-minimization">Variational Divergence Minimization</h3><p>Using the variational lower-bound of the f-divergence <span class="math inline">\(D_{f}(P \| Q)\)</span>, we can obtain the objective <span class="math inline">\(\mathcal{F}(\theta,\omega)\)</span>, for the generator <span class="math inline">\(q_{\theta}(\mathbf{x})\)</span>, which should be <strong>minimized</strong> w.r.t. <span class="math inline">\(\theta\)</span>, and <strong>maximized</strong> w.r.t. <span class="math inline">\(\omega\)</span>:</p><p><span class="math display">\[ \mathcal{F}(\theta,\omega) = \mathbb{E}_{p_d(\mathbf{x})}\left[ T_{\omega}(\mathbf{x}) \right] - \mathbb{E}_{q_{\theta}(\mathbf{x})}\left[ f^*(T_{\omega}(\mathbf{x})) \right] \]</span></p><p><strong>Note: <span class="math inline">\(\mathcal{F}(\theta,\omega)\)</span> is just a lower-bound of <span class="math inline">\(D_{f}(P \| Q)\)</span>. Minimizing a lower-bound may not lead to optimal result (commented by me).</strong></p><p>To ensure <span class="math inline">\(T_{\omega}\)</span> match the domain of <span class="math inline">\(f^*\)</span>, it can be further reparameterized as: <span class="math display">\[ T_{\omega}(\mathbf{x}) = g_f(V_{\omega}(\mathbf{x})) \]</span> where <span class="math inline">\(V_{\omega}: \mathcal{X} \to \mathbb{R}\)</span>, and <span class="math inline">\(g_f: \mathbb{R} \to \text{dom}_{f^*}\)</span> being the activation function chosen according to <span class="math inline">\(f\)</span>. Then the above objective can be rewritten as: <span class="math display">\[ \mathcal{F}(\theta,\omega) = \mathbb{E}_{p_d(\mathbf{x})}\left[ g_f(V_{\omega}(\mathbf{x})) \right] + \mathbb{E}_{q_{\theta}(\mathbf{x})}\left[ -f^*(g_f(V_{\omega}(\mathbf{x}))) \right] \]</span></p><h3 id="single-step-gradient-method">Single-Step Gradient Method</h3><p>Instead of alternate between optimizing <span class="math inline">\(\omega\)</span> and <span class="math inline">\(\theta\)</span> in different steps, <span class="citation" data-cites="nowozinFGANTrainingGenerative2016">Nowozin, Cseke, and Tomioka (<a href="#ref-nowozinFGANTrainingGenerative2016" role="doc-biblioref">2016</a>)</span> proposed to optimize these two set of parameters in a single step by: <span class="math display">\[ \begin{align} \omega^{t+1} &amp;= \omega^{t} + \eta \,\nabla_{\omega} \mathcal{F}(\theta^t,\omega^t) \\ &amp;= \omega^{t} + \eta \,\nabla_{\omega} \left( \mathbb{E}_{p_d(\mathbf{x})}\left[ g_f(V_{\omega}(\mathbf{x})) \right] + \mathbb{E}_{q_{\theta}(\mathbf{x})}\left[ -f^*(g_f(V_{\omega}(\mathbf{x}))) \right] \right) \\ \theta^{t+1} &amp;= \theta^{t} - \eta \,\nabla_{\theta} \mathcal{F}(\theta^t,\omega^t) \\ &amp;= \theta^{t} - \eta \,\nabla_{\theta} \left( \mathbb{E}_{q_{\theta}(\mathbf{x})}\left[ -f^*(g_f(V_{\omega}(\mathbf{x}))) \right] \right) \end{align} \]</span></p><p>And similar to <span class="citation" data-cites="goodfellowGenerativeAdversarialNets2014">Goodfellow et al. (<a href="#ref-goodfellowGenerativeAdversarialNets2014" role="doc-biblioref">2014</a>)</span>, <span class="citation" data-cites="nowozinFGANTrainingGenerative2016">Nowozin, Cseke, and Tomioka (<a href="#ref-nowozinFGANTrainingGenerative2016" role="doc-biblioref">2016</a>)</span> also proposed an alternative method to update the paramter <span class="math inline">\(\theta\)</span>, so as to speed up training: <span class="math display">\[ \theta^{t+1} = \theta^{t} + \eta \,\nabla_{\theta} \mathbb{E}_{q_{\theta}(\mathbf{x})}\left[ g_f(V_{\omega}(\mathbf{x})) \right] \]</span> Also note that, in many architectures, the gradient estimator <span class="math inline">\(\nabla_{\theta} \mathbb{E}_{q_{\theta}(\mathbf{x})}\)</span> can be computed with the re-paramterization trick, i.e., reparameterize <span class="math inline">\(\mathbf{x}\)</span> by a differentiable function <span class="math inline">\(\mathbf{x} = \mathbf{x}_{\theta}(\mathbf{\epsilon})\)</span>, where <span class="math inline">\(\mathbf{\epsilon}\)</span> is a random noise independent of <span class="math inline">\(\theta\)</span>.</p><h2 id="formulations-for-various-f-divergence">Formulations for Various f-Divergence</h2><p>More details can be found in <span class="citation" data-cites="nowozinFGANTrainingGenerative2016">Nowozin, Cseke, and Tomioka (<a href="#ref-nowozinFGANTrainingGenerative2016" role="doc-biblioref">2016</a>)</span>, including the formulations for Pearson <span class="math inline">\(\chi^2\)</span>, and the Squared Hellinger.</p><h3 id="kullback-leibler">Kullback-Leibler</h3><p><span class="math display">\[ \begin{align} D_{f}(P \| Q) &amp;= D_{\mathrm{KL}}(P\|Q) \\ &amp;= \int p(\mathbf{x}) \log \frac{p(\mathbf{x})}{q(\mathbf{x})}\,\mathrm{d}\mathbf{x} \\ f(u) &amp;= u \log u \\ f^*(t) &amp;= \exp(t - 1) \\ g_f(v) &amp;= v \end{align} \]</span></p><h3 id="reverse-kl">Reverse KL</h3><p><span class="math display">\[ \begin{align} D_{f}(P \| Q) &amp;= D_{\mathrm{KL}}(Q\|P) \\ &amp;= \int q(\mathbf{x}) \log \frac{q(\mathbf{x})}{p(\mathbf{x})}\,\mathrm{d}\mathbf{x} \\ f(u) &amp;= -\log u \\ f^*(t) &amp;= -1 - \log(-t) \\ g_f(v) &amp;= -\exp(-v) \end{align} \]</span></p><h3 id="jensen-shannon">Jensen-Shannon</h3><p><span class="math display">\[ \begin{align} D_{f}(P \| Q) &amp;= \text{JSD}(P\|Q) = \frac{1}{2}\Big( D_{\mathrm{KL}}(P\|M) +D_{\mathrm{KL}}(Q\|M) \Big) \\ &amp;= \frac{1}{2}\int \left( p(\mathbf{x}) \log \frac{2p(\mathbf{x})}{p(\mathbf{x}) + q(\mathbf{x})} + q(\mathbf{x}) \log \frac{2 q(\mathbf{x})}{p(\mathbf{x}) + q(\mathbf{x})} \right)\,\mathrm{d}\mathbf{x} \\ f(u) &amp;= u\log u - (u+1)\log \frac{u+1}{2} \\ f^*(t) &amp;= -\log(2-\exp(t)) \\ g_f(v) &amp;= \log 2 - \log (1+\exp(-v)) \end{align} \]</span></p><p>where <span class="math inline">\(M = \frac{P+Q}{2}\)</span>.</p><h3 id="original-gan-goodfellowgenerativeadversarialnets2014">Original GAN <span class="citation" data-cites="goodfellowGenerativeAdversarialNets2014">(Goodfellow et al. <a href="#ref-goodfellowGenerativeAdversarialNets2014" role="doc-biblioref">2014</a>)</span></h3><p><span class="math display">\[ \begin{align} D_{f}(P \| Q) &amp;= \int \left( p(\mathbf{x}) \log \frac{2p(\mathbf{x})}{p(\mathbf{x}) + q(\mathbf{x})} + q(\mathbf{x}) \log \frac{2 q(\mathbf{x})}{p(\mathbf{x}) + q(\mathbf{x})} \right)\mathrm{d}\mathbf{x} - \log 4 \\ f(u) &amp;= u \log u - (u+1) \log (u+1) \\ f^*(t) &amp;= -\log(1-\exp(t)) \\ g_f(v) &amp;= -\log(1+\exp(-v)) \end{align} \]</span></p><h1 id="bibliography" class="unnumbered">References</h1><div id="refs" class="references" role="doc-bibliography"><div id="ref-goodfellowGenerativeAdversarialNets2014"><p>Goodfellow, Ian, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. “Generative Adversarial Nets.” In <em>Advances in Neural Information Processing Systems</em>, 2672–80.</p></div><div id="ref-nowozinFGANTrainingGenerative2016"><p>Nowozin, Sebastian, Botond Cseke, and Ryota Tomioka. 2016. “F-GAN: Training Generative Neural Samplers Using Variational Divergence Minimization.” In <em>Advances in Neural Information Processing Systems 29</em>, edited by D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, 271–79. Curran Associates, Inc.</p></div></div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Proposed by &lt;span class=&quot;citation&quot; data-cites=&quot;nowozinFGANTrainingGenerative2016&quot;&gt;Nowozin, Cseke, and Tomioka (&lt;a href=&quot;#ref-nowozinFGANT
      
    
    </summary>
    
      <category term="Deep_Learning" scheme="https://wiki.haowen-xu.com/categories/Deep-Learning/"/>
    
      <category term="Generative_Adversarial_Nets" scheme="https://wiki.haowen-xu.com/categories/Deep-Learning/Generative-Adversarial-Nets/"/>
    
    
  </entry>
  
  <entry>
    <title>Mutual Information</title>
    <link href="https://wiki.haowen-xu.com/Deep_Learning/Information_Theoretical/Mutual_Information/"/>
    <id>https://wiki.haowen-xu.com/Deep_Learning/Information_Theoretical/Mutual_Information/</id>
    <published>2020-03-16T20:02:24.000Z</published>
    <updated>2020-06-01T12:15:59.008Z</updated>
    
    <content type="html"><![CDATA[<h2 id="definition">Definition</h2><p><span class="math display">\[ \begin{align} I(X;Y) &amp;= D_{\mathrm{KL}}\left( p(x,y) \,\|\, p(x)\,p(y) \right) \\ &amp;= \int p(x,y)\, \log \frac{p(x,y)}{p(x)\,p(y)}\,dx \end{align} \]</span></p><h3 id="invariant-under-reparameterization">Invariant Under Reparameterization</h3><p>If <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span> are homeormophism, then <span class="math display">\[ I(X;Y) = I(f(X);f(Y)) \]</span> that is, mutual information is invariant under reparameterization <span class="citation" data-cites="brakelLearningIndependentFeatures2017">(Brakel and Bengio <a href="#ref-brakelLearningIndependentFeatures2017" role="doc-biblioref">2017</a>)</span>.</p><h2 id="estimators">Estimators</h2><h3 id="infonce">InfoNCE</h3><p>(to be completed)</p><h3 id="mine">MINE</h3><p><span class="citation" data-cites="belghaziMineMutualInformation2018">Belghazi et al. (<a href="#ref-belghaziMineMutualInformation2018" role="doc-biblioref">2018</a>)</span> proposed a mutual information estimator based on the Donsker-Varadhan dual representation <span class="citation" data-cites="donskerAsymptoticEvaluationCertain1983">(Donsker and Varadhan <a href="#ref-donskerAsymptoticEvaluationCertain1983" role="doc-biblioref">1983</a>)</span>. <span class="math display">\[ \begin{align} I(X;Z) &amp;\geq \sup_{T_{\theta}} \left\{ \mathbb{E}_{p(x,z)}\left[T_{\psi}(x,z)\right] - \log \left( \mathbb{E}_{p(x)\,p(z)}\left[e^{T_{\psi}(x,z)}\right] \right) \right\} \\ &amp;\approx \sup_{T_{\theta}} \left\{ \frac{1}{b} \sum_{i=1}^b\left[T_{\psi}(x^{(i)},z^{(i)})\right] - \log \left( \frac{1}{b} \sum_{i=1}^b\left[e^{T_{\psi}(x^{(i)},\tilde{z}^{(i)})}\right] \right) \right\} \end{align} \]</span> where <span class="math inline">\(x^{(i)}, z^{(i)} \sim p(x,z)\)</span>, and <span class="math inline">\(\tilde{z}^{(i)} \sim p(z)\)</span>. <span class="math inline">\(b\)</span> is the batch size.</p><p>The gradient of MINE estimator might have too large a scale, in which situation the <a href="/Deep_Learning/Optimization/Gradient_Tricks/#adaptive-clipping">Adaptive Clipping</a> might be used. Also, <span class="citation" data-cites="belghaziMineMutualInformation2018">Belghazi et al. (<a href="#ref-belghaziMineMutualInformation2018" role="doc-biblioref">2018</a>)</span> proposed to correct the bias of the gradient estimator by an additional moving average term.</p><h3 id="deep-infomax">Deep InfoMax</h3><p><span class="citation" data-cites="hjelmLearningDeepRepresentations2018">Hjelm et al. (<a href="#ref-hjelmLearningDeepRepresentations2018" role="doc-biblioref">2018</a>)</span> proposed a mutual information estimator based on the JSD estimator, via <a href="/Deep_Learning/Generative_Adversarial_Nets/f-GAN/">f-GAN</a> formulation: <span class="math display">\[ \mathcal{I}^{(JSD)}(X;Z) = \mathbb{E}_{p(x,z)}\left[ -\text{sp}(-T_{\psi}(x,z)) \right] - \mathbb{E}_{p(x)\,p(z)}\left[ \text{sp}(T_{\psi}(x,z)) \right] \]</span> where <span class="math inline">\(\text{sp}(a) = \log(1 + e^a)\)</span>.</p><p>For encoder architecture <span class="math inline">\(z = E_{\theta}(x)\)</span> and <span class="math inline">\(p(x) = p_d(x)\)</span>, it can also be formulated as: <span class="math display">\[ \begin{align} \mathcal{I}^{(JSD)}(X;Z) &amp;= \mathbb{E}_{\mathbb{P}}\left[ -\text{sp}(-T_{\psi}(x,E_{\theta}(x))) \right] - \mathbb{E}_{\mathbb{P}\times\tilde{\mathbb{P}}}\left[ \text{sp}(T_{\psi}(\tilde{x},E_{\theta}(x))) \right] \\ &amp;\approx \frac{1}{b} \sum_{i=1}^b \left[ -\text{sp}\left(-T_{\psi}(x^{(i)},E_{\theta}(x^{(i)}))\right) \right] - \frac{1}{b} \sum_{i=1}^b \left[ \text{sp}\left({T_{\psi}(\tilde{x}^{(i)},E_{\theta}(x^{(i)}))}\right) \right] \end{align} \]</span> where <span class="math inline">\(\mathbb{P} = \tilde{\mathbb{P}} = p(x)\)</span>. <span class="math inline">\(x^{(i)}\)</span> and <span class="math inline">\(\tilde{x}^{(i)}\)</span> are samples from <span class="math inline">\(\mathbb{P}\)</span> and <span class="math inline">\(\tilde{\mathbb{P}}\)</span>, respectively. <span class="math inline">\(b\)</span> is the batch size.</p><p><strong>Note: Deep InfoMax actually has much more content then summarized here. The architecture, the global/local deep information maximum and the prior matching techniques can be referred to the original paper.</strong></p><h2 id="mutual-information-as-regularizers">Mutual Information as Regularizers</h2><h3 id="avoid-mode-collapse-in-gan">Avoid Mode Collapse in GAN</h3><p>Maximizing the entropy <span class="math inline">\(H(X) = H(G(Z))\)</span> of a GAN generator could help avoid mode collapse. <span class="citation" data-cites="chenInfoGANInterpretableRepresentation2016 belghaziMineMutualInformation2018 kumarMaximumEntropyGenerators2019">(Chen et al. <a href="#ref-chenInfoGANInterpretableRepresentation2016" role="doc-biblioref">2016</a>; Belghazi et al. <a href="#ref-belghaziMineMutualInformation2018" role="doc-biblioref">2018</a>; Kumar et al. <a href="#ref-kumarMaximumEntropyGenerators2019" role="doc-biblioref">2019</a>)</span></p><p>Since <span class="math inline">\(X=G(Z)\)</span> is a deterministic mapping, we have <span class="math inline">\(H(G(Z)|Z) \equiv 0\)</span>, and therefore: <span class="math display">\[ H(G(Z)) = I(G(Z);Z) - H(G(Z)|Z) = I(G(Z);Z) \]</span></p><h3 id="bound-the-reconstruction-error">Bound the Reconstruction Error</h3><p>Given the encoder <span class="math inline">\(q(\mathbf{z}|\mathbf{x})\)</span> and decoder <span class="math inline">\(p(\mathbf{x}|\mathbf{z})\)</span>, and the prior distribution <span class="math inline">\(p(\mathbf{z})\)</span>, assuming <span class="math inline">\(q(\mathbf{x}) = p(\mathbf{x}) = p_d(\mathbf{x})\)</span>, then the reconstruction error <span class="math inline">\(\mathcal{R}\)</span> defined as: <span class="math display">\[ \mathcal{R} = \mathbb{E}_{p_d(\mathbf{x})} \mathbb{E}_{q(\mathbf{z}|\mathbf{x})}\left[ -\log p(\mathbf{x}|\mathbf{z}) \right] \]</span> can be decomposed as: <span class="math display">\[ \begin{align} \mathcal{R} &amp;= \mathbb{E}_{q(\mathbf{x},\mathbf{z})}\left[ -\log p(\mathbf{x}|\mathbf{z}) \right] \\ &amp;= \mathbb{E}_{q(\mathbf{x},\mathbf{z})}\left[ -\log p(\mathbf{x},\mathbf{z}) + \log p(\mathbf{z}) \right] \\ &amp;= \mathbb{E}_{q(\mathbf{x},\mathbf{z})}\left[ \log \frac{q(\mathbf{x},\mathbf{z})}{p(\mathbf{x},\mathbf{z})} - \log q(\mathbf{z},\mathbf{x}) + \log p(\mathbf{z}) \right] \\ &amp;= D_{\mathrm{KL}}(q(\mathbf{x},\mathbf{z})\,\|\,p(\mathbf{x},\mathbf{z})) + H_{q}(Z,X) + \mathbb{E}_{q(\mathbf{x},\mathbf{z})}\left[ \log p(\mathbf{z}) \right] \end{align} \]</span> where <span class="math display">\[ \begin{align} \mathbb{E}_{q(\mathbf{x},\mathbf{z})}\left[ \log p(\mathbf{z}) \right] &amp;= \iint q(\mathbf{x},\mathbf{z}) \log p(\mathbf{z}) \,\mathrm{d}\mathbf{z}\,\mathrm{d}\mathbf{x} \\ &amp;= \int \left( \int q(\mathbf{x},\mathbf{z}) \,\mathrm{d}\mathbf{x} \right)\log p(\mathbf{z}) \,\mathrm{d}\mathbf{z} \\ &amp;= \mathbb{E}_{q(\mathbf{z})}\left[ \log p(\mathbf{z}) \right] \\ &amp;= \mathbb{E}_{q(\mathbf{z})}\left[ -\log \frac{q(\mathbf{z})}{p(\mathbf{z})} + \log q(\mathbf{z}) \right] \\ &amp;= -D_{\mathrm{KL}}(q(\mathbf{z})\,\|\,p(\mathbf{z})) - H_{q}(Z) \end{align} \]</span> and since <span class="math display">\[ H_q(Z,X) - H_q(Z) = H_q(Z|X) = H_q(Z) - I_q(Z|X) \]</span> we have <span class="citation" data-cites="belghaziMineMutualInformation2018">(Belghazi et al. <a href="#ref-belghaziMineMutualInformation2018" role="doc-biblioref">2018</a>)</span>: <span class="math display">\[ \begin{align} \mathcal{R} &amp;= D_{\mathrm{KL}}(q(\mathbf{x},\mathbf{z})\,\|\,p(\mathbf{x},\mathbf{z})) + H_{q}(Z,X) - D_{\mathrm{KL}}(q(\mathbf{z})\,\|\,p(\mathbf{z})) - H_{q}(Z) \\ &amp;= D_{\mathrm{KL}}(q(\mathbf{x},\mathbf{z})\,\|\,p(\mathbf{x},\mathbf{z})) - D_{\mathrm{KL}}(q(\mathbf{z})\,\|\,p(\mathbf{z})) + H_q(Z) - I_q(Z|X) \end{align} \]</span></p><p>As long as <span class="math inline">\(q(\mathbf{z})\)</span> is trained to match <span class="math inline">\(p(\mathbf{z})\)</span>, maximizing <span class="math inline">\(I_q(Z|X)\)</span> can be an efficient way to bound the reconstruction loss.</p><h3 id="information-bottleneck">Information Bottleneck</h3><p>For a latent variable model <span class="math inline">\(X \to Z \to Y\)</span>, mutual information can be served as a regularizer to limit the amount of information passed through <span class="math inline">\(Z\)</span>, leaving only the most relevant part reaching the output variable <span class="math inline">\(Y\)</span>.</p><p>Minimizing the Information Bottleneck Lagrangian for encoder <span class="math inline">\(q(z|x)\)</span> can serve the constraint: <span class="math display">\[ \mathcal{L}(q(z|x)) = H(Y|Z) + \beta I(X,Z) \]</span></p><h1 id="bibliography" class="unnumbered">References</h1><div id="refs" class="references" role="doc-bibliography"><div id="ref-belghaziMineMutualInformation2018"><p>Belghazi, Mohamed Ishmael, Aristide Baratin, Sai Rajeswar, Sherjil Ozair, Yoshua Bengio, Aaron Courville, and R. Devon Hjelm. 2018. “Mine: Mutual Information Neural Estimation.” <em>arXiv Preprint arXiv:1801.04062</em>.</p></div><div id="ref-brakelLearningIndependentFeatures2017"><p>Brakel, Philemon, and Yoshua Bengio. 2017. “Learning Independent Features with Adversarial Nets for Non-Linear Ica.” <em>arXiv Preprint arXiv:1710.05050</em>.</p></div><div id="ref-chenInfoGANInterpretableRepresentation2016"><p>Chen, Xi, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. 2016. “InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets.” In <em>Advances in Neural Information Processing Systems 29</em>, edited by D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, 2172–80. Curran Associates, Inc.</p></div><div id="ref-donskerAsymptoticEvaluationCertain1983"><p>Donsker, Monroe D., and SR Srinivasa Varadhan. 1983. “Asymptotic Evaluation of Certain Markov Process Expectations for Large Time. IV.” <em>Communications on Pure and Applied Mathematics</em> 36 (2): 183–212.</p></div><div id="ref-hjelmLearningDeepRepresentations2018"><p>Hjelm, R. Devon, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam Trischler, and Yoshua Bengio. 2018. “Learning Deep Representations by Mutual Information Estimation and Maximization.” <em>arXiv Preprint arXiv:1808.06670</em>.</p></div><div id="ref-kumarMaximumEntropyGenerators2019"><p>Kumar, Rithesh, Anirudh Goyal, Aaron Courville, and Yoshua Bengio. 2019. “Maximum Entropy Generators for Energy-Based Models.” <em>arXiv:1901.08508 [Cs, Stat]</em>, January. <a href="http://arxiv.org/abs/1901.08508" target="_blank" rel="noopener">http://arxiv.org/abs/1901.08508</a>.</p></div></div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;definition&quot;&gt;Definition&lt;/h2&gt;&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[ \begin{align} I(X;Y) &amp;amp;= D_{\mathrm{KL}}\left( p(x,y) \,\|\, p(x)\,p(
      
    
    </summary>
    
      <category term="Deep_Learning" scheme="https://wiki.haowen-xu.com/categories/Deep-Learning/"/>
    
      <category term="Information_Theoretical" scheme="https://wiki.haowen-xu.com/categories/Deep-Learning/Information-Theoretical/"/>
    
    
  </entry>
  
  <entry>
    <title>KL-Divergence</title>
    <link href="https://wiki.haowen-xu.com/Deep_Learning/Information_Theoretical/KL-Divergence/"/>
    <id>https://wiki.haowen-xu.com/Deep_Learning/Information_Theoretical/KL-Divergence/</id>
    <published>2020-03-16T18:56:07.000Z</published>
    <updated>2020-06-01T12:15:59.008Z</updated>
    
    <content type="html"><![CDATA[<h2 id="definition">Definition</h2><p><span class="math display">\[ \begin{align} D_{\mathrm{KL}}\left( p\|q \right) &amp;= \int p(x) \log\frac{p(x)}{q(x)} \,dx \\ &amp;= -\int p(x) \log\frac{q(x)}{p(x)} \,dx \end{align} \]</span></p><p>Measures the divergence between two distributions: <span class="math inline">\(p(x)\)</span> and <span class="math inline">\(q(x)\)</span>. It is always non-negative, since: <span class="math display">\[ D_{\mathrm{KL}}\left( p\|q \right) = -\int p(x) \log\frac{q(x)}{p(x)} \,dx \geq -\log \int p(x) \frac{q(x)}{p(x)} = 0 \]</span></p><h3 id="kl-divergence-is-not-symmetric">KL-Divergence is Not Symmetric</h3><p>There is a fact that <span class="math display">\[ D_{\mathrm{KL}}\left( p\|q \right) \neq D_{\mathrm{KL}}\left( q\|p \right) \]</span> To train model distribution <span class="math inline">\(q(x)\)</span> according to data distribution <span class="math inline">\(p(x)\)</span>, <span class="math inline">\(D_{\mathrm{KL}}\left( p\|q \right)\)</span> will deduce a <em>spread</em>-out <span class="math inline">\(q(x)\)</span> as follows:</p><figure><img src="/Deep_Learning/Information_Theoretical/KL-Divergence/KL_Divergence_p_over_q.png" id="fig:KL_Divergence_p_over_q" data-max-height="350px" alt="" style="max-height:350px"><figcaption>Figure 1: <span class="math inline">\(D_{\mathrm{KL}}\left( p\|q \right)\)</span> will deduce a <em>spread</em>-out <span class="math inline">\(q(x)\)</span><a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></figcaption></figure><p>Conversely, the reverse KL <span class="math inline">\(D_{\mathrm{KL}}\left( q\|p \right)\)</span> will deduce a <span class="math inline">\(q(x)\)</span> concentrated at some modes of <span class="math inline">\(p(x)\)</span>:</p><figure><img src="/Deep_Learning/Information_Theoretical/KL-Divergence/KL_Divergence_q_over_p.png" id="fig:KL_Divergence_q_over_p" data-max-height="350px" alt="" style="max-height:350px"><figcaption>Figure 2: <span class="math inline">\(D_{\mathrm{KL}}\left( p\|q \right)\)</span> will deduce a <span class="math inline">\(q(x)\)</span> concentrated at some modes of <span class="math inline">\(p(x)\)</span></figcaption></figure><h2 id="estimate-kl-divergence-with-classifier">Estimate KL-Divergence with Classifier</h2><h3 id="theory">Theory</h3><p>Define a classifier: <span class="math display">\[ p(x\in p|x) = \frac{1}{1+\exp(-r(x))} \]</span> then for the mixed training data distribution: <span class="math display">\[ \tilde{p}(x) = 0.5 (p(x) + q(x)) \]</span> the <span class="math inline">\(r^{\star}(x)\)</span> for the optimal classifier <span class="math inline">\(p^\star(x\in p|x)\)</span> is: <span class="math display">\[ \begin{align} p^\star(x\in p|x) &amp;= \frac{p(x)}{p(x)+q(x)} \\ r^\star(x) &amp;= \log \frac{p(x)}{q(x)} \end{align} \]</span> Thus we can use the learned <span class="math inline">\(r^{\star}(x)\)</span> to estimate the KL-divergence: <span class="math display">\[ D_{\mathrm{KL}}\left( p\|q \right) = \mathbb{E}_{p(x)}\left[ r^{\star}(x) \right] \]</span></p><h3 id="training-objective">Training objective</h3><p>Maximizing <span class="math inline">\(\mathcal{L}\)</span> can obtain <span class="math inline">\(r(x) = D_{\mathrm{KL}}\left( p\|q \right)\)</span>, where <span class="math inline">\(\mathcal{L}\)</span> is formulated as: <span class="math display">\[ \begin{align} \mathcal{L} &amp;= \mathbb{E}_{\tilde{p}(x)}\left[ I(x\in p) \log p(x\in p|x) + (1 - I(x \in p)) \log (1 - p(x\in p|x)) \right] \\ &amp;= \mathbb{E}_{p(x)}\left[ \log \frac{1}{1+\exp(-r(x))} \right] + \mathbb{E}_{q(x)}\left[ \log \frac{\exp(-r(x))}{1+\exp(-r(x))} \right] \\ &amp;= -\mathbb{E}_{p(x)}\left[ \log \left( 1+\exp(-r(x)) \right) \right] - \mathbb{E}_{q(x)}\left[ \log \left( 1+\exp(r(x)) \right) \right] \end{align} \]</span></p><h2 id="the-donsker-varadhan-representation">The Donsker-Varadhan representation</h2><p><span class="citation" data-cites="donskerAsymptoticEvaluationCertain1983">Donsker and Varadhan (<a href="#ref-donskerAsymptoticEvaluationCertain1983" role="doc-biblioref">1983</a>)</span> proposed a dual representation of the KL-Divergence: <span class="math display">\[ \mathrm{D}_{KL}(p\|q) = \sup_{T:\Omega \to \mathbb{R}} \left\{ \mathbb{E}_{p}[T(\mathbf{x})] - \log\left( \mathbb{E}_{q}[e^{T(x,y)}] \right) \right\} \]</span></p><p>And also, the following inequilty holds for <span class="math inline">\(T \in \mathcal{F}\)</span>, a specific family of functions: <span class="math display">\[ \mathrm{D}_{KL}(p\|q) \geq \sup_{T \in \mathcal{F}} \left\{ \mathbb{E}_{p}[T(\mathbf{x})] - \log\left( \mathbb{E}_{q}[e^{T(x,y)}] \right) \right\} \]</span> where the inequilty is tight for <span class="math inline">\(T^*\)</span> satisfying: <span class="math display">\[ p(x) \,dx = \frac{1}{Z} e^{T^*(x,y)}\,q(x)\,dx, \quad \text{where } Z = \mathbb{E}_{q}\left[e^{T^*(x,y)}\right] \]</span></p><h1 id="bibliography" class="unnumbered">References</h1><div id="refs" class="references" role="doc-bibliography"><div id="ref-donskerAsymptoticEvaluationCertain1983"><p>Donsker, Monroe D., and SR Srinivasa Varadhan. 1983. “Asymptotic Evaluation of Certain Markov Process Expectations for Large Time. IV.” <em>Communications on Pure and Applied Mathematics</em> 36 (2): 183–212.</p></div></div><section class="footnotes" role="doc-endnotes"><hr><ol><li id="fn1" role="doc-endnote"><p>Figures from <a href="https://wiseodd.github.io/techblog/2016/12/21/forward-reverse-kl/" target="_blank" rel="noopener">https://wiseodd.github.io/techblog/2016/12/21/forward-reverse-kl/</a>.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li></ol></section>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;definition&quot;&gt;Definition&lt;/h2&gt;&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[ \begin{align} D_{\mathrm{KL}}\left( p\|q \right) &amp;amp;= \int p(x) \log\f
      
    
    </summary>
    
      <category term="Deep_Learning" scheme="https://wiki.haowen-xu.com/categories/Deep-Learning/"/>
    
      <category term="Information_Theoretical" scheme="https://wiki.haowen-xu.com/categories/Deep-Learning/Information-Theoretical/"/>
    
    
  </entry>
  
  <entry>
    <title>Graph Auto-Encoder</title>
    <link href="https://wiki.haowen-xu.com/Deep_Learning/Graph_Neural_Networks/Graph_Auto_Encoder/"/>
    <id>https://wiki.haowen-xu.com/Deep_Learning/Graph_Neural_Networks/Graph_Auto_Encoder/</id>
    <published>2020-03-11T23:38:10.000Z</published>
    <updated>2020-06-01T12:15:59.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="auto-encoding-the-feature-matrix-given-adjacency-matrix">Auto-Encoding the Feature Matrix Given Adjacency Matrix</h2><p>The task is to learn the encoder <span class="math inline">\(f\)</span> and the decoder <span class="math inline">\(g\)</span> that: <span class="math display">\[ \begin{align} \mathbf{Z} &amp;= f(\mathbf{A},\mathbf{X}) \\ \tilde{\mathbf{X}} &amp;= g(\mathbf{A},\mathbf{Z}) \end{align} \]</span></p><h3 id="graph-u-net">Graph U-Net</h3><figure><img src="/Deep_Learning/Graph_Neural_Networks/Graph_Auto_Encoder/Graph_U-Net.png" id="fig:Graph_U-Net" data-max-height="600px" alt="" style="max-height:600px"><figcaption>Figure 1: Graph U-Net (view <a href="/Deep_Learning/Graph_Neural_Networks/Graph_Auto_Encoder/Graph_U-Net.pdf">pdf</a>)</figcaption></figure><figure><img src="/Deep_Learning/Graph_Neural_Networks/Graph_Auto_Encoder/Graph_U-Net_gUnpool.png" id="fig:Graph_U-Net_gUnpool" data-max-height="600px" alt="" style="max-height:600px"><figcaption>Figure 2: gUnpool (view <a href="/Deep_Learning/Graph_Neural_Networks/Graph_Auto_Encoder/Graph_U-Net_gUnpool.pdf">pdf</a>)</figcaption></figure><p><span class="citation" data-cites="gaoGraphUNets2019">Gao and Ji (<a href="#ref-gaoGraphUNets2019" role="doc-biblioref">2019</a>)</span> proposed the <em>gPool</em>, and <em>gUnpool</em> as the building blocks of graph auto-encoder. It also mimics the hierarchical structure of U-Net <span class="citation" data-cites="ronnebergerUNetConvolutionalNetworks2015">(Ronneberger, Fischer, and Brox <a href="#ref-ronnebergerUNetConvolutionalNetworks2015" role="doc-biblioref">2015</a>)</span>, thus named as Graph U-Net.</p><ul><li><p>gPool: is similar to that proposed by <span class="citation" data-cites="cangeaSparseHierarchicalGraph2018">Cangea et al. (<a href="#ref-cangeaSparseHierarchicalGraph2018" role="doc-biblioref">2018</a>)</span>, see <a href="/Deep_Learning/Graph_Neural_Networks/Graph_Convolution_Network/#gpool">Graph Convolution Network</a>.</p></li><li><p>gUnpool: the gUnpool layer that restores the input of a gUnpool layer can be formulated as; <span class="math display">\[ \tilde{\mathbf{X}}^{(l)} = \mathop{\mathrm{distribute}}\left( \mathbf{X}^{(l+1)}, \mathbf{idx}^{(l)} \right) \]</span> where <span class="math inline">\(\mathbf{X}^{(l+1)}\)</span> is output of the <span class="math inline">\(l\)</span>-th gPool layer, <span class="math inline">\(\mathbf{idx}^{(l)}\)</span> is the top-<span class="math inline">\(k\)</span> nodes of the <span class="math inline">\(l\)</span>-th gPool layer. The function <span class="math inline">\(\text{distribute}(\cdot)\)</span> assigns back each row of <span class="math inline">\(\mathbf{X}^ {(l+1)}\)</span> to the <span class="math inline">\(\tilde{\mathbf{X}}^{(l)}\)</span>, according to the selection index <span class="math inline">\(\mathbf{idx}^{(l)}\)</span>. The shape of <span class="math inline">\(\tilde{\mathbf{X}}^{(l)}\)</span> is the same as the input of the <span class="math inline">\(l\)</span>-th gPool layer (i.e., <span class="math inline">\(\mathbf{X}^{(l)}\)</span>). The un-assigned rows are zero vectors.</p></li></ul><h1 id="bibliography" class="unnumbered">References</h1><div id="refs" class="references" role="doc-bibliography"><div id="ref-cangeaSparseHierarchicalGraph2018"><p>Cangea, Cătălina, Petar Veličković, Nikola Jovanović, Thomas Kipf, and Pietro Liò. 2018. “Towards Sparse Hierarchical Graph Classifiers.” <em>arXiv Preprint arXiv:1811.01287</em>.</p></div><div id="ref-gaoGraphUNets2019"><p>Gao, Hongyang, and Shuiwang Ji. 2019. “Graph U-Nets.” <em>arXiv:1905.05178 [Cs, Stat]</em>, May. <a href="http://arxiv.org/abs/1905.05178" target="_blank" rel="noopener">http://arxiv.org/abs/1905.05178</a>.</p></div><div id="ref-ronnebergerUNetConvolutionalNetworks2015"><p>Ronneberger, Olaf, Philipp Fischer, and Thomas Brox. 2015. “U-Net: Convolutional Networks for Biomedical Image Segmentation.” <em>arXiv:1505.04597 [Cs]</em>, May. <a href="http://arxiv.org/abs/1505.04597" target="_blank" rel="noopener">http://arxiv.org/abs/1505.04597</a>.</p></div></div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;auto-encoding-the-feature-matrix-given-adjacency-matrix&quot;&gt;Auto-Encoding the Feature Matrix Given Adjacency Matrix&lt;/h2&gt;&lt;p&gt;The task is 
      
    
    </summary>
    
      <category term="Deep_Learning" scheme="https://wiki.haowen-xu.com/categories/Deep-Learning/"/>
    
      <category term="Graph_Neural_Networks" scheme="https://wiki.haowen-xu.com/categories/Deep-Learning/Graph-Neural-Networks/"/>
    
    
  </entry>
  
  <entry>
    <title>Image Segmentation</title>
    <link href="https://wiki.haowen-xu.com/Deep_Learning/CV/Image_Segmentation/"/>
    <id>https://wiki.haowen-xu.com/Deep_Learning/CV/Image_Segmentation/</id>
    <published>2020-03-11T17:39:40.000Z</published>
    <updated>2020-06-01T12:15:58.996Z</updated>
    
    <content type="html"><![CDATA[<h2 id="overview">Overview</h2><p>For each pixel <span class="math inline">\(x_{ij}\)</span> on an image, predict its segmentation class <span class="math inline">\(c_{ij}\)</span>.</p><h2 id="supervised-methods">Supervised Methods</h2><h3 id="fully-convolutional-networks-for-semantic-segmentation">Fully convolutional networks for semantic segmentation</h3><figure><img src="/Deep_Learning/CV/Image_Segmentation/Fully_Convolutional_Networks_for_Semantic_Segmentation.png" id="fig:Fully_Convolutional_Networks_for_Semantic_Segmentation" data-max-height="480px" alt="" style="max-height:480px"><figcaption>Figure 1: The Architecture of "Fully convolutional networks for semantic segmentation" (view <a href="/Deep_Learning/CV/Image_Segmentation/Fully_Convolutional_Networks_for_Semantic_Segmentation.pdf">pdf</a>)</figcaption></figure><p><span class="citation" data-cites="longFullyConvolutionalNetworks2015">Long, Shelhamer, and Darrell (<a href="#ref-longFullyConvolutionalNetworks2015" role="doc-biblioref">2015</a>)</span> proposed to use deconvolutional layers to up-sample intermediate feature maps at different levels from a pre-trained convolutional neural network, in order to compose the pixel-wise classification output.</p><h3 id="u-net">U-Net</h3><figure><img src="/Deep_Learning/CV/Image_Segmentation/U-Net.png" id="fig:U-Net" data-max-height="480px" alt="" style="max-height:480px"><figcaption>Figure 2: The Architecture of "U-Net" (view <a href="/Deep_Learning/CV/Image_Segmentation/U-Net.pdf">pdf</a>)</figcaption></figure><ul><li>Hierarchical deconvolution at different levels.</li><li>Weighted cross-entropy loss: to balance the loss of different classes, and to enforce the net to put emphasis on the cell boundaries. <span class="math display">\[ \begin{align} \mathcal{L} &amp;= \sum w(\mathbf{x}) \log p(\mathbf{x}) \\ w(\mathbf{x}) &amp;= w_c(\mathbf{x}) + w_0 \cdot \exp \left( -\frac{d_1(\mathbf{x}) + d_2(\mathbf{x})^2}{2 \sigma^2} \right) \end{align} \]</span> where <span class="math inline">\(w_c(\mathbf{x})\)</span> is the normalizing weight for the class of the pixel, while <span class="math inline">\(w_0\)</span> is a hyper-parameter. <span class="math inline">\(d_1\)</span> and <span class="math inline">\(d_2\)</span> is the distance from the background pixel <span class="math inline">\(\mathbf{x}\)</span> to the closest and second closest cell.</li></ul><h2 id="evaluation">Evaluation</h2><h3 id="metrics">Metrics</h3><p>Let <span class="math inline">\(n_{ij}\)</span> be the number of pixels of class <span class="math inline">\(i\)</span> being predicted to belong to class <span class="math inline">\(j\)</span>. Suppose there are <span class="math inline">\(k\)</span> different classes, and <span class="math inline">\(t_i = \sum_{j} n_{ij}\)</span> be the total number of pixels of class <span class="math inline">\(i\)</span>. Then we have the following metrics for image segmentation <span class="citation" data-cites="longFullyConvolutionalNetworks2015">(Long, Shelhamer, and Darrell <a href="#ref-longFullyConvolutionalNetworks2015" role="doc-biblioref">2015</a>)</span>:</p><ul><li><p>Pixel accuracy <span class="math display">\[ \text{Pixel Acc} = \frac{\sum_i n_{ii}}{\sum_i t_i} \]</span></p></li><li><p>Mean accuracy <span class="math display">\[ \text{Mean Acc} = \frac{1}{k} \sum_i \frac{n_{ii}}{t_i} \]</span></p></li><li><p>Mean IoU (Intersection over Union): <span class="math display">\[ \text{Mean IoU} = \frac{1}{k} \sum_{i} \frac{n_ii}{t_i + \sum_j n_{ji} - n_{ii}} \]</span></p></li><li><p>Weighted IoU <span class="math display">\[ \text{Weighted IoU} = \frac{1}{\sum_j t_j} \cdot\frac{\sum_i t_i n_{ii}}{t_i + \sum_j n_{ji} -n_{ii}} \]</span></p></li></ul><h1 id="bibliography" class="unnumbered">References</h1><div id="refs" class="references" role="doc-bibliography"><div id="ref-longFullyConvolutionalNetworks2015"><p>Long, Jonathan, Evan Shelhamer, and Trevor Darrell. 2015. “Fully Convolutional Networks for Semantic Segmentation.” In <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>, 3431–40.</p></div></div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;&lt;p&gt;For each pixel &lt;span class=&quot;math inline&quot;&gt;\(x_{ij}\)&lt;/span&gt; on an image, predict its segmentation class &lt;sp
      
    
    </summary>
    
      <category term="Deep_Learning" scheme="https://wiki.haowen-xu.com/categories/Deep-Learning/"/>
    
      <category term="CV" scheme="https://wiki.haowen-xu.com/categories/Deep-Learning/CV/"/>
    
    
  </entry>
  
  <entry>
    <title>Graph Convolution Network</title>
    <link href="https://wiki.haowen-xu.com/Deep_Learning/Graph_Neural_Networks/Graph_Convolution_Network/"/>
    <id>https://wiki.haowen-xu.com/Deep_Learning/Graph_Neural_Networks/Graph_Convolution_Network/</id>
    <published>2020-03-07T19:02:45.000Z</published>
    <updated>2020-06-01T12:15:59.004Z</updated>
    
    <content type="html"><![CDATA[<h2 id="overview">Overview</h2><p>To spread the node features across the graph, according to the graph structure (typically local connectivity among the nodes).</p><p>The features after applying the <span class="math inline">\(l\)</span>-th graph convolution layer can be denoted as <span class="math inline">\(\mathbf{H}^{(l)}\)</span>, where it should be a <span class="math inline">\(n \times k\)</span> matrix, with <span class="math inline">\(n\)</span> being the number of nodes in the graph, and <span class="math inline">\(k\)</span> being the number of feature dimensions. <span class="math inline">\(\mathbf{h}^{(l)}_i\)</span> refers to the <span class="math inline">\(i\)</span>-th row of <span class="math inline">\(\mathbf{H}^{(l)}\)</span>, denoting the feature of the <span class="math inline">\(i\)</span>-th node after applying the <span class="math inline">\(l\)</span>-th layer.</p><p>The connectivity among the nodes can be represented by the adjacency matrix <span class="math inline">\(\mathbf{A}\)</span>, an <span class="math inline">\(n \times n\)</span> matrix, where <span class="math inline">\(\mathbf{A}_{ij}\)</span> representing the connectivity from node <span class="math inline">\(j\)</span> to <span class="math inline">\(i\)</span>. For undirected graph, <span class="math inline">\(\mathbf{A}\)</span> should be symmetric.</p><h2 id="feature-matrix">Feature Matrix</h2><p>Every node in the graph requires an initial feature matrix, for GCN layers to propagate along the connections between nodes. Besides using the task-specific features, one may also use:</p><ul><li>Node degree information <span class="citation" data-cites="cangeaSparseHierarchicalGraph2018">(Cangea et al. <a href="#ref-cangeaSparseHierarchicalGraph2018" role="doc-biblioref">2018</a>)</span>: one-hot encoding the node degree for all degrees up to a given upper bound.</li><li>One-hot index vector <span class="citation" data-cites="yaoGraphConvolutionalNetworks2019">(Yao, Mao, and Luo <a href="#ref-yaoGraphConvolutionalNetworks2019" role="doc-biblioref">2019</a>)</span>: one-hot encoding the index of nodes.</li></ul><h2 id="graph-convolution">Graph Convolution</h2><h3 id="basic-formulation">Basic Formulation</h3><p>The GCN layer can be formulated as: <span class="math display">\[ \mathbf{H}^{(l + 1)} = f(\hat{\mathbf{A}} \mathbf{H}^{(l)}\mathbf{W}^{(l)}) \]</span> where <span class="math inline">\(\hat{\mathbf{A}}\)</span> is the normalized adjacency matrix. A popular choice for undirected graph may be <span class="citation" data-cites="wuComprehensiveSurveyGraph2019">(Wu et al. <a href="#ref-wuComprehensiveSurveyGraph2019" role="doc-biblioref">2019</a>)</span>: <span class="math display">\[ \hat{\mathbf{A}} = \mathbf{D}^{-1/2} \mathbf{A} \,\mathbf{D}^{-1/2} \]</span> where <span class="math inline">\(\mathbf{D}_{ii} = \sum_j \mathbf{A}_{ij}\)</span> is the degree of node <span class="math inline">\(i\)</span>.</p><p>Another choice for <span class="math inline">\(\hat{\mathbf{A}}\)</span>, which not only can be used for undirected grpah, but also can be used for directed ones, can be formulated as: <span class="math display">\[ \hat{\mathbf{A}} = \mathbf{D}^{-1} \mathbf{A} \]</span> Some work may add auxiliary self-loop to the adjacency matrix, such that the normalized adjacency matrix becomes: <span class="math display">\[ \hat{\mathbf{A}} = \mathbf{D}^{-1/2} (\mathbf{A} + \lambda\,\mathbf{I}) \,\mathbf{D}^{-1/2} \]</span> or (used by <span class="citation" data-cites="zhangEndtoendDeepLearning2018">Zhang et al. (<a href="#ref-zhangEndtoendDeepLearning2018" role="doc-biblioref">2018</a>)</span>): <span class="math display">\[ \hat{\mathbf{A}} = \mathbf{D}^{-1} (\mathbf{A} + \lambda\,\mathbf{I}) \]</span></p><h3 id="hetergeneous-graph-with-various-edge-relationship">Hetergeneous Graph with Various Edge Relationship</h3><p>If the link between nodes represent different relationships (<span class="math inline">\(R = \{r\}\)</span>), each relationship may have their own kernel <span class="math inline">\(\mathbf{W}_r^{(l)}\)</span>, such that the GCN layer can be formulated as <span class="citation" data-cites="schlichtkrullModelingRelationalData2017">(Schlichtkrull et al. <a href="#ref-schlichtkrullModelingRelationalData2017" role="doc-biblioref">2017</a>)</span>: <span class="math display">\[ \mathbf{h}^{(l+1)}_i = f\left(\sum_{r \in R} \sum_{j \in \mathcal{N}^r_i} \frac{1}{c_{ij}} \mathbf{W}_r^{(l)} \mathbf{h}^{(l)}_j + \mathbf{W}_0^{(l)} \mathbf{h}^{(l)}_i \right) \]</span></p><p>where <span class="math inline">\(c_{ij} = \left| \mathcal{N}_i^r \right|\)</span> is the normalizing factor.</p><h3 id="hetergeneuos-graph-with-various-node-type">Hetergeneuos Graph with Various Node Type</h3><p>If the node are grouped into different types, i.e., node <span class="math inline">\(v_i\)</span> has type <span class="math inline">\(t_i\)</span>, then each type can have their own kernel <span class="math inline">\(\mathbf{W}_{t}^{(l)}\)</span>, and the GCN layer can be formulated as: <span class="math display">\[ \begin{align} \mathbf{h}^{(l+1)}_i &amp;= f\left( \sum_{j \in \mathcal{N}_{i}} c_{ij} \, g(\mathbf{h}_j^{(l)}) + g(\mathbf{h}_i^{(l)}) \right) \\ g(\mathbf{h}_i^{(l)}) &amp;= \mathbf{W}_{t_i}^{(l)} \mathbf{h}_i^{(l)} \end{align} \]</span> as is proposed by <span class="citation" data-cites="wangHeterogeneousAttributedNetwork2019">Wang et al. (<a href="#ref-wangHeterogeneousAttributedNetwork2019" role="doc-biblioref">2019</a>)</span>. For the normalizing factor <span class="math inline">\(c_{ij}\)</span>, <span class="citation" data-cites="wangHeterogeneousAttributedNetwork2019">Wang et al. (<a href="#ref-wangHeterogeneousAttributedNetwork2019" role="doc-biblioref">2019</a>)</span> also proposed to learn the attention score by self-attention: <span class="math display">\[ \begin{align} c_{ij}^{(l)} &amp;= \frac{\exp\left( s_{ij} \right)}{\sum_{\tilde{j} \in \mathcal{N}_i^{(l)}} \exp\left( s_{i\tilde{j}} \right)} \\ s_{ij} &amp;= g(\mathbf{h}_i^{(l)})^{\top} \cdot g(\mathbf{h}_j^{(l)}) \end{align} \]</span> where <span class="math inline">\(s_{ij}\)</span> is the attention score.</p><h3 id="lstm-aggregation-with-random-permuation-of-neighborhood-nodes">LSTM Aggregation with Random Permuation of Neighborhood Nodes</h3><p><span class="citation" data-cites="hamiltonInductiveRepresentationLearning2017">Hamilton, Ying, and Leskovec (<a href="#ref-hamiltonInductiveRepresentationLearning2017" role="doc-biblioref">2017</a>)</span> proposed a candidate neighborhoold information aggregation method, based on LSTM with random permutation of neighborhood nodes.</p><h3 id="neighborhood-sampling-instead-of-summation">Neighborhood Sampling Instead of Summation</h3><p><span class="citation" data-cites="hamiltonInductiveRepresentationLearning2017">Hamilton, Ying, and Leskovec (<a href="#ref-hamiltonInductiveRepresentationLearning2017" role="doc-biblioref">2017</a>)</span> proposed to use neighborhood sampling to reduce the training computation time.</p><h3 id="monte-carlo-approximation-instead-of-summation">Monte-Carlo Approximation Instead of Summation</h3><p><span class="citation" data-cites="chenFastGCNFastLearning2018">Chen, Ma, and Xiao (<a href="#ref-chenFastGCNFastLearning2018" role="doc-biblioref">2018</a>)</span> proposed to view the summation over nodes within one GCN layer as an expectation, formulated as: <span class="math display">\[ \mathbf{h}^{(l+1)}(v) = f\left( \int \hat{\mathbf{A}}(v,u) \,\mathbf{h}^{(l)}(u)\,\mathbf{W}^{(l)}\,dP(u) \right) \]</span> where <span class="math inline">\(u\)</span> and <span class="math inline">\(v\)</span> represent the nodes. <span class="math inline">\(P(u)\)</span> is regarded as a uniform distribution. According to this formulation, the authors proposed to use a proposal distribution: <span class="math display">\[ q(u) = \left\| \hat{\mathbf{A}}(:,u) \right\|^2 / \sum_{u'} \left\| \hat{\mathbf{A}}(:,u') \right\|^2 \]</span> which is the power of node <span class="math inline">\(u\)</span>'s out-degree, divided by the sum of the power of all nodes' out-degree. The proposal distribution is used to sample <span class="math inline">\(t_l\)</span> i.i.d. nodes for each GCN layer, namely <span class="math inline">\(u_1^{(l)}, \dots, u_{t_l}^{(l)}\)</span>, and estimate <span class="math inline">\(\mathbf{h}^{(l+1)}\)</span> by: <span class="math display">\[ \mathbf{h}^{(l+1)}(v) \approx f\left( \frac{1}{t_l}\sum_{j=1}^{t_l} \frac{\hat{\mathbf{A}(v,u_j^{(l)})}\,\mathbf{h}^{(l)}(u_j^{(l)})\,\mathbf{W}^{(l)}}{q(u_j^{(l)})} \right) \]</span></p><h2 id="learning-the-weight-of-message-passing">Learning the Weight of Message Passing</h2><h3 id="self-attention">Self-Attention</h3><p>Instead of relying on the fixed adjacency matrix to weight the neighborhood information in graph convolution networks, <span class="citation" data-cites="velickovicGraphAttentionNetworks2017">Veličković et al. (<a href="#ref-velickovicGraphAttentionNetworks2017" role="doc-biblioref">2017</a>)</span> proposed to use a self-attention mechanism to weight the neighborhood information.</p><p>The <em>attention coefficients</em> is calculated by: <span class="math display">\[ e_{ij} = a\left(\mathbf{W} \mathbf{h}_i, \mathbf{W} \mathbf{h}_j\right) \]</span> where <span class="math inline">\(a(\cdot)\)</span> is the attention network. <span class="math inline">\(e_{ij}\)</span> is formulated by <span class="citation" data-cites="velickovicGraphAttentionNetworks2017">Veličković et al. (<a href="#ref-velickovicGraphAttentionNetworks2017" role="doc-biblioref">2017</a>)</span> as follows: <span class="math display">\[ e_{ij} = a\left(\mathbf{W} \mathbf{h}_i, \mathbf{W} \mathbf{h}_j\right) = \mathrm{LeakyReLU}\left({\mathbf{a}}^{\top}\left[ \mathbf{W} \mathbf{h}_i \big\| \mathbf{W} \mathbf{h}_j \right]\right) \]</span> This attention score is further normalized as: <span class="math display">\[ \alpha_{ij} = \frac{\exp\left(e_{ij}\right)}{\sum_{k \in \mathcal{N}_i} \exp\left(e_{ik}\right)} = \frac{\exp\left(\mathrm{LeakyReLU}\left({\mathbf{a}}^{\top}\left[ \mathbf{W} \mathbf{h}_i \big\| \mathbf{W} \mathbf{h}_j \right]\right)\right)}{\sum_{k \in \mathcal{N}_i} \exp\left(\mathrm{LeakyReLU}\left({\mathbf{a}}^{\top}\left[ \mathbf{W} \mathbf{h}_i \big\| \mathbf{W} \mathbf{h}_k \right]\right)\right)} \]</span> The final output features of this layer is: <span class="math display">\[ \mathbf{h}' = \sigma\left( \sum_{j \in \mathcal{N}_i} \alpha_{ij} \mathbf{W} \mathbf{h}_j \right) \]</span> If using <em>mult-head attention</em>, the output features can be formulated as either one of the following (i.e., <em>concat</em> and <em>average</em> multi-head attention): <span class="math display">\[ \begin{align} \mathbf{h}' &amp;= \mathop{\bigg\|}_{k=1}^K \sigma\left( \sum_{j \in \mathcal{N}_i} \alpha^k_{ij} \mathbf{W}^k \mathbf{h}_j \right) \\ \mathbf{h}' &amp;= \sigma\left( \frac{1}{K} \sum_{k=1}^K \sum_{j \in \mathcal{N}_i} \alpha^k_{ij} \mathbf{W}^k \mathbf{h}_j \right) \end{align} \]</span></p><p>where the second formulation is adopted as the output layer.</p><h2 id="node-embedding">Node Embedding</h2><p>The feature vector <span class="math inline">\(\mathbf{h}^{(L)}\)</span> after <span class="math inline">\(L\)</span>-th GCN layer can be used as the node embedding <span class="citation" data-cites="hamiltonInductiveRepresentationLearning2017 schlichtkrullModelingRelationalData2017">(Hamilton, Ying, and Leskovec <a href="#ref-hamiltonInductiveRepresentationLearning2017" role="doc-biblioref">2017</a>; Schlichtkrull et al. <a href="#ref-schlichtkrullModelingRelationalData2017" role="doc-biblioref">2017</a>)</span>. The unsupervised loss, using negative sampling, should be: <span class="math display">\[ \mathcal{L} = \sum_{v_i \in V} \sum_{v_j \in N(v_i)} \left[ \log \frac{\exp(\mathbf{h}^{(L)}_i \cdot \mathbf{h}^{(L)}_j)}{\exp(\mathbf{h}^{(L)}_i \cdot \mathbf{h}^{(L)}_j) + 1} + \sum_{\tilde{v}_n \sim q(v) \\ n = 1 \dots k} \log\frac{1}{\exp(\mathbf{h}^{(L)}_i \cdot \tilde{\mathbf{h}}^{(L)}_n) + 1} \right] \]</span> same as the <a href="/Deep_Learning/Graph_Neural_Networks/Node_Embedding/#undirected-graph">negative sampling</a> method for node embedding. This embedding loss can be used along in a fully unsupervised task; or as a augumented/regularization term for a supervised task, along with the main objective for the specific task <span class="citation" data-cites="hamiltonInductiveRepresentationLearning2017">(Hamilton, Ying, and Leskovec <a href="#ref-hamiltonInductiveRepresentationLearning2017" role="doc-biblioref">2017</a>)</span>.</p><h2 id="pooling">Pooling</h2><h3 id="gpool">gPool</h3><p><span class="citation" data-cites="cangeaSparseHierarchicalGraph2018">Cangea et al. (<a href="#ref-cangeaSparseHierarchicalGraph2018" role="doc-biblioref">2018</a>)</span> proposed the <em>projection score</em> to sort the nodes, and choose the top-<span class="math inline">\(\lceil kN \rceil\)</span> nodes after pooling. <span class="math inline">\(k \in (0, 1]\)</span> is the <em>pooling ratio</em>, while <span class="math inline">\(N\)</span> is the numberr of input nodes. The projection score <span class="math inline">\(\mathbf{y}\)</span> and the pooling method is formulated as: <span class="math display">\[ \mathbf{Z} = \frac{\mathbf{X} \mathbf{p}}{\left\| \mathbf{p} \right\|_2} \qquad \overrightarrow{i} = \mathop{\text{top-rank}}\left( \mathbf{Z}, \lceil kN \rceil \right) \qquad \mathbf{X}' = \left( \mathbf{X} \odot \tanh(\mathbf{Z}) \right)_{\overrightarrow{i}} \qquad \mathbf{A}' = \mathbf{A}_{\overrightarrow{i},\overrightarrow{i}} \]</span> where <span class="math inline">\(\mathbf{X}\)</span> is feature matrix of input nodes, <span class="math inline">\(\mathbf{p}\)</span> is the learnable parameter for the projection. <span class="math inline">\(\cdot_{\overrightarrow{i}}\)</span> is the indexing operation.</p><p><span class="citation" data-cites="cangeaSparseHierarchicalGraph2018">Cangea et al. (<a href="#ref-cangeaSparseHierarchicalGraph2018" role="doc-biblioref">2018</a>)</span> proposed a similar architecture, named <em>gPool</em>, with <span class="math inline">\(\tanh(\mathbf{Z})\)</span> in <span class="math inline">\(\mathbf{X}' = \left( \mathbf{X} \odot \tanh(\mathbf{Z}) \right)_{\overrightarrow{i}}\)</span> replaced by <span class="math inline">\(\text{sigmoid}(\mathbf{\mathbf{Z}})\)</span>, mimic the often adopted gate mechanism.</p><p><strong>Note</strong>: <span class="math inline">\(\tanh(\mathbf{Z})\)</span> or <span class="math inline">\(\text{sigmoid}(\mathbf{Z})\)</span> enables the gradient to be passed along <span class="math inline">\(\mathbf{p}\)</span>. Without this term, <span class="math inline">\(\mathbf{p}\)</span> produces a solely discrete element selection, which cannot be trained by stochastic gradient descent.</p><h3 id="self-attention-graph-pooling">Self-Attention Graph Pooling</h3><p><span class="citation" data-cites="leeSelfattentionGraphPooling2019">Lee, Lee, and Kang (<a href="#ref-leeSelfattentionGraphPooling2019" role="doc-biblioref">2019</a>)</span> proposed to improve the projection score based pooling by adding a self-attention mechanism to calculate the projection score. The new projection score is derived as: <span class="math display">\[ \mathbf{Z} = f(\mathop{\mathrm{GNN}}(\mathbf{X},\mathbf{A})) \]</span> where <span class="math inline">\(f\)</span> is the activation function, <span class="math inline">\(\mathbf{X}\)</span> is the node feature matrix to be pooled, <span class="math inline">\(\mathbf{A}\)</span> is the adjacency matrix, which may be normalized. <span class="math inline">\(\mathop{\mathrm{GNN}}\)</span> indicates a general graph network, and authors proposed the following variants of this GNN network:</p><ul><li><span class="math inline">\(\mathbf{Z} = f\left(\tilde{\mathbf{D}}^{-1/2}\,\tilde{\mathbf{A}}\,\tilde{\mathbf{D}}^{1/2}\,\mathbf{X}\,\mathbf{p}\right)\)</span>, where <span class="math inline">\(\tilde{\mathbf{A}} = \mathbf{A}+\mathbf{I}\)</span> is the normalized adjacency matrix with self-loop, and <span class="math inline">\(\tilde{\mathbf{D}}\)</span> is the degree matrix of <span class="math inline">\(\tilde{\mathbf{A}}\)</span>.</li><li><span class="math inline">\(\mathbf{Z} = f\left(\mathop{\mathrm{GNN}}(\mathbf{X},\mathbf{A}+\mathbf{A}^2)\right)\)</span>, which considers the second-order neighborhood by augument 2-hop nodes in the adjacency matrix <span class="math inline">\(\mathbf{A}\)</span>.</li><li><span class="math inline">\(\mathbf{Z} = f\left(\mathop{\mathrm{GNN}_2}(\mathop{\mathrm{GNN}_1}(\mathbf{X},\mathbf{A}),\mathbf{A})\right)\)</span>, which considers the second-order neighborhood by stacking GNN layers.</li><li><span class="math inline">\(\mathbf{Z}=\frac{1}{M}\sum_{m} f\left( \mathop{\mathrm{GNN}_m}(\mathbf{X},\mathbf{A}) \right))\)</span>, which using an average of multiple attention scores.</li></ul><h3 id="hierarchical-pooling-vs-global-pooling">Hierarchical Pooling vs Global Pooling</h3><figure><img src="/Deep_Learning/Graph_Neural_Networks/Graph_Convolution_Network/Global_Pooling_vs_Hierarchical_Pooling.png" id="fig:Global_Pooling_vs_Hierarchical_Pooling" data-max-height="640px" alt="" style="max-height:640px"><figcaption>Figure 1: Global Pooling vs Hierarchical Pooling (view <a href="/Deep_Learning/Graph_Neural_Networks/Graph_Convolution_Network/Global_Pooling_vs_Hierarchical_Pooling.pdf">pdf</a>)</figcaption></figure><p><span class="citation" data-cites="leeSelfattentionGraphPooling2019">Lee, Lee, and Kang (<a href="#ref-leeSelfattentionGraphPooling2019" role="doc-biblioref">2019</a>)</span> found that hierarchical pooling seems to work better on large graphs, where global pooling works better on small graphs.</p><h2 id="readout">Readout</h2><h3 id="combine-the-global-average-and-max-pooling">Combine the Global Average and Max Pooling</h3><figure><img src="/Deep_Learning/Graph_Neural_Networks/Graph_Convolution_Network/Combine_the_Global_Average_and_Max_Pooling.png" id="fig:Combine_the_Global_Average_and_Max_Pooling" data-max-height="240px" alt="" style="max-height:240px"><figcaption>Figure 2: Combine the Global Average and Max Pooling (view <a href="/Deep_Learning/Graph_Neural_Networks/Graph_Convolution_Network/Combine_the_Global_Average_and_Max_Pooling.pdf">pdf</a>)</figcaption></figure><p><span class="citation" data-cites="cangeaSparseHierarchicalGraph2018">Cangea et al. (<a href="#ref-cangeaSparseHierarchicalGraph2018" role="doc-biblioref">2018</a>)</span> proposed a graph readout layer that combines both the <em>global average</em> and <em>global max</em> pooling techniques. Specifically, for the output graph of every <span class="math inline">\(l\)</span>-th pooling layer <span class="math inline">\((\mathbf{X}^{(l)}, \mathbf{A}^{(l)})\)</span>, the readout layer produces the summarized output by: <span class="math display">\[ \mathbf{s}^{(l)} = \mathop{\mathrm{Concat}}\left(\frac{1}{N^{(l)}} \sum_{i=1}^{N^{(l)}} \mathbf{x}^{(l)}_i \Bigg\|\max_{i=1}^{N^{(l)}} \mathbf{x}^{(l)}_i\right) \]</span> And the summarized output of all pooling layers <span class="math inline">\(\mathbf{s}^{(1)}, \dots, \mathbf{s}^{(L)}\)</span> are then summed together: <span class="math display">\[ \mathbf{s} = \sum_{l=1}^L \mathbf{s}^{(l)} \]</span> as the final aggregated feature of the graph. The whole structure is demonstrated as fig.&nbsp;<a href="#fig:Combine_the_Global_Average_and_Max_Pooling">2</a>.</p><h2 id="practical-gcn-architectures">Practical GCN Architectures</h2><h3 id="modeling-relational-data-with-graph-convolutional-networks-2017">Modeling Relational Data with Graph Convolutional Networks [2017]</h3><p><span class="citation" data-cites="schlichtkrullModelingRelationalData2017">Schlichtkrull et al. (<a href="#ref-schlichtkrullModelingRelationalData2017" role="doc-biblioref">2017</a>)</span> proposed the following formulation for their GCN layer with regarding to the node relationship: <span class="math display">\[ \mathbf{h}^{(l+1)}_i = f\left(\sum_r \sum_{j \in \mathcal{N}^r_i} \frac{1}{c_{ij}} \mathbf{W}_r^{(l)} \mathbf{h}^{(l)}_j + \mathbf{W}_0^{(l)} \mathbf{h}^{(l)}_i \right) \]</span> where the term <span class="math inline">\(\mathbf{W}_0^{(l)} \mathbf{h}^{(l)}_i\)</span> represents the information passed along the self-loop from <span class="math inline">\(l\)</span>-th layer to the <span class="math inline">\((l+1)\)</span>-layer. <span class="math inline">\(c_{ij}\)</span> is the normalizing factor, and is chosen to be <span class="math inline">\(\left| \mathcal{N}_i^r \right|\)</span> (i.e., the in-coming degree of node <span class="math inline">\(i\)</span>) in their paper.</p><p>To avoid having too many parameters, which may be potentially prone to over-fitting, <span class="citation" data-cites="schlichtkrullModelingRelationalData2017">Schlichtkrull et al. (<a href="#ref-schlichtkrullModelingRelationalData2017" role="doc-biblioref">2017</a>)</span> also proposed two methods to regularize the weights <span class="math inline">\(\mathbf{W}^{(l)}_r\)</span> of different relationships.</p><p>For link prediction, <span class="citation" data-cites="schlichtkrullModelingRelationalData2017">Schlichtkrull et al. (<a href="#ref-schlichtkrullModelingRelationalData2017" role="doc-biblioref">2017</a>)</span> proposed to use the GCN output at <span class="math inline">\(L\)</span>-th layer <span class="math inline">\(\mathbf{h}_i^{(L)}\)</span> as the embedding <span class="math inline">\(\mathbf{e}_i\)</span> of nodes, deriving the score function as: <span class="math display">\[ f(i,r,j) = \mathbf{e}_i^{\top}\mathbf{R}_r\mathbf{e}_j \]</span> where <span class="math inline">\(\mathbf{R}_r\)</span> is a learned diagonal matrix, to represent the "dot-product" between <span class="math inline">\(\mathbf{e}_i\)</span> and <span class="math inline">\(\mathbf{e}_j\)</span> under the relationship <span class="math inline">\(r\)</span>. The whole graph is then trained by <a href="/Deep_Learning/Confronting_Partition_Function/Softmax_Speedup/#nagative-sampling">negative sampling</a>. See the <a href="/Deep_Learning/Graph_Neural_Networks/Node_Embedding/#multi-relational-link">loss function</a> for this situation.</p><h1 id="bibliography" class="unnumbered">References</h1><div id="refs" class="references" role="doc-bibliography"><div id="ref-cangeaSparseHierarchicalGraph2018"><p>Cangea, Cătălina, Petar Veličković, Nikola Jovanović, Thomas Kipf, and Pietro Liò. 2018. “Towards Sparse Hierarchical Graph Classifiers.” <em>arXiv Preprint arXiv:1811.01287</em>.</p></div><div id="ref-chenFastGCNFastLearning2018"><p>Chen, Jie, Tengfei Ma, and Cao Xiao. 2018. “FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling.” <em>arXiv:1801.10247 [Cs]</em>, January. <a href="http://arxiv.org/abs/1801.10247" target="_blank" rel="noopener">http://arxiv.org/abs/1801.10247</a>.</p></div><div id="ref-hamiltonInductiveRepresentationLearning2017"><p>Hamilton, Will, Zhitao Ying, and Jure Leskovec. 2017. “Inductive Representation Learning on Large Graphs.” In <em>Advances in Neural Information Processing Systems</em>, 1024–34.</p></div><div id="ref-leeSelfattentionGraphPooling2019"><p>Lee, Junhyun, Inyeop Lee, and Jaewoo Kang. 2019. “Self-Attention Graph Pooling.” <em>arXiv Preprint arXiv:1904.08082</em>.</p></div><div id="ref-schlichtkrullModelingRelationalData2017"><p>Schlichtkrull, Michael, Thomas N. Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and Max Welling. 2017. “Modeling Relational Data with Graph Convolutional Networks.” <em>arXiv:1703.06103 [Cs, Stat]</em>, October. <a href="http://arxiv.org/abs/1703.06103" target="_blank" rel="noopener">http://arxiv.org/abs/1703.06103</a>.</p></div><div id="ref-velickovicGraphAttentionNetworks2017"><p>Veličković, Petar, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. 2017. “Graph Attention Networks.” <em>arXiv Preprint arXiv:1710.10903</em>.</p></div><div id="ref-wangHeterogeneousAttributedNetwork2019"><p>Wang, Yueyang, Ziheng Duan, Binbing Liao, Fei Wu, and Yueting Zhuang. 2019. “Heterogeneous Attributed Network Embedding with Graph Convolutional Networks.” In <em>Proceedings of the AAAI Conference on Artificial Intelligence</em>, 33:10061–2.</p></div><div id="ref-wuComprehensiveSurveyGraph2019"><p>Wu, Zonghan, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and Philip S. Yu. 2019. “A Comprehensive Survey on Graph Neural Networks.” <em>arXiv:1901.00596 [Cs, Stat]</em>, August. <a href="http://arxiv.org/abs/1901.00596" target="_blank" rel="noopener">http://arxiv.org/abs/1901.00596</a>.</p></div><div id="ref-yaoGraphConvolutionalNetworks2019"><p>Yao, Liang, Chengsheng Mao, and Yuan Luo. 2019. “Graph Convolutional Networks for Text Classification.” In <em>Proceedings of the AAAI Conference on Artificial Intelligence</em>, 33:7370–7.</p></div><div id="ref-zhangEndtoendDeepLearning2018"><p>Zhang, Muhan, Zhicheng Cui, Marion Neumann, and Yixin Chen. 2018. “An End-to-End Deep Learning Architecture for Graph Classification.” In <em>Thirty-Second AAAI Conference on Artificial Intelligence</em>.</p></div></div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;&lt;p&gt;To spread the node features across the graph, according to the graph structure (typically local connectivi
      
    
    </summary>
    
      <category term="Deep_Learning" scheme="https://wiki.haowen-xu.com/categories/Deep-Learning/"/>
    
      <category term="Graph_Neural_Networks" scheme="https://wiki.haowen-xu.com/categories/Deep-Learning/Graph-Neural-Networks/"/>
    
    
  </entry>
  
  <entry>
    <title>Node Embedding</title>
    <link href="https://wiki.haowen-xu.com/Deep_Learning/Graph_Neural_Networks/Node_Embedding/"/>
    <id>https://wiki.haowen-xu.com/Deep_Learning/Graph_Neural_Networks/Node_Embedding/</id>
    <published>2020-03-06T01:07:30.000Z</published>
    <updated>2020-06-01T12:15:59.008Z</updated>
    
    <content type="html"><![CDATA[<h2 id="overview">Overview</h2><p>A graph is defined by <span class="math inline">\(G = V \times E\)</span>, where <span class="math inline">\(V = \{v_i\}\)</span> is the set of all vertex (nodes), and <span class="math inline">\(E=\{e_i\}\)</span> is the set of all edges.</p><p>For directed graph, the task is often to learn one embedding for a particular node <span class="math inline">\(v_i\)</span>, namely, <span class="math inline">\(\mathbf{u}_i\)</span>.</p><p>For undirected graph, the task is often to learn two embeddings for a particular node <span class="math inline">\(v_i\)</span>, the out-going (source) embedding <span class="math inline">\(\mathbf{s}_i\)</span>, and the in-going (destination) embedding <span class="math inline">\(\mathbf{t}_i\)</span>.</p><h2 id="objective-for-learning-the-embedding">Objective for Learning the Embedding</h2><h3 id="learning-embedding-that-represents-the-local-connectivity-homophily-of-nodes">Learning Embedding that Represents the Local Connectivity (homophily) of Nodes</h3><h4 id="undirected-graph">Undirected Graph</h4><p>For undirected graph, the connectivity from <span class="math inline">\(v_i\)</span> to <span class="math inline">\(v_j\)</span> can be defined as the probability of predicting <span class="math inline">\(v_j\)</span> given <span class="math inline">\(v_i\)</span>: <span class="math display">\[ p(v_j|v_i) = \frac{\exp(\mathbf{u}_i \cdot \mathbf{u}_j)}{\sum_{v_n \in G} \exp(\mathbf{u}_i \cdot \mathbf{u}_n)} \]</span> The dot product can be seen as a score to indicate the connectivity from <span class="math inline">\(v_i\)</span> to <span class="math inline">\(v_j\)</span>. The training objective is to maximize <span class="math inline">\(p(v_j|v_i)\)</span> for every <span class="math inline">\(v_j\)</span> among the neighbors of <span class="math inline">\(v_i\)</span> (i.e., <span class="math inline">\(N(v_i)\)</span>, and all the nodes <span class="math inline">\(v_i\)</span> in <span class="math inline">\(V\)</span>. <span class="math display">\[ \mathcal{L} = \mathbb{E}_{v_i \in V} \mathbb{E}_{v_j \in N(v_i)} \left[ \log p(v_j|v_i) \right] = \mathbb{E}_{v_i \in V} \mathbb{E}_{v_j \in N(v_i)} \left[ \log\frac{\exp(\mathbf{u}_i \cdot \mathbf{u}_j)}{\sum_{v_n \in G} \exp(\mathbf{u}_i \cdot \mathbf{u}_n)} \right] \]</span> The partition function of <span class="math inline">\(p(v_j|v_i)\)</span> is hard to evaluate. For learning an embedding, <a href="/Deep_Learning/Confronting_Partition_Function/Softmax_Speedup/#negative_sampling">negative sampling</a> could be adopted to train the above objective. If <span class="math inline">\(p(v_i)\)</span> and <span class="math inline">\(p(v_j|v_i)\)</span> are uniform, we have: <span class="math display">\[ \tilde{\mathcal{L}} = \sum_{v_i \in V} \sum_{v_j \in N(v_i)} \left[ \log \frac{\exp(\mathbf{u}_i \cdot \mathbf{u}_j)}{\exp(\mathbf{u}_i \cdot \mathbf{u}_j) + 1} + \sum_{\tilde{v}_n \sim q(v) \\ n = 1 \dots k} \log\frac{1}{\exp(\mathbf{u}_i \cdot \tilde{\mathbf{u}}_n) + 1} \right] \]</span> where <span class="math inline">\(q(v)\)</span> is a noise distribution that samples negative node samples for a given pair <span class="math inline">\((v_i,v_j)\)</span>. <span class="math inline">\(k\)</span> controls the number of negative samples for each pair <span class="math inline">\((v_i,v_j)\)</span>. The noise distribution is usually also uniform.</p><h4 id="directed-graph">Directed Graph</h4><p>In directed graph, the connectivity from <span class="math inline">\(v_i\)</span> to <span class="math inline">\(v_j\)</span> can be defined as the probability of predicting <span class="math inline">\(v_j\)</span> given <span class="math inline">\(v_i\)</span>: <span class="math display">\[ p(v_j|v_i) = \frac{\exp(\mathbf{s}_i \cdot \mathbf{t}_j)}{\sum_{v_n \in G} \exp(\mathbf{s}_i \cdot \mathbf{t}_n)} \]</span></p><h4 id="multi-relational-link">Multi-Relational Link</h4><p>A relational matrix <span class="math inline">\(\mathbf{R}_r, \, r \in R\)</span> can be used to replace the dot-product in homogeneous graphs, which brings the probability of predicting <span class="math inline">\(v_j\)</span> given <span class="math inline">\(v_i\)</span>:</p><p><span class="math display">\[ p(v_j|v_i,r) = \frac{\exp(\mathbf{s}_i^{\top} \mathbf{R}_r \mathbf{t}_j)}{\sum_{\tilde{r} \in R, v_n \in G} \exp(\mathbf{s}_i^{\top} \mathbf{R}_{\tilde{r}} \mathbf{t}_n)} \]</span></p><p>The negative-sampling based training objective then becomes:</p><p><span class="math display">\[ \tilde{\mathcal{L}} = \sum_{v_i \in V} \sum_{r \in R} \sum_{v_j \in N_r(v_i)} \left[ \log \frac{\exp(\mathbf{u}_i^{\top} \mathbf{R}_r \mathbf{u}_j)}{\exp(\mathbf{u}_i^{\top} \mathbf{R}_r \mathbf{u}_j) + 1} + \sum_{\tilde{r}_n \in R \\ \tilde{v}_n \sim q_{\tilde{r}_n}(v) \\ n = 1 \dots k} \log\frac{1}{\exp(\mathbf{u}_i^{\top} \mathbf{R}_{\tilde{r}_n} \tilde{\mathbf{u}}_n) + 1} \right] \]</span></p><p>where <span class="citation" data-cites="yangEmbeddingEntitiesRelations2014">Yang et al. (<a href="#ref-yangEmbeddingEntitiesRelations2014" role="doc-biblioref">2014</a>)</span> and <span class="citation" data-cites="schlichtkrullModelingRelationalData2017">Schlichtkrull et al. (<a href="#ref-schlichtkrullModelingRelationalData2017" role="doc-biblioref">2017</a>)</span> used diagonal <span class="math inline">\(\mathbf{R}_r\)</span> to reduce the parameters.</p><h2 id="random-walk">Random Walk</h2><ul><li>Monte-Carlo End-Point sampling method</li><li>Balancing DFS and BFS<ul><li><a href="http://go.ipwx.me/zotero/MPUCA3Q8" target="_blank" rel="noopener">node2vec: Scalable Feature Learning for Networks</a></li></ul></li><li>Path sharing techniques: for a sampled random-walk path <span class="math inline">\(v_1, \dots, v_L\)</span>, consider that<ul><li>Suffixes: <span class="math inline">\(v_i, \dots, v_L\)</span> are valid paths.</li><li>Prefixes: <span class="math inline">\(v_1, \dots, v_i\)</span> are valid paths. [Powerwalk: Scalable personalized pagerank via random walks with vertexcentric decomposition.]</li><li>Sliding windows: <span class="math inline">\(v_i, \dots, v_{i+W}\)</span> are valid paths. <span class="citation" data-cites="groverNode2vecScalableFeature2016">(Grover and Leskovec <a href="#ref-groverNode2vecScalableFeature2016" role="doc-biblioref">2016</a>)</span></li></ul></li></ul><h1 id="bibliography" class="unnumbered">References</h1><div id="refs" class="references" role="doc-bibliography"><div id="ref-groverNode2vecScalableFeature2016"><p>Grover, Aditya, and Jure Leskovec. 2016. “Node2vec: Scalable Feature Learning for Networks.” <em>arXiv:1607.00653 [Cs, Stat]</em>, July. <a href="http://arxiv.org/abs/1607.00653" target="_blank" rel="noopener">http://arxiv.org/abs/1607.00653</a>.</p></div><div id="ref-schlichtkrullModelingRelationalData2017"><p>Schlichtkrull, Michael, Thomas N. Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and Max Welling. 2017. “Modeling Relational Data with Graph Convolutional Networks.” <em>arXiv:1703.06103 [Cs, Stat]</em>, October. <a href="http://arxiv.org/abs/1703.06103" target="_blank" rel="noopener">http://arxiv.org/abs/1703.06103</a>.</p></div><div id="ref-yangEmbeddingEntitiesRelations2014"><p>Yang, Bishan, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. 2014. “Embedding Entities and Relations for Learning and Inference in Knowledge Bases.” <em>arXiv Preprint arXiv:1412.6575</em>.</p></div></div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;&lt;p&gt;A graph is defined by &lt;span class=&quot;math inline&quot;&gt;\(G = V \times E\)&lt;/span&gt;, where &lt;span class=&quot;math inline&quot;
      
    
    </summary>
    
      <category term="Deep_Learning" scheme="https://wiki.haowen-xu.com/categories/Deep-Learning/"/>
    
      <category term="Graph_Neural_Networks" scheme="https://wiki.haowen-xu.com/categories/Deep-Learning/Graph-Neural-Networks/"/>
    
    
  </entry>
  
  <entry>
    <title>Directions to Explore</title>
    <link href="https://wiki.haowen-xu.com/Research_Work/Directions_to_Explore/"/>
    <id>https://wiki.haowen-xu.com/Research_Work/Directions_to_Explore/</id>
    <published>2020-03-05T00:59:49.000Z</published>
    <updated>2020-06-01T12:15:59.028Z</updated>
    
    <content type="html"><![CDATA[<div id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <div class="hbe-input-container">  <input type="password" id="hbePass" placeholder="" />    <label for="hbePass">Hey, password is required here.</label>    <div class="bottom-line"></div>  </div>  <script id="hbeData" type="hbeData" data-hmacdigest="ad3e49908b4fe90f7312c1eb657b597360552d91f8b27c76e7f22f5a3678d6f7">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</script></div><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
    
    <summary type="html">
    
      Here&#39;s something encrypted, password is required to continue reading.
    
    </summary>
    
      <category term="Research_Work" scheme="https://wiki.haowen-xu.com/categories/Research-Work/"/>
    
    
  </entry>
  
  <entry>
    <title>Softmax Speedup</title>
    <link href="https://wiki.haowen-xu.com/Deep_Learning/Confronting_Partition_Function/Softmax_Speedup/"/>
    <id>https://wiki.haowen-xu.com/Deep_Learning/Confronting_Partition_Function/Softmax_Speedup/</id>
    <published>2020-03-04T02:59:36.000Z</published>
    <updated>2020-06-01T12:15:59.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="problem-statement">Problem Statement</h2><p>To optimize <span class="math inline">\(\theta\)</span> for a softmax classification model <span class="math inline">\(p(y|\mathbf{x};\theta)\)</span> (or formulated as <span class="math inline">\(p(w|\mathbf{c};\theta)\)</span> in many NLP literature) without estimating the partition function <span class="math inline">\(Z(\theta)\)</span>:</p><p><span class="math display">\[ \begin{align} p(y=i|\mathbf{x};\theta) &amp;= \frac{\tilde{p}(y=i|\mathbf{x};\theta)}{Z(\theta)} \\ Z(\mathbf{x};\theta) &amp;= \sum_{j=1}^m \tilde{p}(y=j|\mathbf{x};\theta) \end{align} \]</span> where the unnormalized <span class="math inline">\(\tilde{p}(y|\mathbf{x};\theta)\)</span> is typically formulated as <span class="math inline">\(\tilde{p}(y=i|\mathbf{x};\theta)=\exp\left( u_i(\mathbf{x},y;\theta) \right)\)</span>, where <span class="math inline">\(i=1 \dots m\)</span> is the <span class="math inline">\(k\)</span> classes.</p><h2 id="hierarchical-softmax">Hierarchical Softmax</h2><p>Encode each class <span class="math inline">\(y\)</span> by a binary sequence <span class="math inline">\(b_1(y), b_2(y), \dots, b_k(y)\)</span> , where <span class="math inline">\(b_i(y) \in \{0, 1\}\)</span>, and decompose <span class="math inline">\(p(y|\mathbf{x};\theta)\)</span> as: <span class="math display">\[ p(y|\mathbf{x};\theta) = \prod_{i=1}^k p(b_i(y)|b_{i-1}(y),\dots,b_1(y),\mathbf{x};\theta) \]</span> such that each of the decomposed term is a binary classifier.</p><h3 id="further-speedup">Further Speedup</h3><p>Hoffman coding can be used to construct the binary tree, such that more frequently visited classes <span class="math inline">\(y\)</span> can have a shorter binary sequence.</p><h2 id="noise-contrastive-estimation">Noise Contrastive Estimation</h2><p>The NCE method starts by choosing a noise distribution <span class="math inline">\(q(y)\)</span>, and modify the original <span class="math inline">\(m\)</span>-target classification problem as a binary classfication problem.</p><p>The binary classifier is defined as: <span class="math display">\[ \begin{align} p(D=1|\mathbf{x},y;\theta) &amp;= \frac{p(y|\mathbf{x};\theta)}{p(y|\mathbf{x};\theta) + k\, q(y)} = \frac{\tilde{p}(y|\mathbf{x};\theta) / Z(\mathbf{x};\theta)}{\tilde{p}(y|\mathbf{x};\theta) / Z(\mathbf{x};\theta) + k\, q(y)} \\ p(D=0|\mathbf{x},y;\theta) &amp;= \frac{k\, q(y)}{p(y|\mathbf{x};\theta) + k\, q(y)} = \frac{k\, q(y)}{\tilde{p}(y|\mathbf{x};\theta)/Z(\mathbf{x};\theta) + k\, q(y)} \end{align} \]</span> and the NCE objective (which should be <strong>maximized</strong>) is defined as: <span class="math display">\[ \begin{align} \mathcal{L}_{\mathrm{NCE}} &amp;= \mathbb{E}_{(\mathbf{x},y) \sim p_d(\mathbf{x},y)} \left[ \log p(D=1|\mathbf{x},y;\theta) + k\, \mathbb{E}_{\bar{y} \sim q(y)}\left[ \log p(D=0|\mathbf{x},\bar{y}) \right] \right] \\ &amp;\approx \sum_{\mathbf{x},y } \left[ \log p(D=1|\mathbf{x},y;\theta) + k\cdot\frac{1}{k}\sum_{j = 1 \dots k \\ \bar{y}_j \sim q(y)} \log p(D=0|\mathbf{x},\bar{y}_j;\theta) \right] \\ &amp;= \sum_{\mathbf{x},y } \left[ \log p(D=1|\mathbf{x},y;\theta) + \sum_{j = 1}^k \log p(D=0|\mathbf{x},\bar{y}_j;\theta) \right] \end{align} \]</span> where <span class="math inline">\(\sum_{\mathbf{x},y}\)</span> is a summation over the train data in a mini-batch, and <span class="math inline">\(\frac{1}{k}\sum_{j = 1 \dots k \\ \bar{y}_j \sim q(y)}\)</span> is a Monte Carlo estimator of <span class="math inline">\(\mathbb{E}_{\bar{y} \sim q(y)}\)</span>.</p><p>A necessary condition for NCE to work is that, <span class="math inline">\(q(y) \neq 0\)</span> wherever <span class="math inline">\(p(y|\mathbf{x};\theta) \neq 0\)</span>.</p><h3 id="the-gradient-of-nce-loss">The Gradient of NCE Loss</h3><p><span class="math display">\[ \begin{align} \frac{\partial \mathcal{L}_{\mathrm{NCE}}}{\partial \theta} &amp;= \frac{\partial}{\partial \theta} \mathbb{E}_{p_d(\mathbf{x},y)} \left[ \log \frac{p(y|\mathbf{x};\theta)}{p(y|\mathbf{x};\theta) + k\, q(y)} + k\, \mathbb{E}_{q(y)}\left[ \log \frac{k\, q(y)}{p(y|\mathbf{x};\theta) + k\, q(y)} \right] \right] \\ &amp;= \frac{\partial}{\partial \theta} \mathbb{E}_{p_d(\mathbf{x})} \left[ \mathbb{E}_{p_d(y|\mathbf{x})} \left[ \log \frac{p(y|\mathbf{x};\theta)}{p(y|\mathbf{x};\theta) + k\, q(y)} \right] + k\, \mathbb{E}_{q(y)}\left[ \log \frac{k\, q(y)}{p(y|\mathbf{x};\theta) + k\, q(y)} \right] \right] \\ &amp;= \mathbb{E}_{p_d(\mathbf{x})} \left[ \mathbb{E}_{p_d(y|\mathbf{x})} \left[ \frac{1}{p(y|\mathbf{x};\theta)} \cdot \frac{k\,q(y)}{p(y|\mathbf{x};\theta) + k\, q(y)} \cdot \frac{\partial p(y|\mathbf{x};\theta)}{\partial \theta} \right] - k\, \mathbb{E}_{q(y)}\left[ \frac{1}{p(y|\mathbf{x};\theta) + k\, q(y)} \cdot \frac{\partial p(y|\mathbf{x};\theta)}{\partial \theta} \right] \right] \\ &amp;= \mathbb{E}_{p_d(\mathbf{x})} \left[ \mathbb{E}_{p_d(y|\mathbf{x})} \left[ \frac{k\,q(y)}{p(y|\mathbf{x};\theta) + k\, q(y)} \cdot \frac{\partial \log p(y|\mathbf{x};\theta)}{\partial \theta} \right] - k\, \mathbb{E}_{q(y)}\left[ \frac{p(y|\mathbf{x};\theta)}{p(y|\mathbf{x};\theta) + k\, q(y)} \cdot \frac{\partial \log p(y|\mathbf{x};\theta)}{\partial \theta} \right] \right] \\ &amp;= \mathbb{E}_{p_d(\mathbf{x})} \sum_y \left[ \frac{k\,q(y)}{p(y|\mathbf{x};\theta) + k\, q(y)} \cdot \bigg( p_d(y|\mathbf{x}) - p(y|\mathbf{x};\theta) \bigg) \cdot \frac{\partial \log p(y|\mathbf{x};\theta)}{\partial \theta} \right] \end{align} \]</span></p><p>Note that:</p><p><span class="math display">\[ \sum_y \left[ p(y|\mathbf{x};\theta) \cdot \frac{\partial \log p(y|\mathbf{x};\theta)}{\partial \theta} \right] = \sum_y \frac{\partial p(y|\mathbf{x};\theta)}{\partial \theta} = \frac{\partial}{\partial \theta} \sum_y p(y|\mathbf{x};\theta) = \frac{\partial 1}{\partial \theta} = 0 \]</span></p><p>Thus, when <span class="math inline">\(k \to \infty\)</span>, <span class="math inline">\(\frac{\partial \mathcal{L}_{\mathrm{NCE}}}{\partial \theta}\)</span> equals to <span class="math inline">\(\frac{\partial}{\partial \theta} \mathbb{E}_{p_d(\mathbf{x},y)}\left[ \log p(y|\mathbf{x};\theta) \right]\)</span>, which is the gradient for the original classification problem.</p><h3 id="the-global-optimum-of-nce-loss">The Global Optimum of NCE Loss</h3><p><em>(This section is added by me. The original NCE paper <span class="citation" data-cites="gutmannNoisecontrastiveEstimationNew2010">(Gutmann and Hyvärinen <a href="#ref-gutmannNoisecontrastiveEstimationNew2010" role="doc-biblioref">2010</a>)</span> has a similar theorem, but did not give the proof. I use Euler's formula from Calculus of Variations, but in fact the condition of using this theorem is not strictly satisfied. Thus this proof may only be seen as a discussion.)</em> <span class="math display">\[ \begin{align} \frac{\partial \mathcal{L}_{\mathrm{NCE}}}{\partial p(y|\mathbf{x};\theta)} &amp;= \mathbb{E}_{p_d(\mathbf{x})} \sum_y \left[ \frac{k\,q(y)}{p(y|\mathbf{x};\theta) + k\,q(y)} \cdot \bigg( p_d(y|\mathbf{x}) - p(y|\mathbf{x};\theta) \bigg) \cdot \frac{1}{p(y|\mathbf{x};\theta)} \right] \end{align} \]</span> According to Euler's formula: <span class="math display">\[ \begin{align} \frac{\partial \mathcal{L}_{\mathrm{NCE}}}{\partial p(y|\mathbf{x};\theta)} = 0 &amp;\Leftrightarrow \frac{k\,q(y)}{p(y|\mathbf{x};\theta) + k\,q(y)} \cdot \bigg( p_d(y|\mathbf{x}) - p(y|\mathbf{x};\theta) \bigg) \cdot \frac{1}{p(y|\mathbf{x};\theta)} = 0 \end{align} \]</span> Since <span class="math inline">\(q(y)\)</span> is arbitrary, the only solution for <span class="math inline">\(\mathcal{L}_{\mathrm{NCE}}\)</span> to attains its global optimum is: <span class="math display">\[ p(y|\mathbf{x};\theta) \equiv p_d(y|\mathbf{x}) \]</span></p><h3 id="dealing-with-zmathbfxtheta">Dealing with <span class="math inline">\(Z(\mathbf{x};\theta)\)</span></h3><p>For NCE, <span class="math inline">\(Z(\mathbf{x};\theta)\)</span> can be learned instead of estimated. To avoid having extra free parameters, <span class="citation" data-cites="mnihFastSimpleAlgorithm2012">Mnih and Teh (<a href="#ref-mnihFastSimpleAlgorithm2012" role="doc-biblioref">2012</a>)</span> suggested to let <span class="math inline">\(Z(\mathbf{x};\theta) \equiv 1\)</span>, and found it works well. Having a fixed <span class="math inline">\(Z(\mathbf{x};\theta) \equiv 1\)</span> will likely to induce a normalized <span class="math inline">\(p(y|\mathbf{x};\theta)\)</span>.</p><h2 id="negative-sampling">Negative Sampling</h2><p>Negative sampling <span class="citation" data-cites="mikolovDistributedRepresentationsWords2013">(Mikolov et al. <a href="#ref-mikolovDistributedRepresentationsWords2013" role="doc-biblioref">2013</a>)</span> can be seen as a simplified version of noise contrastive estimation. The binary classifier is defined as: <span class="math display">\[ \begin{align} p(D=1|\mathbf{x},y;\theta) &amp;= \frac{p(y|\mathbf{x};\theta)}{p(y|\mathbf{x};\theta) + 1} \\ p(D=0|\mathbf{x},y;\theta) &amp;= \frac{1}{p(y|\mathbf{x};\theta) + 1} \end{align} \]</span> that is, <span class="math inline">\(q(y) \equiv \frac{1}{k}\)</span>. The loss is derived as: <span class="math display">\[ \begin{align} \mathcal{L}_{\mathrm{NEG}} &amp;= \mathbb{E}_{(\mathbf{x},y) \sim p_d(\mathbf{x},y)} \left[ \log p(D=1|\mathbf{x},y;\theta) + \mathbb{E}_{\bar{y} \sim q(y)}\left[ \log p(D=0|\mathbf{x},\bar{y}) \right] \right] \\ &amp;= \mathbb{E}_{p_d(\mathbf{x})} \left[ \mathbb{E}_{ p(y|\mathbf{x};\theta)}\left[ \log \frac{p(y|\mathbf{x};\theta)}{p(y|\mathbf{x};\theta) + 1} \right] + k\,\mathbb{E}_{q(y)}\left[ \log \frac{1}{p(y|\mathbf{x};\theta) + 1} \right] \right] \\ &amp;\approx \frac{1}{N} \sum_{\mathbf{x},y } \left[ \log \frac{p(y|\mathbf{x};\theta)}{p(y|\mathbf{x};\theta) + 1} + \sum_{j = 1}^k \log \frac{1}{p(\bar{y}_j|\mathbf{x};\theta) + 1} \right] \end{align} \]</span></p><h3 id="the-normalizing-factor">The Normalizing Factor</h3><p>The negative sampling does not train a properly normalized <span class="math inline">\(p(y|\mathbf{x};\theta)\)</span> <span class="citation" data-cites="dyerNotesNoiseContrastive2014">(Dyer <a href="#ref-dyerNotesNoiseContrastive2014" role="doc-biblioref">2014</a>)</span>, unless <span class="math inline">\(k = m\)</span>, where <span class="math inline">\(q(y) \equiv \frac{1}{m}\)</span> is a normalized uniform distribution, since: <span class="math display">\[ \begin{align} \mathcal{L}_{\mathrm{NEG}} &amp;= \mathbb{E}_{p_d(\mathbf{x})} \left[ \mathbb{E}_{ p_d(y|\mathbf{x})}\left[ \log \frac{p(y|\mathbf{x};\theta)}{p(y|\mathbf{x};\theta) + 1} \right] + k\,\mathbb{E}_{q(y)}\left[ \log \frac{1}{p(y|\mathbf{x};\theta) + 1} \right] \right] \\ &amp;= \mathbb{E}_{p_d(\mathbf{x})} \left[ \mathbb{E}_{ p_d(y|\mathbf{x}) }\left[ \log \frac{p(y|\mathbf{x};\theta) / \frac{m}{k}}{p(y|\mathbf{x};\theta) / \frac{m}{k} + k \cdot \left( \frac{1}{k} \cdot \frac{k}{m} \right)} \right] + k\,\mathbb{E}_{q(y)}\left[ \log \frac{k \cdot \left( \frac{1}{k} \cdot \frac{k}{m} \right)}{p(y|\mathbf{x};\theta) / \frac{m}{k} + k \cdot \left( \frac{1}{k} \cdot \frac{k}{m} \right)} \right] \right] \end{align} \]</span> <em>If we believe <span class="math inline">\(q(y) \equiv \frac{1}{m}\)</span> (which is often indeed the case for uniform sampling of the noise samples), then this should be a standard NCE loss, and the learned "probability distribution" <span class="math inline">\(p(y|\mathbf{x};\theta)\)</span> could potentially be converted into a truely normalized distribution, by scaling it with the normalizing factor <span class="math inline">\(Z(\mathbf{x};\theta) \equiv \frac{m}{k}\)</span>. (This assertion is made by me)</em></p><h1 id="bibliography" class="unnumbered">References</h1><div id="refs" class="references" role="doc-bibliography"><div id="ref-dyerNotesNoiseContrastive2014"><p>Dyer, Chris. 2014. “Notes on Noise Contrastive Estimation and Negative Sampling.” <em>arXiv Preprint arXiv:1410.8251</em>.</p></div><div id="ref-gutmannNoisecontrastiveEstimationNew2010"><p>Gutmann, Michael, and Aapo Hyvärinen. 2010. “Noise-Contrastive Estimation: A New Estimation Principle for Unnormalized Statistical Models.” In <em>Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</em>, 297–304.</p></div><div id="ref-mikolovDistributedRepresentationsWords2013"><p>Mikolov, Tomas, Ilya Sutskever, Kai Chen, Greg S. Corrado, and Jeff Dean. 2013. “Distributed Representations of Words and Phrases and Their Compositionality.” In <em>Advances in Neural Information Processing Systems</em>, 3111–9.</p></div><div id="ref-mnihFastSimpleAlgorithm2012"><p>Mnih, Andriy, and Yee Whye Teh. 2012. “A Fast and Simple Algorithm for Training Neural Probabilistic Language Models.” <em>arXiv:1206.6426 [Cs]</em>, June. <a href="http://arxiv.org/abs/1206.6426" target="_blank" rel="noopener">http://arxiv.org/abs/1206.6426</a>.</p></div></div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;problem-statement&quot;&gt;Problem Statement&lt;/h2&gt;&lt;p&gt;To optimize &lt;span class=&quot;math inline&quot;&gt;\(\theta\)&lt;/span&gt; for a softmax classification mod
      
    
    </summary>
    
      <category term="Deep_Learning" scheme="https://wiki.haowen-xu.com/categories/Deep-Learning/"/>
    
      <category term="Confronting_Partition_Function" scheme="https://wiki.haowen-xu.com/categories/Deep-Learning/Confronting-Partition-Function/"/>
    
    
  </entry>
  
  <entry>
    <title>Energy Function in Probabilistic Models</title>
    <link href="https://wiki.haowen-xu.com/Deep_Learning/Energy_Based_Models/Energy_Function_in_Probabilistic_Models/"/>
    <id>https://wiki.haowen-xu.com/Deep_Learning/Energy_Based_Models/Energy_Function_in_Probabilistic_Models/</id>
    <published>2020-02-28T22:43:51.000Z</published>
    <updated>2020-06-01T12:15:59.000Z</updated>
    
    <content type="html"><![CDATA[<p>This post summarizes the relationship between energy function and the deduced probabilistic model by a specified energy function.</p><h2 id="common-formulation">Common Formulation</h2><p>The probabilistic model deduced from an energy function can have the following formulations.</p><h3 id="gibbs-distribution">Gibbs Distribution</h3><p>Given an energy function <span class="math inline">\(U(\mathbf{x};\theta)\)</span> with parameters <span class="math inline">\(\theta\)</span>, the probability distribution can be deduced as: <span class="math display">\[ \begin{align} p(\mathbf{x};\theta) &amp;= \frac{1}{Z(\theta)}\,\exp\left( -U(\mathbf{x};\theta) \right) \\ Z(\theta) &amp;= \int \exp\left( -U(\mathbf{x};\theta) \right)\,\mathrm{d}\mathbf{x} \end{align} \]</span> The gradient of <span class="math inline">\(\mathbb{E}_{p_D(\mathbf{x})}\left[ -\log p(\mathbf{x};\theta) \right]\)</span> (i.e., the expectation of the negative log-likelihood <span class="math inline">\(-\log p(\mathbf{x})\)</span> over data distribution <span class="math inline">\(p_D(\mathbf{x})\)</span>) is then derived as: <span class="math display">\[ \begin{align} \nabla \mathbb{E}_{p_D(\mathbf{x};\theta)}\left[ -\log p(\mathbf{x};\theta) \right] &amp;= \mathbb{E}_{p_D(\mathbf{x})}\left[ -\nabla \log p(\mathbf{x};\theta) \right] \\ &amp;= \mathbb{E}_{p_D(\mathbf{x})} \left[ \nabla U(\mathbf{x};\theta) + \nabla \log Z(\theta) \right] \\ &amp;= \mathbb{E}_{p_D(\mathbf{x})} \left[ \nabla U(\mathbf{x};\theta) \right] + \nabla \log Z(\theta) \end{align} \]</span></p><p>where <span class="math inline">\(\nabla \log Z(\theta)\)</span> is: <span class="math display">\[ \begin{align} \nabla \log Z(\theta) &amp;= \frac{\nabla Z(\theta)}{Z(\theta)} \\ &amp;= \frac{1}{Z(\theta)} \int \nabla \exp\left( -U(\mathbf{x};\theta) \right)\,\mathrm{d}\mathbf{x} \\ &amp;= \int \frac{\exp\left( -U(\mathbf{x};\theta) \right) / Z(\theta)}{\exp\left( -U(\mathbf{x};\theta) \right)} \nabla \exp\left( -U(\mathbf{x};\theta) \right) \,\mathrm{d}\mathbf{x} \\ &amp;= \int p(\mathbf{x};\theta) \, \nabla \log \exp\left( -U(\mathbf{x};\theta) \right) \,\mathrm{d}\mathbf{x} \\ &amp;= -\int p(\mathbf{x};\theta) \, \nabla U(\mathbf{x};\theta) \,\mathrm{d}\mathbf{x} \\ &amp;= -\mathbb{E}_{p(\mathbf{x};\theta)} \left[ \nabla U(\mathbf{x};\theta) \right] \end{align} \]</span> thus the final gradient can be derived as: <span class="math display">\[ \nabla \mathbb{E}_{p_D(\mathbf{x};\theta)}\left[ -\log p(\mathbf{x};\theta) \right] = \mathbb{E}_{p_D(\mathbf{x})} \left[ \nabla U(\mathbf{x};\theta) \right] - \mathbb{E}_{p(\mathbf{x};\theta)} \left[ \nabla U(\mathbf{x};\theta) \right] \]</span></p><h4 id="positive-and-negative-phase">Positive and Negative Phase</h4><p>The above gradient consists of the positive phase term <span class="math inline">\(\mathbb{E}_{p_D(\mathbf{x})} \left[ \nabla U(\mathbf{x};\theta) \right]\)</span>, and the negative phase term <span class="math inline">\(\mathbb{E}_{p(\mathbf{x};\theta)} \left[ \nabla U(\mathbf{x};\theta) \right]\)</span>. The gradient reaches zero (which indicates a local minima) when these two terms are equal.</p><p><em>If the path of the gradient to <span class="math inline">\(\mathbb{E}_{p(\mathbf{x};\theta)}\)</span> is blocked (this sentence is added by me)</em>, then the positive phase term can be seen as minimizing the energy on "positive samples" from data distribution, and the negative phase can be seen as maximizing the energy on "negative samples" from model distribution. <span class="citation" data-cites="kimDeepDirectedGenerative2016">(Kim and Bengio <a href="#ref-kimDeepDirectedGenerative2016" role="doc-biblioref">2016</a>)</span></p><p>Sampling from <span class="math inline">\(\mathbb{E}_{p(\mathbf{x};\theta)}\)</span> often requires MCMC techniques, for example, the <a href="/Deep_Learning/Confronting_Partition_Function/Contrastive_Divergence/">Contrastive Divergence</a> algorithm.</p><h2 id="conditional-and-independence">Conditional and Independence</h2><h3 id="definition">Definition</h3><p>The distribution <span class="math inline">\(p(\mathbf{x},\mathbf{y},\mathbf{z})\)</span> deduced by energy function <span class="math inline">\(U(\mathbf{x},\mathbf{y},\mathbf{z})\)</span>: <span class="math display">\[ p(\mathbf{x},\mathbf{y},\mathbf{z}) = \frac{\exp\left( -U(\mathbf{x},\mathbf{y},\mathbf{z}) \right)}{\iiint \exp\left( -U(\mathbf{x}^*,\mathbf{y}^*,\mathbf{z}^*) \right) \, \mathrm{d}\mathbf{z}^* \,\mathrm{d}\mathbf{y}^* \,\mathrm{d}\mathbf{x}^*} \]</span> Also, the conditional distribution <span class="math inline">\(p(\mathbf{y},\mathbf{z}|\mathbf{x})\)</span> is defined as: <span class="math display">\[ p(\mathbf{y},\mathbf{z}|\mathbf{x}) = \frac{\exp\left( -U(\mathbf{x},\mathbf{y},\mathbf{z}) \right)}{\iint \exp\left( -U(\mathbf{x},\mathbf{y}^*,\mathbf{z}^*) \right) \, \mathrm{d}\mathbf{z}^* \,\mathrm{d}\mathbf{y}^*} \]</span></p><h3 id="theorem-1">Theorem 1</h3><p>If <span class="math inline">\(U(\mathbf{x},\mathbf{y},\mathbf{z}) = f(\mathbf{x},\mathbf{y}) + g(\mathbf{x},\mathbf{z}) + h(\mathbf{x})\)</span>, then: <span class="math display">\[ \begin{align} p(\mathbf{y}|\mathbf{x}) &amp;= \frac{\exp\left( -f(\mathbf{x},\mathbf{y}) \right)}{\int \exp\left( -f(\mathbf{x},\mathbf{y}^*) \right) \,\mathrm{d}\mathbf{y}^*} \\ p(\mathbf{z}|\mathbf{x}) &amp;= \frac{\exp\left( -g(\mathbf{x},\mathbf{z}) \right)}{\int \exp\left( -g(\mathbf{x},\mathbf{z}^*) \right) \,\mathrm{d}\mathbf{z}^*} \end{align} \]</span> <em>Proof</em>: <span class="math display">\[ \begin{align} p(\mathbf{y}|\mathbf{x}) &amp;= \int p(\mathbf{x},\mathbf{y},\mathbf{z})\,\mathrm{d}\mathbf{z} \\ &amp;= \int \frac{\exp\left( -U(\mathbf{x},\mathbf{y},\mathbf{z}) \right)}{\iint \exp\left( -U(\mathbf{x},\mathbf{y}^*,\mathbf{z}^*) \right) \, \mathrm{d}\mathbf{z}^* \,\mathrm{d}\mathbf{y}^*}\,\mathrm{d}\mathbf{z} \\ &amp;= \frac{\exp\left( -h(\mathbf{x}) \right)\cdot\exp\left( -f(\mathbf{x},\mathbf{y}) \right) \int \exp\left( -g(\mathbf{x},\mathbf{z}) \right)\,\mathrm{d}\mathbf{z}}{\iint \exp\left( -h(\mathbf{x}) \right)\cdot\exp\left( -f(\mathbf{x},\mathbf{y}^*)\right) \cdot\exp\left( -g(\mathbf{x},\mathbf{z}^*) \right)\,\mathrm{d}\mathbf{z}^*\,\mathrm{d}\mathbf{y}^*} \\ &amp;= \frac{\exp\left( -h(\mathbf{x}) \right)\cdot\exp\left( -f(\mathbf{x},\mathbf{y}) \right) \int \exp\left( -g(\mathbf{x},\mathbf{z}) \right)\,\mathrm{d}\mathbf{z}}{\exp\left( -h(\mathbf{x}) \right)\cdot\left( \int \exp\left( -f(\mathbf{x},\mathbf{y}^*)\right)\,\mathrm{d}\mathbf{y}^* \right) \cdot \left( \int \exp\left( -g(\mathbf{x},\mathbf{z}^*) \right)\,\mathrm{d}\mathbf{z}^* \right)} \\ &amp;= \frac{\exp\left( -f(\mathbf{x},\mathbf{y}) \right)}{\int \exp\left( -f(\mathbf{x},\mathbf{y}^*)\right)\,\mathrm{d}\mathbf{y}^*} \end{align} \]</span> <span class="math inline">\(p(\mathbf{z}|\mathbf{x}) = \frac{\exp\left( -g(\mathbf{x},\mathbf{z}) \right)}{\int \exp\left( -g(\mathbf{x},\mathbf{z}^*) \right) \,\mathrm{d}\mathbf{z}^*}\)</span> can be proven in the same way.</p><h4 id="collary-1">Collary 1</h4><p>If <span class="math inline">\(U(\mathbf{x},\mathbf{y},\mathbf{z}) = f(\mathbf{x},\mathbf{y}) + g(\mathbf{x},\mathbf{z}) + h(\mathbf{x})\)</span>, then <span class="math inline">\(\mathbf{y} \perp\!\!\!\perp \mathbf{z} \mid \mathbf{x}\)</span>.</p><p><em>Proof</em>: <span class="math display">\[ \begin{align} p(\mathbf{y},\mathbf{z}|\mathbf{x}) &amp;= \frac{\exp\left( -U(\mathbf{x},\mathbf{y},\mathbf{z}) \right)}{\iint \exp\left( -U(\mathbf{x},\mathbf{y}^*,\mathbf{z}^*) \right) \, \mathrm{d}\mathbf{z}^* \,\mathrm{d}\mathbf{y}^*} \\ &amp;= \frac{\exp\left( -f(\mathbf{x},\mathbf{y}) \right)}{\int \exp\left( -f(\mathbf{x},\mathbf{y}^*) \right) \,\mathrm{d}\mathbf{y}^*} \cdot \frac{\exp\left( -g(\mathbf{x},\mathbf{z}) \right)}{\int \exp\left( -g(\mathbf{x},\mathbf{z}^*) \right) \,\mathrm{d}\mathbf{z}^*} \\ &amp;= p(\mathbf{y}|\mathbf{x}) \cdot p(\mathbf{z}|\mathbf{x}) \end{align} \]</span> which implies <span class="math inline">\(\mathbf{y} \perp\!\!\!\perp \mathbf{z} \mid \mathbf{x}\)</span>.</p><h4 id="collary-2">Collary 2</h4><p>If <span class="math inline">\(U(\mathbf{y},\mathbf{z}) = f(\mathbf{y}) + g(\mathbf{z})\)</span>, then: <span class="math display">\[ \begin{align} p(\mathbf{y}) &amp;= \frac{\exp\left( -f(\mathbf{y}) \right)}{\int \exp\left( -f(\mathbf{y}^*) \right) \,\mathrm{d}\mathbf{y}^*} \\ p(\mathbf{z}) &amp;= \frac{\exp\left( -g(\mathbf{z}) \right)}{\int \exp\left( -g(\mathbf{z}^*) \right) \,\mathrm{d}\mathbf{z}^*} \end{align} \]</span> <em>Proof</em>: similar to Theorem 1.</p><h4 id="collary-3">Collary 3</h4><p>If <span class="math inline">\(U(\mathbf{y},\mathbf{z}) = f(\mathbf{y}) + g(\mathbf{z})\)</span>, then <span class="math inline">\(\mathbf{y} \perp\!\!\!\perp \mathbf{z}\)</span>.</p><p><em>Proof</em>: according to Collary 2, and similar to Collary 1.</p><h1 id="bibliography" class="unnumbered">References</h1><div id="refs" class="references" role="doc-bibliography"><div id="ref-kimDeepDirectedGenerative2016"><p>Kim, Taesup, and Yoshua Bengio. 2016. “Deep Directed Generative Models with Energy-Based Probability Estimation.” <em>arXiv Preprint arXiv:1606.03439</em>.</p></div></div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;This post summarizes the relationship between energy function and the deduced probabilistic model by a specified energy function.&lt;/p&gt;&lt;h2 
      
    
    </summary>
    
      <category term="Deep_Learning" scheme="https://wiki.haowen-xu.com/categories/Deep-Learning/"/>
    
      <category term="Energy_Based_Models" scheme="https://wiki.haowen-xu.com/categories/Deep-Learning/Energy-Based-Models/"/>
    
    
  </entry>
  
  <entry>
    <title>Restricted Boltzmann Machine</title>
    <link href="https://wiki.haowen-xu.com/Deep_Learning/Energy_Based_Models/Restricted_Boltzmann_Machine/"/>
    <id>https://wiki.haowen-xu.com/Deep_Learning/Energy_Based_Models/Restricted_Boltzmann_Machine/</id>
    <published>2020-02-28T18:56:31.000Z</published>
    <updated>2020-06-01T12:15:59.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="standard-rbm">Standard RBM</h2><p>The formuation of a standard Restricted Boltzmann Machine (RBM) consists of an observed binary variable <span class="math inline">\(\mathbf{v}\)</span> and a latent binary variable <span class="math inline">\(\mathbf{h}\)</span>, as well as an energy function defined as: <span class="math display">\[ E(\mathbf{v},\mathbf{h}) = -\mathbf{a}^{\top} \mathbf{v} - \mathbf{b}^{\top}\mathbf{h}-\mathbf{v}^{\top}\mathbf{W}\,\mathbf{h} \]</span> the probability of the model is defined as: <span class="math display">\[ P(\mathbf{v},\mathbf{h}) = \frac{1}{Z} \, \exp\left( -E(\mathbf{v},\mathbf{h}) \right) \]</span> where <span class="math inline">\(Z\)</span> is the partition function.</p><p>The conditional distributions are: <span class="math display">\[ \begin{align} P(\mathbf{v}|\mathbf{h}) &amp;= \frac{1}{Z(\mathbf{h})} \, \exp\left(\left( \mathbf{a} + \mathbf{W}\mathbf{h} \right)^{\top}\,\mathbf{v}\right) \\ P(\mathbf{h}|\mathbf{v}) &amp;= \frac{1}{Z(\mathbf{v})} \, \exp\left(\left( \mathbf{b}^{\top} + \mathbf{v}^{\top}\mathbf{W} \right)\mathbf{h}\right) \end{align} \]</span></p><p>It is easy to verify that <span class="math inline">\(h_i\)</span> is independent of <span class="math inline">\(h_j\)</span>, for <span class="math inline">\(i \neq j\)</span>, given an observed <span class="math inline">\(\mathbf{v}\)</span>. This is also true for <span class="math inline">\(v_i\)</span>, <span class="math inline">\(v_j\)</span> given <span class="math inline">\(\mathbf{h}\)</span>. Thus sampling from <span class="math inline">\(P(\mathbf{v},\mathbf{h})\)</span> could be achieved by sampling from the two conditional distributions alternatively (i.e., a block Gibbs sampler).</p><p>The parameters <span class="math inline">\(\mathbf{a},\mathbf{b},\mathbf{W}\)</span> of <span class="math inline">\(E(\mathbf{v},\mathbf{h})\)</span> can be optimized by <a href="/Deep_Learning/Confronting_Partition_Function/Contrastive_Divergence/">Contrastive Divergence</a> algorithm, with the energy function <span class="math inline">\(U(\mathbf{z})\)</span> for <span class="math inline">\(\mathbf{v}\)</span>, satisfying <span class="math inline">\(U(\mathbf{v}) + \log Z = \log P(\mathbf{v})\)</span>. Whereas it is more simply to deduce the energy function <span class="math inline">\(U(\mathbf{v})\)</span> if we consider the conditional independence of <span class="math inline">\(h_i\)</span>, <span class="math inline">\(h_j\)</span> beforehand, and use the element-wise notations, I provide here the deduction using vector notation.</p><h3 id="deduction-of-umathbfv-using-vector-notation">Deduction of <span class="math inline">\(U(\mathbf{v})\)</span> using Vector Notation</h3><p>The marginal distribution <span class="math inline">\(P(\mathbf{v})\)</span> for the training data is: <span class="math display">\[ P(\mathbf{v}) = \sum_{\mathbf{h}} P(\mathbf{v},\mathbf{h}) = \frac{1}{Z} \, \exp\left( \mathbf{a}^{\top}\mathbf{v} \right) \cdot \sum_{\mathbf{h}} \exp\left( \left( \mathbf{b}^{\top}+\mathbf{v}^{\top}\mathbf{W} \right)\mathbf{h} \right) \]</span> which results in the following energy function for <span class="math inline">\(\mathbf{v}\)</span>: <span class="math display">\[ U(\mathbf{v}) = \log P(\mathbf{v}) = \mathbf{a}^{\top}\mathbf{v} + \log\sum_{\mathbf{h}}\exp\left( \left( \mathbf{b}^{\top}+\mathbf{v}^{\top}\mathbf{W} \right)\mathbf{h} \right) \]</span> If the number of elements of the vector <span class="math inline">\(\mathbf{h}\)</span> is k, we can further get: <span class="math display">\[ \begin{align} \sum_{\mathbf{h}}\exp\left( \left( \mathbf{b}^{\top}+\mathbf{v}^{\top}\mathbf{W} \right)\mathbf{h} \right) &amp;= \sum_{h_1,h_2,\dots,h_k} \exp\left( \sum_{j=1}^k(b_j + \mathbf{v}^{\top} \mathbf{W}_j)\,h_j \right) \\ &amp;= \sum_{h_1,h_2,\dots,h_k} \prod_{j=1}^k \exp\left( (b_j+\mathbf{v}^{\top} \mathbf{W}_j) \,h_j \right) \\ &amp;= \prod_{j=1}^k \sum_{h_j} \exp\left( (b_j+\mathbf{v}^{\top} \mathbf{W}_j) \,h_j \right) \end{align} \]</span> where <span class="math inline">\(\mathbf{W}_j\)</span> is the <span class="math inline">\(j\)</span>-th column of the matrix <span class="math inline">\(\mathbf{W}\)</span>. We then have: <span class="math display">\[ U(\mathbf{v}) = \mathbf{a}^{\top}\mathbf{v} + \sum_{j=1}^k \log \sum_{h_j} \exp\left( (b_j+\mathbf{v}^{\top} \mathbf{W}_j) \,h_j \right) \]</span> Given that <span class="math inline">\(h_j\)</span> is a binary variable, <span class="math inline">\(\sum_{h_j}\)</span> can be it further deduced into: <span class="math display">\[ U(\mathbf{v}) = \mathbf{a}^{\top}\mathbf{v} + \sum_{j=1}^k \log \left( 1 + \exp\left( b_j+\mathbf{v}^{\top} \mathbf{W}_j \right) \right) \]</span></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;standard-rbm&quot;&gt;Standard RBM&lt;/h2&gt;&lt;p&gt;The formuation of a standard Restricted Boltzmann Machine (RBM) consists of an observed binary var
      
    
    </summary>
    
      <category term="Deep_Learning" scheme="https://wiki.haowen-xu.com/categories/Deep-Learning/"/>
    
      <category term="Energy_Based_Models" scheme="https://wiki.haowen-xu.com/categories/Deep-Learning/Energy-Based-Models/"/>
    
    
  </entry>
  
  <entry>
    <title>Tracking the Concepts</title>
    <link href="https://wiki.haowen-xu.com/Research_Work/Tracking_the_Concepts/"/>
    <id>https://wiki.haowen-xu.com/Research_Work/Tracking_the_Concepts/</id>
    <published>2020-02-25T11:50:00.000Z</published>
    <updated>2020-06-01T12:15:59.028Z</updated>
    
    <content type="html"><![CDATA[<div id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <div class="hbe-input-container">  <input type="password" id="hbePass" placeholder="" />    <label for="hbePass">Hey, password is required here.</label>    <div class="bottom-line"></div>  </div>  <script id="hbeData" type="hbeData" data-hmacdigest="784de48c880dedb38c03d079bfe509dce9a07750a8bff31ca8941e6270ef7197">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</script></div><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
    
    <summary type="html">
    
      Here&#39;s something encrypted, password is required to continue reading.
    
    </summary>
    
      <category term="Research_Work" scheme="https://wiki.haowen-xu.com/categories/Research-Work/"/>
    
    
  </entry>
  
  <entry>
    <title>Overview</title>
    <link href="https://wiki.haowen-xu.com/Deep_Learning/Energy_Based_Models/Overview/"/>
    <id>https://wiki.haowen-xu.com/Deep_Learning/Energy_Based_Models/Overview/</id>
    <published>2020-02-25T08:45:00.000Z</published>
    <updated>2020-06-01T12:15:59.000Z</updated>
    
    <content type="html"><![CDATA[<p>Energy based models (EBM) consists of three ingredients:</p><ol type="1"><li>The energy function: to assign a scalar to each configuration of the variables.</li><li>Inference: given a set of observed variables, to find values of the remaining variables that minimizes the energy.</li><li>Learning: to find an energy function that assigns low energy values to correct configurations of the variables, and high energy values to incorrect configurations of the variables.</li><li>Loss function: minimized during training, which measures to quality of the energy function.</li></ol><h2 id="loss-functions">Loss functions</h2><p>The general form of the training objective of energy based model can be formulated as <span class="citation" data-cites="lecunTutorialEnergybasedLearning2006">(LeCun et al. <a href="#ref-lecunTutorialEnergybasedLearning2006" role="doc-biblioref">2006</a>)</span>: <span class="math display">\[ \mathcal{L}(E,\mathcal{S})=\frac{1}{P}\sum_{i=1}^P L(Y^i, E(W,\mathcal{Y},X^i)) + R(W) \]</span></p><p>where <span class="math inline">\(L\)</span> is the loss function that should "pull-down" the energy function at "correct" configurations (i.e., the training data <span class="math inline">\((X^i, Y^i)\)</span>), and "pull-up" the energy function at incorrect configurations. <span class="math inline">\(R\)</span> is the constraint on the model parameter <span class="math inline">\(W\)</span>, which may effectively restrict the VC-dimension of the model.</p><p>Here are some popular choices of loss functions for energy based models:</p><h3 id="energy-loss">Energy loss</h3><p><span class="math display">\[ L=E(W,Y^i,X^i) \]</span></p><p>which requires <span class="math inline">\(L\)</span> to satisfy the property that: "pull-down" the energy at correct configurations will automatically "pull-up" the energy at incorrect configurations.</p><h3 id="generalized-perceptron-loss">Generalized Perceptron Loss</h3><p><span class="math display">\[ L = E(W,Y^i,X^i) - \min_{Y \in \mathcal{Y}} E(W,Y,X^i) \]</span></p><p>There is no mechanism for creating an energy gap between the correct configurations and the incorrect ones, which may potentially produce (almost) flat energy surfaces if the architecture allows it.</p><h3 id="generalized-margin-losses">Generalized Margin Losses</h3><p>Here is some common definitions of this section:</p><ul><li><span class="math inline">\(\bar{Y}^i = \arg\min_{Y\in \mathcal{Y},\left\| Y-Y^i \right\| &gt; \epsilon} E(W,Y,X^i)\)</span></li></ul><h4 id="hinge-loss">Hinge loss</h4><p><span class="math display">\[ L=\max\left(0, m+E(W,Y^i,X^i)-E(W,\bar{Y}^i,X^i)\right) \]</span></p><h4 id="log-loss">Log Loss</h4><p><span class="math display">\[ L = \log\left( 1 + \exp\left(E(W,Y^i,X^i) - E(W,\bar{Y}^i,X^i)\right) \right) \]</span></p><h4 id="lvq2-loss">LVQ2 Loss</h4><p><span class="math display">\[ L = \max\left( 0, m+E(W,Y^i,X^i) - E(W,\bar{Y}^i, X^i) \right) \]</span></p><h4 id="mce-loss">MCE Loss</h4><p><span class="math display">\[ L = \sigma\left( E(W,Y^i,X^i) - E(W,\bar{Y}^i,X^i) \right) \]</span></p><h4 id="square-square-loss">Square-Square Loss</h4><p><span class="math display">\[ L = E(W,Y^i,X^i) + \left( \max\left(0, m-E(W,\bar{Y}^i,X^i)\right)^2 \right) \]</span></p><h4 id="square-exponential-loss">Square-Exponential Loss</h4><p><span class="math display">\[ L = E(W,Y^i,X^i)^2 + \gamma \exp\left( -E(W,\bar{Y}^i,X^i) \right) \]</span></p><h3 id="negative-log-likelihood-loss-nll-loss">Negative Log-Likelihood Loss (NLL Loss)</h3><p><span class="math display">\[ \begin{align} L &amp;= E(W,Y^i,X^i) + \mathcal{F}_{\beta}(W,\mathcal{Y},X^i) \\ \mathcal{F}_{\beta}(W,\mathcal{Y},X^i) &amp;= \frac{1}{\beta} \log\left( \int_{y\in \mathcal{Y}}\exp\left( -\beta E(W,y,X^i)\right) \,dy \right) \end{align} \]</span></p><p>where <span class="math display">\[ P(Y|X^i,W) = \frac{\exp\left( -\beta E(W,Y,X^i) \right)}{\int_{y\in \mathcal{Y}}\exp\left( -\beta E(W,y,X^i)\right) \, dy} \]</span> Some authors have argued that the NLL loss puts too much emphasis on mistakes, which inspires the MEE loss.</p><h4 id="minimum-empirical-error-loss-mee-loss">Minimum Empirical Error Loss (MEE Loss)</h4><p><span class="math display">\[ L = 1 - P(Y|X^i,W) = 1 - \frac{\exp\left( -\beta E(W,Y,X^i) \right)}{\int_{y\in \mathcal{Y}}\exp\left( -\beta E(W,y,X^i)\right) \, dy} \]</span></p><h2 id="learning-with-approximate-inference">Learning with Approximate Inference</h2><p>Many of the losses or algorithms used by energy based models does not guarantee that the energy for <span class="math inline">\(E(W,Y,X^i)\)</span> is pull-up propertly at all <span class="math inline">\(Y \in \mathcal{Y}, Y \neq Y^i\)</span>, such that <span class="math inline">\(E(W,Y^i,X^i)\)</span> should be a global minimum.</p><p><span class="citation" data-cites="lecunTutorialEnergybasedLearning2006">LeCun et al. (<a href="#ref-lecunTutorialEnergybasedLearning2006" role="doc-biblioref">2006</a>)</span> justifies that, if learning is driven by approximated inference, such that all the <em>constrastive samples</em> found by the <em>approximated inference</em> algorithm was pull-up, then there is no need to worry about the far away samples which cannot be found by the inference algorithm (which I personally disagree with this, where it might cause some "out-distribution" problems).</p><p>An example of such approximated inference driven learning algorithm is <a href="/Deep_Learning/Confronting_Partition_Function/Contrastive_Divergence/">Contrastive Divergence</a>:</p><h3 id="contrastive-divergence">Contrastive Divergence</h3><p><span class="math display">\[ W \leftarrow W - \eta \left( \frac{\partial E(W,Y^i,X^i)}{\partial W} - \frac{\partial E(W,\bar{Y}^i,X)}{\partial W} \right) \]</span></p><h1 id="bibliography" class="unnumbered">References</h1><div id="refs" class="references" role="doc-bibliography"><div id="ref-lecunTutorialEnergybasedLearning2006"><p>LeCun, Yann, Sumit Chopra, Raia Hadsell, M. Ranzato, and F. Huang. 2006. “A Tutorial on Energy-Based Learning.” <em>Predicting Structured Data</em> 1 (0).</p></div></div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Energy based models (EBM) consists of three ingredients:&lt;/p&gt;&lt;ol type=&quot;1&quot;&gt;&lt;li&gt;The energy function: to assign a scalar to each configuratio
      
    
    </summary>
    
      <category term="Deep_Learning" scheme="https://wiki.haowen-xu.com/categories/Deep-Learning/"/>
    
      <category term="Energy_Based_Models" scheme="https://wiki.haowen-xu.com/categories/Deep-Learning/Energy-Based-Models/"/>
    
    
  </entry>
  
</feed>
